<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:media="http://www.rssboard.org/media-rss">
<channel>
<title>
   g-simmons-papercast
  </title>
<link/>
<language>
   en-us
  </language>
<atom:link href="" rel="self" type="application/rss+xml"/>
<copyright>
   Rights to paper content are reserved by the authors for each paper. I make no claim to ownership or copyright of the content of this podcast.
  </copyright>
<itunes:subtitle/>
<itunes:author>
   Gabriel Simmons
  </itunes:author>
<itunes:summary>
   Podcast of text-to-speech for arbitrarily chosen NLP papers.
  </itunes:summary>
<itunes:keywords>
   Machine Learning, Natural Language Processing, Artificial Intelligence
  </itunes:keywords>
<description>
   Podcast of text-to-speech for arbitrarily chosen NLP papers.
  </description>
<itunes:owner>
<itunes:name>
    Gabriel Simmons
   </itunes:name>
<itunes:email>
    gsimmons@ucdavis.edu
   </itunes:email>
</itunes:owner>
<itunes:image href="https://g-simmons.github.io/g-simmons-papercast/cover.jpg"/>
<itunes:category text="Mathematics"/>
<itunes:category text="Tech News"/>
<itunes:category text="Courses"/>
<item>
<title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts Maarten Sap
   </title>
<itunes:title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts Maarten Sap
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformerbased classifiers on TOXICHAT that achieve 0.71 F 1 for offensive labels and 0.53 Macro-F 1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.
   </itunes:summary>
<description>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformerbased classifiers on TOXICHAT that achieve 0.71 F 1 for offensive labels and 0.53 Macro-F 1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.397.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2864.7445
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.397.mp3
   </guid>
<itunes:episode>
    1
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference
   </title>
<itunes:title>
    Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area
   </itunes:summary>
<description>
    A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.01007v4.Right_for_the_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3551.6605
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.01007v4.Right_for_the_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference.mp3
   </guid>
<itunes:episode>
    2
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks
   </title>
<itunes:title>
    Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.
   </itunes:summary>
<description>
    Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2004.40175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3
   </guid>
<itunes:episode>
    3
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Generalist Agent
   </title>
<itunes:title>
    A Generalist Agent
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.
   </itunes:summary>
<description>
    Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06175v2.A_Generalist_Agent.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5319.94125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06175v2.A_Generalist_Agent.mp3
   </guid>
<itunes:episode>
    4
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    StruBERT: Structure-aware BERT for Table Search and Matching
   </title>
<itunes:title>
    StruBERT: Structure-aware BERT for Table Search and Matching
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    A large amount of information is stored in data tables. Users can search for data tables using a keyword-based query. A table is composed primarily of data values that are organized in rows and columns providing implicit structural information. A table is usually accompanied by secondary information such as the caption, page title, etc., that form the textual information. Understanding the connection between the textual and structural information is an important yet neglected aspect in table retrieval as previous methods treat each source of information independently. In addition, users can search for data tables that are similar to an existing table, and this setting can be seen as a content-based table retrieval. In this paper, we propose StruBERT, a structure-aware BERT model that fuses the textual and structural information of a data table to produce context-aware representations for both textual and tabular content of a data table. StruBERT features are integrated in a new end-to-end neural ranking model to solve three table-related downstream tasks: keyword- and content-based table retrieval, and table similarity. We evaluate our approach using three datasets, and we demonstrate substantial improvements in terms of retrieval and classification metrics over state-of-the-art methods.
   </itunes:summary>
<description>
    A large amount of information is stored in data tables. Users can search for data tables using a keyword-based query. A table is composed primarily of data values that are organized in rows and columns providing implicit structural information. A table is usually accompanied by secondary information such as the caption, page title, etc., that form the textual information. Understanding the connection between the textual and structural information is an important yet neglected aspect in table retrieval as previous methods treat each source of information independently. In addition, users can search for data tables that are similar to an existing table, and this setting can be seen as a content-based table retrieval. In this paper, we propose StruBERT, a structure-aware BERT model that fuses the textual and structural information of a data table to produce context-aware representations for both textual and tabular content of a data table. StruBERT features are integrated in a new end-to-end neural ranking model to solve three table-related downstream tasks: keyword- and content-based table retrieval, and table similarity. We evaluate our approach using three datasets, and we demonstrate substantial improvements in terms of retrieval and classification metrics over state-of-the-art methods.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14278v1.StruBERT_Structure_aware_BERT_for_Table_Search_and_Matching.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2297.835
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14278v1.StruBERT_Structure_aware_BERT_for_Table_Search_and_Matching.mp3
   </guid>
<itunes:episode>
    5
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    On Calibration of Modern Neural Networks
   </title>
<itunes:title>
    On Calibration of Modern Neural Networks
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.
   </itunes:summary>
<description>
    Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.04599v2.On_Calibration_of_Modern_Neural_Networks.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2863.6995
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.04599v2.On_Calibration_of_Modern_Neural_Networks.mp3
   </guid>
<itunes:episode>
    6
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Debiasing Masks: A New Framework for Shortcut Mitigation in NLU
   </title>
<itunes:title>
    Debiasing Masks: A New Framework for Shortcut Mitigation in NLU
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Debiasing language models from unwanted behaviors in Natural Language Understanding tasks is a topic with rapidly increasing interest in the NLP community. Spurious statistical correlations in the data allow models to perform shortcuts and avoid uncovering more advanced and desirable linguistic features. A multitude of effective debiasing approaches has been proposed, but flexibility remains a major issue. For the most part, models must be retrained to find a new set of weights with debiased behavior. We propose a new debiasing method in which we identify debiased pruning masks that can be applied to a finetuned model. This enables the selective and conditional application of debiasing behaviors. We assume that bias is caused by a certain subset of weights in the network; our method is, in essence, a mask search to identify and remove biased weights. Our masks show equivalent or superior performance to the standard counterparts, while offering important benefits. Pruning masks can be stored with high efficiency in memory, and it becomes possible to switch among several debiasing behaviors (or revert back to the original biased model) at inference time. Finally, it opens the doors to further research on how biases are acquired by studying the generated masks. For example, we observed that the early layers and attention heads were pruned more aggressively, possibly hinting towards the location in which biases may be encoded.
   </itunes:summary>
<description>
    Debiasing language models from unwanted behaviors in Natural Language Understanding tasks is a topic with rapidly increasing interest in the NLP community. Spurious statistical correlations in the data allow models to perform shortcuts and avoid uncovering more advanced and desirable linguistic features. A multitude of effective debiasing approaches has been proposed, but flexibility remains a major issue. For the most part, models must be retrained to find a new set of weights with debiased behavior. We propose a new debiasing method in which we identify debiased pruning masks that can be applied to a finetuned model. This enables the selective and conditional application of debiasing behaviors. We assume that bias is caused by a certain subset of weights in the network; our method is, in essence, a mask search to identify and remove biased weights. Our masks show equivalent or superior performance to the standard counterparts, while offering important benefits. Pruning masks can be stored with high efficiency in memory, and it becomes possible to switch among several debiasing behaviors (or revert back to the original biased model) at inference time. Finally, it opens the doors to further research on how biases are acquired by studying the generated masks. For example, we observed that the early layers and attention heads were pruned more aggressively, possibly hinting towards the location in which biases may be encoded.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.16079v1.Debiasing_Masks_A_New_Framework_for_Shortcut_Mitigation_in_NLU.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1356.87825
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.16079v1.Debiasing_Masks_A_New_Framework_for_Shortcut_Mitigation_in_NLU.mp3
   </guid>
<itunes:episode>
    7
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change
   </title>
<itunes:title>
    Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.
   </itunes:summary>
<description>
    Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1605.09096v6.Diachronic_Word_Embeddings_Reveal_Statistical_Laws_of_Semantic_Change.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2799.96075
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1605.09096v6.Diachronic_Word_Embeddings_Reveal_Statistical_Laws_of_Semantic_Change.mp3
   </guid>
<itunes:episode>
    8
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Incivility is Rising among American Politicians on Twitter
   </title>
<itunes:title>
    Incivility is Rising among American Politicians on Twitter
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Frimer et al., 2022_Incivility is Rising among American Politicians on Twitter.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6438.2955
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Frimer et al., 2022_Incivility is Rising among American Politicians on Twitter.mp3
   </guid>
<itunes:episode>
    9
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation
   </title>
<itunes:title>
    A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Automatic evaluation of summarization focuses on developing a metric to represent the quality of the resulting text. However, text quality is represented in a variety of dimensions ranging from grammaticality to readability and coherence. In our work, we analyze the dependencies between a variety of quality dimensions on automatically created multi-document summaries and which dimensions automatic evaluation metrics such as ROUGE, PEAK or JSD are able to capture. Our results indicate that variants of ROUGE are correlated to various quality dimensions and that some automatic summarization methods achieve higher quality summaries than others with respect to individual summary quality dimensions. Our results also indicate that differentiating between quality dimensions facilitates inspection and fine-grained comparison of summarization methods and its characteristics.
   </itunes:summary>
<description>
    Automatic evaluation of summarization focuses on developing a metric to represent the quality of the resulting text. However, text quality is represented in a variety of dimensions ranging from grammaticality to readability and coherence. In our work, we analyze the dependencies between a variety of quality dimensions on automatically created multi-document summaries and which dimensions automatic evaluation metrics such as ROUGE, PEAK or JSD are able to capture. Our results indicate that variants of ROUGE are correlated to various quality dimensions and that some automatic summarization methods achieve higher quality summaries than others with respect to individual summary quality dimensions. Our results also indicate that differentiating between quality dimensions facilitates inspection and fine-grained comparison of summarization methods and its characteristics.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2020.lrec-1.826.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2476.6695
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2020.lrec-1.826.mp3
   </guid>
<itunes:episode>
    10
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations
   </title>
<itunes:title>
    CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, humans have the ability to learn new concepts from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. CLUES consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop ExEnt, an entailment-based model that learns classifiers using explanations. ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future. Code and datasets are available at: https://clues-benchmark.github.io.
   </itunes:summary>
<description>
    Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, humans have the ability to learn new concepts from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. CLUES consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop ExEnt, an entailment-based model that learns classifiers using explanations. ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future. Code and datasets are available at: https://clues-benchmark.github.io.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.07142v1.CLUES_A_Benchmark_for_Learning_Classifiers_using_Natural_Language_Explanations.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3474.39025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.07142v1.CLUES_A_Benchmark_for_Learning_Classifiers_using_Natural_Language_Explanations.mp3
   </guid>
<itunes:episode>
    11
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning
   </title>
<itunes:title>
    Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.
   </itunes:summary>
<description>
    Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09712v1.Selection_Inference_Exploiting_Large_Language_Models_for_Interpretable_Logical_Reasoning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4674.4555
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09712v1.Selection_Inference_Exploiting_Large_Language_Models_for_Interpretable_Logical_Reasoning.mp3
   </guid>
<itunes:episode>
    12
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation
   </title>
<itunes:title>
    Play the Shannon Game With Language Models: A Human-Free Approach to Summary Evaluation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The goal of a summary is to concisely state the most important information in a document. With this principle in mind, we introduce new reference-free summary evaluation metrics that use a pretrained language model to estimate the information content shared between a document and its summary. These metrics are a modern take on the Shannon Game, a method for summary quality scoring proposed decades ago, where we replace human annotators with language models. We also view these metrics as an extension of BLANC, a recently proposed approach to summary quality measurement based on the performance of a language model with and without the help of a summary. Using transformer based language models, we empirically verify that our metrics achieve state-of-the-art correlation with human judgement of the summary quality dimensions of both coherence and relevance, as well as competitive correlation with human judgement of consistency and fluency.
   </itunes:summary>
<description>
    The goal of a summary is to concisely state the most important information in a document. With this principle in mind, we introduce new reference-free summary evaluation metrics that use a pretrained language model to estimate the information content shared between a document and its summary. These metrics are a modern take on the Shannon Game, a method for summary quality scoring proposed decades ago, where we replace human annotators with language models. We also view these metrics as an extension of BLANC, a recently proposed approach to summary quality measurement based on the performance of a language model with and without the help of a summary. Using transformer based language models, we empirically verify that our metrics achieve state-of-the-art correlation with human judgement of the summary quality dimensions of both coherence and relevance, as well as competitive correlation with human judgement of consistency and fluency.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.10918v2.Play_the_Shannon_Game_With_Language_Models_A_Human_Free_Approach_to_Summary_Evaluation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4344.99925
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.10918v2.Play_the_Shannon_Game_With_Language_Models_A_Human_Free_Approach_to_Summary_Evaluation.mp3
   </guid>
<itunes:episode>
    13
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Understanding Gradient Descent on Edge of Stability in Deep Learning
   </title>
<itunes:title>
    Understanding Gradient Descent on Edge of Stability in Deep Learning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Deep learning experiments in Cohen et al. (2021) using deterministic Gradient Descent (GD) revealed an {\em Edge of Stability (EoS)} phase when learning rate (LR) and sharpness (\emph{i.e.}, the largest eigenvalue of Hessian) no longer behave as in traditional optimization. Sharpness stabilizes around $2/$LR and loss goes up and down across iterations, yet still with an overall downward trend. The current paper mathematically analyzes a new mechanism of implicit regularization in the EoS phase, whereby GD updates due to non-smooth loss landscape turn out to evolve along some deterministic flow on the manifold of minimum loss. This is in contrast to many previous results about implicit bias either relying on infinitesimal updates or noise in gradient. Formally, for any smooth function $L$ with certain regularity condition, this effect is demonstrated for (1) {\em Normalized GD}, i.e., GD with a varying LR $ \eta_t =\frac{ \eta }{ || \nabla L(x(t)) || } $ and loss $L$; (2) GD with constant LR and loss $\sqrt{L}$. Both provably enter the Edge of Stability, with the associated flow on the manifold minimizing $\lambda_{\max}(\nabla^2 L)$. The above theoretical results have been corroborated by an experimental study.
   </itunes:summary>
<description>
    Deep learning experiments in Cohen et al. (2021) using deterministic Gradient Descent (GD) revealed an {\em Edge of Stability (EoS)} phase when learning rate (LR) and sharpness (\emph{i.e.}, the largest eigenvalue of Hessian) no longer behave as in traditional optimization. Sharpness stabilizes around $2/$LR and loss goes up and down across iterations, yet still with an overall downward trend. The current paper mathematically analyzes a new mechanism of implicit regularization in the EoS phase, whereby GD updates due to non-smooth loss landscape turn out to evolve along some deterministic flow on the manifold of minimum loss. This is in contrast to many previous results about implicit bias either relying on infinitesimal updates or noise in gradient. Formally, for any smooth function $L$ with certain regularity condition, this effect is demonstrated for (1) {\em Normalized GD}, i.e., GD with a varying LR $ \eta_t =\frac{ \eta }{ || \nabla L(x(t)) || } $ and loss $L$; (2) GD with constant LR and loss $\sqrt{L}$. Both provably enter the Edge of Stability, with the associated flow on the manifold minimizing $\lambda_{\max}(\nabla^2 L)$. The above theoretical results have been corroborated by an experimental study.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09745v1.Understanding_Gradient_Descent_on_Edge_of_Stability_in_Deep_Learning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    15057.8155
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09745v1.Understanding_Gradient_Descent_on_Edge_of_Stability_in_Deep_Learning.mp3
   </guid>
<itunes:episode>
    14
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Automated and Explainable Ontology Extension Based on Deep Learning: A Case Study in the Chemical Domain
   </title>
<itunes:title>
    Automated and Explainable Ontology Extension Based on Deep Learning: A Case Study in the Chemical Domain
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction enables them to maintain a high quality, allowing them to be widely accepted across their community. However, the manual development process does not scale for large domains. We present a new methodology for automatic ontology extension and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We trained a Transformer-based deep learning model on the leaf node structures from the ChEBI ontology and the classes to which they belong. The model is then capable of automatically classifying previously unseen chemical structures. The proposed model achieved an overall F1 score of 0.80, an improvement of 6 percentage points over our previous results on the same dataset. Additionally, we demonstrate how visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions.
   </itunes:summary>
<description>
    Reference ontologies provide a shared vocabulary and knowledge resource for their domain. Manual construction enables them to maintain a high quality, allowing them to be widely accepted across their community. However, the manual development process does not scale for large domains. We present a new methodology for automatic ontology extension and apply it to the ChEBI ontology, a prominent reference ontology for life sciences chemistry. We trained a Transformer-based deep learning model on the leaf node structures from the ChEBI ontology and the classes to which they belong. The model is then capable of automatically classifying previously unseen chemical structures. The proposed model achieved an overall F1 score of 0.80, an improvement of 6 percentage points over our previous results on the same dataset. Additionally, we demonstrate how visualizing the model's attention weights can help to explain the results by providing insight into how the model made its decisions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.09202v1.Automated_and_Explainable_Ontology_Extension_Based_on_Deep_Learning_A_Case_Study_in_the_Chemical_Domain.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2167.275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.09202v1.Automated_and_Explainable_Ontology_Extension_Based_on_Deep_Learning_A_Case_Study_in_the_Chemical_Domain.mp3
   </guid>
<itunes:episode>
    15
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    How Responsible Are You For Your Actions?
   </title>
<itunes:title>
    How Responsible Are You For Your Actions?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    I would like to extend my profound gratitude to Dr. Heidi Storl for her immense contribution to the success of this research. I would also like to thank the Ayensah family and the Siaw family for their constant support and encouragement throughout my study.In a world where our sense of responsibility rests solely on the existence of a notion of morality and free will, how do we make sense of responsibility when neuroscientific findings have been shown to trim morality and free will? How can a civil society held together by justice emanating from a retributive sense of responsibility keep running when the basis of retribution has been undermined? This paper examines the relationship between morality, free will, responsibility, and neuroscience so as to determine whether we can justifiably attribute moral responsibility. In this paper, I argue that moral responsibility can still be attributed, but only through a lens of free will skepticism. How would such a responsibility materialize in our contemporary society? What problems will it encounter? My research seeks to draw a comprehensive plan for acting on this new sense of moral responsibility through an examination of findings in philosophy, neuroscience and psychology.
   </itunes:summary>
<description>
    I would like to extend my profound gratitude to Dr. Heidi Storl for her immense contribution to the success of this research. I would also like to thank the Ayensah family and the Siaw family for their constant support and encouragement throughout my study.In a world where our sense of responsibility rests solely on the existence of a notion of morality and free will, how do we make sense of responsibility when neuroscientific findings have been shown to trim morality and free will? How can a civil society held together by justice emanating from a retributive sense of responsibility keep running when the basis of retribution has been undermined? This paper examines the relationship between morality, free will, responsibility, and neuroscience so as to determine whether we can justifiably attribute moral responsibility. In this paper, I argue that moral responsibility can still be attributed, but only through a lens of free will skepticism. How would such a responsibility materialize in our contemporary society? What problems will it encounter? My research seeks to draw a comprehensive plan for acting on this new sense of moral responsibility through an examination of findings in philosophy, neuroscience and psychology.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/cmv7i2_Ayensah.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2803.04325
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/cmv7i2_Ayensah.mp3
   </guid>
<itunes:episode>
    16
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Human-level play in the game of Diplomacy by combining language models with strategic reasoning
   </title>
<itunes:title>
    Human-level play in the game of Diplomacy by combining language models with strategic reasoning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/science.ade9097.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    16.431
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/science.ade9097.mp3
   </guid>
<itunes:episode>
    17
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Symbolic Behaviour in Artificial Intelligence
   </title>
<itunes:title>
    Symbolic Behaviour in Artificial Intelligence
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.
   </itunes:summary>
<description>
    The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.03406v2.Symbolic_Behaviour_in_Artificial_Intelligence.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2102.03406v2.Symbolic_Behaviour_in_Artificial_Intelligence.mp3.vtt"/>
<itunes:duration>
    2281.212
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.03406v2.Symbolic_Behaviour_in_Artificial_Intelligence.mp3
   </guid>
<itunes:episode>
    18
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Structured Prediction as Translation between Augmented Natural Languages
   </title>
<itunes:title>
    Structured Prediction as Translation between Augmented Natural Languages
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.
   </itunes:summary>
<description>
    We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3945.35175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3
   </guid>
<itunes:episode>
    19
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Reason, emotion, and the problem of world poverty: moral sentiment theory and international ethics
   </title>
<itunes:title>
    Reason, emotion, and the problem of world poverty: moral sentiment theory and international ethics
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This article defends a sentimentalist cosmopolitan approach to international ethics against the rationalist cosmopolitan claim that emotions ought to be subjugated by their master, reason, and in processes of ethical deliberation. It argues that emotions play an indispensable role in making moral judgements and help to motivate ethical actions. Drawing on elements of 18th century moral sentiment theory and recent advances in neuroscience and psychology, the article demonstrates that reason and emotion are intimately linked forms of reflective thought, that emotion is central to reason and, far from disrupting processes of ethical deliberation, may actually enhance our ability to make moral judgements. Focusing on the problem of global poverty, the article shows that a sentimentalist cosmopolitan ethic provides a holistic approach to moral dilemmas in world politics that is capable of identifying injustices, prescribing how we ought to respond to them, and motivating ethical action in response to the injustices we observe.
   </itunes:summary>
<description>
    This article defends a sentimentalist cosmopolitan approach to international ethics against the rationalist cosmopolitan claim that emotions ought to be subjugated by their master, reason, and in processes of ethical deliberation. It argues that emotions play an indispensable role in making moral judgements and help to motivate ethical actions. Drawing on elements of 18th century moral sentiment theory and recent advances in neuroscience and psychology, the article demonstrates that reason and emotion are intimately linked forms of reflective thought, that emotion is central to reason and, far from disrupting processes of ethical deliberation, may actually enhance our ability to make moral judgements. Focusing on the problem of global poverty, the article shows that a sentimentalist cosmopolitan ethic provides a holistic approach to moral dilemmas in world politics that is capable of identifying injustices, prescribing how we ought to respond to them, and motivating ethical action in response to the injustices we observe.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/reason-emotion-and-the-problem-of-world-poverty-moral-sentiment-theory-and-international-ethics.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6181.747
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/reason-emotion-and-the-problem-of-world-poverty-moral-sentiment-theory-and-international-ethics.mp3
   </guid>
<itunes:episode>
    20
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction
   </title>
<itunes:title>
    RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.
   </itunes:summary>
<description>
    Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09101v1.RelationPrompt_Leveraging_Prompts_to_Generate_Synthetic_Data_for_Zero_Shot_Relation_Triplet_Extraction.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2930.18125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09101v1.RelationPrompt_Leveraging_Prompts_to_Generate_Synthetic_Data_for_Zero_Shot_Relation_Triplet_Extraction.mp3
   </guid>
<itunes:episode>
    21
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation
   </title>
<itunes:title>
    Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.
   </itunes:summary>
<description>
    Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.03132v1.Greedy_Search_with_Probabilistic_N_gram_Matching_for_Neural_Machine_Translation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1153.20175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.03132v1.Greedy_Search_with_Probabilistic_N_gram_Matching_for_Neural_Machine_Translation.mp3
   </guid>
<itunes:episode>
    22
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Wav2Letter: an End-to-End ConvNet-based Speech Recognition System
   </title>
<itunes:title>
    Wav2Letter: an End-to-End ConvNet-based Speech Recognition System
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.
   </itunes:summary>
<description>
    This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1327.72575
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3
   </guid>
<itunes:episode>
    23
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers
   </title>
<itunes:title>
    Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that specific distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language -- burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data influenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model -- a skewed, Zipfian distribution over classes -- which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own.
   </itunes:summary>
<description>
    Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that specific distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language -- burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data influenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model -- a skewed, Zipfian distribution over classes -- which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.05055v2.Data_Distributional_Properties_Drive_Emergent_Few_Shot_Learning_in_Transformers.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2260.00975
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.05055v2.Data_Distributional_Properties_Drive_Emergent_Few_Shot_Learning_in_Transformers.mp3
   </guid>
<itunes:episode>
    24
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy
   </title>
<itunes:title>
    Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -- and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.
   </itunes:summary>
<description>
    In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -- and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.12771v1.Does_Moral_Code_Have_a_Moral_Code_Probing_Delphi_s_Moral_Philosophy.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3818.29225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.12771v1.Does_Moral_Code_Have_a_Moral_Code_Probing_Delphi_s_Moral_Philosophy.mp3
   </guid>
<itunes:episode>
    25
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    STaR: Bootstrapping Reasoning With Reasoning
   </title>
<itunes:title>
    STaR: Bootstrapping Reasoning With Reasoning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.
   </itunes:summary>
<description>
    Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14465v1.STaR_Bootstrapping_Reasoning_With_Reasoning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3674.40975
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14465v1.STaR_Bootstrapping_Reasoning_With_Reasoning.mp3
   </guid>
<itunes:episode>
    26
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The whistleblower's dilemma and the fairness-loyalty tradeoff
   </title>
<itunes:title>
    The whistleblower's dilemma and the fairness-loyalty tradeoff
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
     The tradeoff between fairness and loyalty corresponds to whistleblowing decisions.  Experimental and dispositional variation in this tradeoff maps onto whistleblowing.  Five studies demonstrate this previously undocumented relationship.  These results shed light on a novel psychological determinant of whistleblowing. a b s t r a c t a r t i c l e i n f o Whistleblowing -reporting another person's unethical behavior to a third party -often constitutes a conflict between competing moral concerns. Whistleblowing promotes justice and fairness but can also appear disloyal. Five studies demonstrate that a fairness-loyalty tradeoff predicts people's willingness to blow the whistle. Study 1 demonstrates that individual differences in valuing fairness over loyalty predict willingness to report unethical behavior. Studies 2a and 2b demonstrate that experimentally manipulating endorsement of fairness versus loyalty increases willingness to report unethical behavior. Study 3 demonstrates that people recall their decisions to report unethical behavior as driven by valuation of fairness, whereas people recall decisions not to report unethical behavior as driven by valuation of loyalty. Study 4 demonstrates that experimentally manipulating the endorsement of fairness versus loyalty increases whistleblowing in an online marketplace. These findings reveal the psychological determinants of whistleblowing and shed light on factors that encourage or discourage this practice.
   </itunes:summary>
<description>
     The tradeoff between fairness and loyalty corresponds to whistleblowing decisions.  Experimental and dispositional variation in this tradeoff maps onto whistleblowing.  Five studies demonstrate this previously undocumented relationship.  These results shed light on a novel psychological determinant of whistleblowing. a b s t r a c t a r t i c l e i n f o Whistleblowing -reporting another person's unethical behavior to a third party -often constitutes a conflict between competing moral concerns. Whistleblowing promotes justice and fairness but can also appear disloyal. Five studies demonstrate that a fairness-loyalty tradeoff predicts people's willingness to blow the whistle. Study 1 demonstrates that individual differences in valuing fairness over loyalty predict willingness to report unethical behavior. Studies 2a and 2b demonstrate that experimentally manipulating endorsement of fairness versus loyalty increases willingness to report unethical behavior. Study 3 demonstrates that people recall their decisions to report unethical behavior as driven by valuation of fairness, whereas people recall decisions not to report unethical behavior as driven by valuation of loyalty. Study 4 demonstrates that experimentally manipulating the endorsement of fairness versus loyalty increases whistleblowing in an online marketplace. These findings reveal the psychological determinants of whistleblowing and shed light on factors that encourage or discourage this practice.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0022103113001352-main.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2721.01875
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0022103113001352-main.mp3
   </guid>
<itunes:episode>
    27
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Breaking Up is (Relatively) Easy to Do: A Script for the Dissolution of Close Relationships
   </title>
<itunes:title>
    Breaking Up is (Relatively) Easy to Do: A Script for the Dissolution of Close Relationships
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/0265407598156007.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6.45225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/0265407598156007.mp3
   </guid>
<itunes:episode>
    28
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Probing Pre-Trained Language Models for Cross-Cultural Differences in Values
   </title>
<itunes:title>
    Probing Pre-Trained Language Models for Cross-Cultural Differences in Values
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.
   </itunes:summary>
<description>
    Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13722v1.Probing_Pre_Trained_Language_Models_for_Cross_Cultural_Differences_in_Values.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1589.76
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13722v1.Probing_Pre_Trained_Language_Models_for_Cross_Cultural_Differences_in_Values.mp3
   </guid>
<itunes:episode>
    29
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Transformers learn in-context by gradient descent *
   </title>
<itunes:title>
    Transformers learn in-context by gradient descent *
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Transformers have become the state-of-the-art neural network architecture across numerous domains of machine learning. This is partly due to their celebrated ability to transfer and to learn in-context based on few examples. Nevertheless, the mechanisms by which Transformers become in-context learners are not well understood and remain mostly an intuition. Here, we argue that training Transformers on auto-regressive tasks can be closely related to wellknown gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers implement gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of optimized Transformers that learn in-context. Furthermore, we identify how Transformers surpass plain gradient descent by an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022)  and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.
   </itunes:summary>
<description>
    Transformers have become the state-of-the-art neural network architecture across numerous domains of machine learning. This is partly due to their celebrated ability to transfer and to learn in-context based on few examples. Nevertheless, the mechanisms by which Transformers become in-context learners are not well understood and remain mostly an intuition. Here, we argue that training Transformers on auto-regressive tasks can be closely related to wellknown gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers implement gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of optimized Transformers that learn in-context. Furthermore, we identify how Transformers surpass plain gradient descent by an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022)  and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.07677v1.Transformers_learn_in_context_by_gradient_descent.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3863.40575
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.07677v1.Transformers_learn_in_context_by_gradient_descent.mp3
   </guid>
<itunes:episode>
    30
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets
   </title>
<itunes:title>
    Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.
   </itunes:summary>
<description>
    Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12942v1.Generating_Data_to_Mitigate_Spurious_Correlations_in_Natural_Language_Inference_Datasets.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2826.39675
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12942v1.Generating_Data_to_Mitigate_Spurious_Correlations_in_Natural_Language_Inference_Datasets.mp3
   </guid>
<itunes:episode>
    31
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Multiple moral foundations predict responses to sacrificial dilemmas
   </title>
<itunes:title>
    Multiple moral foundations predict responses to sacrificial dilemmas
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Moral dilemmas, by definition, demand trade-offs between competing moral goods (e.g., causing one harm to prevent another). Although moral dilemmas have served as a methodological pillar for moral psychology, surprisingly little research has explored how individual differences in moral values influence responses to dilemmatic trade-offs. In a cross-sectional study (N = 307), we tested competing claims regarding the relationship between the endorsement of foundational moral values and responses to sacrificial dilemmas, in which one judges the moral acceptability of causing fatal harm to one person to save multiple others. Inconsistent with Moral Dyad Theory, our results did not support the prediction that Harm concerns would be the unequivocally most important predictor of sacrifice endorsement. Consistent with Moral Foundations Theory, however, multiple moral values are predictive of sacrifice judgments: Harm and Purity negatively predict, and Ingroup positively predicts, endorsement of harmful action in service of saving lives, with Harm and Purity explaining similar amounts of unique variance. The present study demonstrates the utility of pluralistic accounts of morality, even in moral situations in which harm is central. Crown
   </itunes:summary>
<description>
    Moral dilemmas, by definition, demand trade-offs between competing moral goods (e.g., causing one harm to prevent another). Although moral dilemmas have served as a methodological pillar for moral psychology, surprisingly little research has explored how individual differences in moral values influence responses to dilemmatic trade-offs. In a cross-sectional study (N = 307), we tested competing claims regarding the relationship between the endorsement of foundational moral values and responses to sacrificial dilemmas, in which one judges the moral acceptability of causing fatal harm to one person to save multiple others. Inconsistent with Moral Dyad Theory, our results did not support the prediction that Harm concerns would be the unequivocally most important predictor of sacrifice endorsement. Consistent with Moral Foundations Theory, however, multiple moral values are predictive of sacrifice judgments: Harm and Purity negatively predict, and Ingroup positively predicts, endorsement of harmful action in service of saving lives, with Harm and Purity explaining similar amounts of unique variance. The present study demonstrates the utility of pluralistic accounts of morality, even in moral situations in which harm is central. Crown
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886915003049-main.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2000.90125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886915003049-main.mp3
   </guid>
<itunes:episode>
    32
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    B-cos Networks: Alignment is All We Need for Interpretability
   </title>
<itunes:title>
    B-cos Networks: Alignment is All We Need for Interpretability
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at https://www.github.com/moboehle/B-cos.
   </itunes:summary>
<description>
    We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at https://www.github.com/moboehle/B-cos.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10268v1.B_cos_Networks_Alignment_is_All_We_Need_for_Interpretability.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3974.191
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10268v1.B_cos_Networks_Alignment_is_All_We_Need_for_Interpretability.mp3
   </guid>
<itunes:episode>
    33
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors
   </title>
<itunes:title>
    Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.
   </itunes:summary>
<description>
    Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10279v1.Pre_Train_Your_Loss_Easy_Bayesian_Transfer_Learning_with_Informative_Priors.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2752.47025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10279v1.Pre_Train_Your_Loss_Easy_Bayesian_Transfer_Learning_with_Informative_Priors.mp3
   </guid>
<itunes:episode>
    34
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization
   </title>
<itunes:title>
    Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.
   </itunes:summary>
<description>
    Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.05090v1.Semi_supervised_Formality_Style_Transfer_using_Language_Model_Discriminator_and_Mutual_Information_Maximization.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2673.11025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.05090v1.Semi_supervised_Formality_Style_Transfer_using_Language_Model_Discriminator_and_Mutual_Information_Maximization.mp3
   </guid>
<itunes:episode>
    35
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures
   </title>
<itunes:title>
    The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Although numerous models attempted to explain the nature of moral judgment, moral foundations theory (MFT) led to a paradigmatic change in this field by proposing pluralist "moralities" (care, fairness, loyalty, authority, sanctity). The five-factor structure of MFT is thought to be universal and rooted in the evolutionary past but the evidence is scarce regarding the stability of this five-factor structure across diverse cultures. We tested this universality argument in a cross-cultural dataset of 30 diverse societies spanning the WEIRD (Western, educated, industrialized, rich, democratic) and non-WEIRD cultures by testing measurement invariance of the short-form of the moral foundations questionnaire. The results supported the original conceptualization that there are at least five diverse moralities although loadings of items differ across WEIRD and non-WEIRD cultures. In other words, the current research shows for the first time that the five-factor structure of MFT is stable in the WEIRD and non-WEIRD cultures.
   </itunes:summary>
<description>
    Although numerous models attempted to explain the nature of moral judgment, moral foundations theory (MFT) led to a paradigmatic change in this field by proposing pluralist "moralities" (care, fairness, loyalty, authority, sanctity). The five-factor structure of MFT is thought to be universal and rooted in the evolutionary past but the evidence is scarce regarding the stability of this five-factor structure across diverse cultures. We tested this universality argument in a cross-cultural dataset of 30 diverse societies spanning the WEIRD (Western, educated, industrialized, rich, democratic) and non-WEIRD cultures by testing measurement invariance of the short-form of the moral foundations questionnaire. The results supported the original conceptualization that there are at least five diverse moralities although loadings of items differ across WEIRD and non-WEIRD cultures. In other words, the current research shows for the first time that the five-factor structure of MFT is stable in the WEIRD and non-WEIRD cultures.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886919304799-main.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1921.12325
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886919304799-main.mp3
   </guid>
<itunes:episode>
    36
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Belief-based Generation of Argumentative Claims
   </title>
<itunes:title>
    Belief-based Generation of Argumentative Claims
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    When engaging in argumentative discourse, skilled human debaters tailor claims to the beliefs of the audience, to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we additionally evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances, but demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach.
   </itunes:summary>
<description>
    When engaging in argumentative discourse, skilled human debaters tailor claims to the beliefs of the audience, to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we additionally evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances, but demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.09765v2.Belief_based_Generation_of_Argumentative_Claims.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2260.63675
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.09765v2.Belief_based_Generation_of_Argumentative_Claims.mp3
   </guid>
<itunes:episode>
    37
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation in RICE-N
   </title>
<itunes:title>
    AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation in RICE-N
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Comprehensive global cooperation is essential to limit global temperature increases while continuing economic development, e.g., reducing severe inequality or achieving long-term economic growth. Achieving long-term cooperation on climate change mitigation with n strategic agents poses a complex game-theoretic problem. For example, agents may negotiate and reach climate agreements, but there is no central authority to enforce adherence to those agreements. Hence, it is critical to design negotiation and agreement frameworks that foster cooperation, allow all agents to meet their individual policy objectives, and incentivize long-term adherence. This is an interdisciplinary challenge that calls for collaboration between researchers in machine learning, economics, climate science, law, policy, ethics, and other fields. In particular, we argue that machine learning is a critical tool to address the complexity of this domain. To facilitate this research, here we introduce RICE-N, a multi-region integrated assessment model that simulates the global climate and economy, and which can be used to design and evaluate the strategic outcomes for different negotiation and agreement frameworks. We also describe how to use multi-agent reinforcement learning to train rational agents using RICE-N. This framework underpinsAI for Global Climate Cooperation, a working group collaboration and competition on climate negotiation and agreement design. Here, we invite the scientific community to design and evaluate their solutions using RICE-N, machine learning, economic intuition, and other domain knowledge. More information can be found on www.ai4climatecoop.org.
   </itunes:summary>
<description>
    Comprehensive global cooperation is essential to limit global temperature increases while continuing economic development, e.g., reducing severe inequality or achieving long-term economic growth. Achieving long-term cooperation on climate change mitigation with n strategic agents poses a complex game-theoretic problem. For example, agents may negotiate and reach climate agreements, but there is no central authority to enforce adherence to those agreements. Hence, it is critical to design negotiation and agreement frameworks that foster cooperation, allow all agents to meet their individual policy objectives, and incentivize long-term adherence. This is an interdisciplinary challenge that calls for collaboration between researchers in machine learning, economics, climate science, law, policy, ethics, and other fields. In particular, we argue that machine learning is a critical tool to address the complexity of this domain. To facilitate this research, here we introduce RICE-N, a multi-region integrated assessment model that simulates the global climate and economy, and which can be used to design and evaluate the strategic outcomes for different negotiation and agreement frameworks. We also describe how to use multi-agent reinforcement learning to train rational agents using RICE-N. This framework underpinsAI for Global Climate Cooperation, a working group collaboration and competition on climate negotiation and agreement design. Here, we invite the scientific community to design and evaluate their solutions using RICE-N, machine learning, economic intuition, and other domain knowledge. More information can be found on www.ai4climatecoop.org.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.07004v1.AI_for_Global_Climate_Cooperation_Modeling_Global_Climate_Negotiations_Agreements_and_Long_Term_Cooperation_in_RICE_N.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5695.84325
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.07004v1.AI_for_Global_Climate_Cooperation_Modeling_Global_Climate_Negotiations_Agreements_and_Long_Term_Cooperation_in_RICE_N.mp3
   </guid>
<itunes:episode>
    38
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation
   </title>
<itunes:title>
    BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.
   </itunes:summary>
<description>
    Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.11718v1.BOLD_Dataset_and_Metrics_for_Measuring_Biases_in_Open_Ended_Language_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3449.86125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.11718v1.BOLD_Dataset_and_Metrics_for_Measuring_Biases_in_Open_Ended_Language_Generation.mp3
   </guid>
<itunes:episode>
    39
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Fast and Scalable Structural SVM with Slack Rescaling
   </title>
<itunes:title>
    Fast and Scalable Structural SVM with Slack Rescaling
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.
   </itunes:summary>
<description>
    We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1173.133
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3
   </guid>
<itunes:episode>
    40
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    College and the Culture War: Assessing Higher Educations Influence on Moral Attitudes
   </title>
<itunes:title>
    College and the Culture War: Assessing Higher Educations Influence on Moral Attitudes
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Moral differences contribute to social and political conflicts. Against this backdrop, colleges and universities have been criticized for promoting liberal moral attitudes. However, direct evidence for these claims is sparse, and suggestive evidence from studies of political attitudes is inconclusive. Using four waves of data from the National Study of Youth and Religion, we examine the effects of higher education on attitudes related to three dimensions of morality that have been identified as central to conflict: moral relativism, concern for others, and concern for social order. Our results indicate that higher education liberalizes moral concerns for most students, but it also departs from the standard liberal profile by promoting moral absolutism rather than relativism. These effects are strongest for individuals majoring in the humanities, arts, or social sciences, and for students pursuing graduate studies. We conclude with a discussion of the implications of our results for work on political conflict and moral socialization.
   </itunes:summary>
<description>
    Moral differences contribute to social and political conflicts. Against this backdrop, colleges and universities have been criticized for promoting liberal moral attitudes. However, direct evidence for these claims is sparse, and suggestive evidence from studies of political attitudes is inconclusive. Using four waves of data from the National Study of Youth and Religion, we examine the effects of higher education on attitudes related to three dimensions of morality that have been identified as central to conflict: moral relativism, concern for others, and concern for social order. Our results indicate that higher education liberalizes moral concerns for most students, but it also departs from the standard liberal profile by promoting moral absolutism rather than relativism. These effects are strongest for individuals majoring in the humanities, arts, or social sciences, and for students pursuing graduate studies. We conclude with a discussion of the implications of our results for work on political conflict and moral socialization.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/00031224211041094.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6928.7705
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/00031224211041094.mp3
   </guid>
<itunes:episode>
    41
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    MetaFormer is Actually What You Need for Vision
   </title>
<itunes:title>
    MetaFormer is Actually What You Need for Vision
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer
   </itunes:summary>
<description>
    Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2111.11418v2.MetaFormer_is_Actually_What_You_Need_for_Vision.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2160.06525
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2111.11418v2.MetaFormer_is_Actually_What_You_Need_for_Vision.mp3
   </guid>
<itunes:episode>
    42
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Five sources of bias in natural language processing
   </title>
<itunes:title>
    Five sources of bias in natural language processing
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.
   </itunes:summary>
<description>
    Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Language and Linguist Compass - 2021 - Hovy - Five sources of bias in natural language processing.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2844.21225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Language and Linguist Compass - 2021 - Hovy - Five sources of bias in natural language processing.mp3
   </guid>
<itunes:episode>
    43
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    OPT: Open Pre-trained Transformer Language Models
   </title>
<itunes:title>
    OPT: Open Pre-trained Transformer Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.
   </itunes:summary>
<description>
    Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.01068v3.OPT_Open_Pre_trained_Transformer_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3418.9845
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.01068v3.OPT_Open_Pre_trained_Transformer_Language_Models.mp3
   </guid>
<itunes:episode>
    44
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning
   </title>
<itunes:title>
    Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.
   </itunes:summary>
<description>
    We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.04325v1.Will_we_run_out_of_data_An_analysis_of_the_limits_of_scaling_datasets_in_Machine_Learning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2186.81475
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.04325v1.Will_we_run_out_of_data_An_analysis_of_the_limits_of_scaling_datasets_in_Machine_Learning.mp3
   </guid>
<itunes:episode>
    45
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem
   </title>
<itunes:title>
    Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.
   </itunes:summary>
<description>
    We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05990v1.Detection_Disambiguation_Re_ranking_Autoregressive_Entity_Linking_as_a_Multi_Task_Problem.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2432.05225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05990v1.Detection_Disambiguation_Re_ranking_Autoregressive_Entity_Linking_as_a_Multi_Task_Problem.mp3
   </guid>
<itunes:episode>
    46
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Fine-tuning language models to find agreement among humans with diverse preferences
   </title>
<itunes:title>
    Fine-tuning language models to find agreement among humans with diverse preferences
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (&gt; 70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (&gt; 65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another. * Authors contributed equally to this work
   </itunes:summary>
<description>
    Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (&gt; 70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (&gt; 65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another. * Authors contributed equally to this work
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.15006v1.Fine_tuning_language_models_to_find_agreement_among_humans_with_diverse_preferences.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5809.97225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.15006v1.Fine_tuning_language_models_to_find_agreement_among_humans_with_diverse_preferences.mp3
   </guid>
<itunes:episode>
    47
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    What Makes Data-to-Text Generation Hard for Pretrained Language Models?
   </title>
<itunes:title>
    What Makes Data-to-Text Generation Hard for Pretrained Language Models?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Expressing natural language descriptions of structured facts or relations -- data-to-text generation (D2T) -- increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models(PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T.   In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how these data are incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.
   </itunes:summary>
<description>
    Expressing natural language descriptions of structured facts or relations -- data-to-text generation (D2T) -- increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models(PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T.   In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how these data are incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.11505v1.What_Makes_Data_to_Text_Generation_Hard_for_Pretrained_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2205.11505v1.What_Makes_Data_to_Text_Generation_Hard_for_Pretrained_Language_Models.mp3.vtt"/>
<itunes:duration>
    2351.376
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.11505v1.What_Makes_Data_to_Text_Generation_Hard_for_Pretrained_Language_Models.mp3
   </guid>
<itunes:episode>
    48
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities
   </title>
<itunes:title>
    A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples still remains a serious challenge. In this context, we extensively investigated 200+ latest papers on FSL published in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL along with impartial comparisons of the strengths and weaknesses of the existing works. For the sake of avoiding conceptual confusion, we first elaborate and compare a set of similar concepts including few-shot learning, transfer learning, and meta-learning. Furthermore, we propose a novel taxonomy to classify the existing work according to the level of abstraction of knowledge in accordance with the challenges of FSL. To enrich this survey, in each subsection we provide in-depth analysis and insightful discussion about recent advances on these topics. Moreover, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into the technology evolution trends together with potential future research opportunities in the hope of providing guidance to follow-up research.
   </itunes:summary>
<description>
    Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples still remains a serious challenge. In this context, we extensively investigated 200+ latest papers on FSL published in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL along with impartial comparisons of the strengths and weaknesses of the existing works. For the sake of avoiding conceptual confusion, we first elaborate and compare a set of similar concepts including few-shot learning, transfer learning, and meta-learning. Furthermore, we propose a novel taxonomy to classify the existing work according to the level of abstraction of knowledge in accordance with the challenges of FSL. To enrich this survey, in each subsection we provide in-depth analysis and insightful discussion about recent advances on these topics. Moreover, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into the technology evolution trends together with potential future research opportunities in the hope of providing guidance to follow-up research.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06743v1.A_Comprehensive_Survey_of_Few_shot_Learning_Evolution_Applications_Challenges_and_Opportunities.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5407.6865
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06743v1.A_Comprehensive_Survey_of_Few_shot_Learning_Evolution_Applications_Challenges_and_Opportunities.mp3
   </guid>
<itunes:episode>
    49
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Doc2Dict: Information Extraction as Text Generation
   </title>
<itunes:title>
    Doc2Dict: Information Extraction as Text Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2235.2195
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3
   </guid>
<itunes:episode>
    50
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population
   </title>
<itunes:title>
    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in http://deepke.openkg.cn/EN/re_doc_show.html for real-time extraction of various tasks, and a demo video.
   </itunes:summary>
<description>
    We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in http://deepke.openkg.cn/EN/re_doc_show.html for real-time extraction of various tasks, and a demo video.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v5.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1777.92
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v5.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3
   </guid>
<itunes:episode>
    51
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims
   </title>
<itunes:title>
    Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.
   </itunes:summary>
<description>
    With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2004.07213v2.Toward_Trustworthy_AI_Development_Mechanisms_for_Supporting_Verifiable_Claims.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    10216.986
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2004.07213v2.Toward_Trustworthy_AI_Development_Mechanisms_for_Supporting_Verifiable_Claims.mp3
   </guid>
<itunes:episode>
    52
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do
   </title>
<itunes:title>
    Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended state of the art for many NLP tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show that recent LMs also contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society -- they bring a "moral direction" to surface. That is, we show that these norms can be captured geometrically by a direction, which can be computed, e.g., by a PCA, in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts and providing a path for attenuating or even preventing toxic degeneration in LMs. Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.
   </itunes:summary>
<description>
    Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended state of the art for many NLP tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show that recent LMs also contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society -- they bring a "moral direction" to surface. That is, we show that these norms can be captured geometrically by a direction, which can be computed, e.g., by a PCA, in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts and providing a path for attenuating or even preventing toxic degeneration in LMs. Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11790v3.Large_Pre_trained_Language_Models_Contain_Human_like_Biases_of_What_is_Right_and_Wrong_to_Do.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    8467.38275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11790v3.Large_Pre_trained_Language_Models_Contain_Human_like_Biases_of_What_is_Right_and_Wrong_to_Do.mp3
   </guid>
<itunes:episode>
    53
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Article 34 Citation: Card D and Smith NA (2020) On Consequentialism and Fairness. Front
   </title>
<itunes:title>
    Article 34 Citation: Card D and Smith NA (2020) On Consequentialism and Fairness. Front
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage "fair" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.
   </itunes:summary>
<description>
    Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage "fair" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frai-03-00034.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3851.25875
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frai-03-00034.mp3
   </guid>
<itunes:episode>
    54
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    When Life Gives You Lemons , Make Cherryade : Converting Feedback from Bad Responses into Good Labels
   </title>
<itunes:title>
    When Life Gives You Lemons , Make Cherryade : Converting Feedback from Bad Responses into Good Labels
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose JUICER, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed DIRECTOR model.
   </itunes:summary>
<description>
    Deployed dialogue agents have the potential to integrate human feedback to continuously improve themselves. However, humans may not always provide explicit signals when the chatbot makes mistakes during interactions. In this work, we propose JUICER, a framework to make use of both binary and free-form textual human feedback. It works by: (i) extending sparse binary feedback by training a satisfaction classifier to label the unlabeled data; and (ii) training a reply corrector to map the bad replies to good ones. We find that augmenting training with model-corrected replies improves the final dialogue model, and we can further improve performance by using both positive and negative replies through the recently proposed DIRECTOR model.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.15893v1.When_Life_Gives_You_Lemons_Make_Cherryade_Converting_Feedback_from_Bad_Responses_into_Good_Labels.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3176.17625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.15893v1.When_Life_Gives_You_Lemons_Make_Cherryade_Converting_Feedback_from_Bad_Responses_into_Good_Labels.mp3
   </guid>
<itunes:episode>
    55
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Attribute Alignment: Controlling Text Generation from Pre-trained Language Models
   </title>
<itunes:title>
    Attribute Alignment: Controlling Text Generation from Pre-trained Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity.
   </itunes:summary>
<description>
    Large language models benefit from training with a large amount of unlabeled text, which gives them increasingly fluent and diverse generation capabilities. However, using these models for text generation that takes into account target attributes, such as sentiment polarity or specific topics, remains a challenge. We propose a simple and flexible method for controlling text generation by aligning disentangled attribute representations. In contrast to recent efforts on training a discriminator to perturb the token level distribution for an attribute, we use the same data to learn an alignment function to guide the pre-trained, non-controlled language model to generate texts with the target attribute without changing the original language model parameters. We evaluate our method on sentiment- and topic-controlled generation, and show large performance gains over previous methods while retaining fluency and diversity.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11070v2.Attribute_Alignment_Controlling_Text_Generation_from_Pre_trained_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3672.6595
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11070v2.Attribute_Alignment_Controlling_Text_Generation_from_Pre_trained_Language_Models.mp3
   </guid>
<itunes:episode>
    56
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Unifying Data Perspectivism and Personalization: An Application to Social Norms
   </title>
<itunes:title>
    Unifying Data Perspectivism and Personalization: An Application to Social Norms
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.
   </itunes:summary>
<description>
    Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.14531v2.Unifying_Data_Perspectivism_and_Personalization_An_Application_to_Social_Norms.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2598.89625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.14531v2.Unifying_Data_Perspectivism_and_Personalization_An_Application_to_Social_Norms.mp3
   </guid>
<itunes:episode>
    57
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Non-Parametric Class Completeness Estimators for Collaborative Knowledge Graphs -- The Case of Wikidata
   </title>
<itunes:title>
    Non-Parametric Class Completeness Estimators for Collaborative Knowledge Graphs -- The Case of Wikidata
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Collaborative Knowledge Graph platforms allow humans and automated scripts to collaborate in creating, updating and interlinking entities and facts. To ensure both the completeness of the data as well as a uniform coverage of the different topics, it is crucial to identify underrepresented classes in the Knowledge Graph. In this paper, we tackle this problem by developing statistical techniques for class cardinality estimation in collaborative Knowledge Graph platforms. Our method is able to estimate the completeness of a class - as defined by a schema or ontology - hence can be used to answer questions such as "Does the knowledge base have a complete list of all {Beer Brands|Volcanos|Video Game Consoles}?" As a use-case, we focus on Wikidata, which poses unique challenges in terms of the size of its ontology, the number of users actively populating its graph, and its extremely dynamic nature. Our techniques are derived from species estimation and data-management methodologies, and are applied to the case of graphs and collaborative editing. In our empirical evaluation, we observe that i) the number and frequency of unique class instances drastically influence the performance of an estimator, ii) bursts of inserts cause some estimators to overestimate the true size of the class if they are not properly handled, and iii) one can effectively measure the convergence of a class towards its true size by considering the stability of an estimator against the number of available instances.
   </itunes:summary>
<description>
    Collaborative Knowledge Graph platforms allow humans and automated scripts to collaborate in creating, updating and interlinking entities and facts. To ensure both the completeness of the data as well as a uniform coverage of the different topics, it is crucial to identify underrepresented classes in the Knowledge Graph. In this paper, we tackle this problem by developing statistical techniques for class cardinality estimation in collaborative Knowledge Graph platforms. Our method is able to estimate the completeness of a class - as defined by a schema or ontology - hence can be used to answer questions such as "Does the knowledge base have a complete list of all {Beer Brands|Volcanos|Video Game Consoles}?" As a use-case, we focus on Wikidata, which poses unique challenges in terms of the size of its ontology, the number of users actively populating its graph, and its extremely dynamic nature. Our techniques are derived from species estimation and data-management methodologies, and are applied to the case of graphs and collaborative editing. In our empirical evaluation, we observe that i) the number and frequency of unique class instances drastically influence the performance of an estimator, ii) bursts of inserts cause some estimators to overestimate the true size of the class if they are not properly handled, and iii) one can effectively measure the convergence of a class towards its true size by considering the stability of an estimator against the number of available instances.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1909.01109v1.Non_Parametric_Class_Completeness_Estimators_for_Collaborative_Knowledge_Graphs_The_Case_of_Wikidata.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2411.46775
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1909.01109v1.Non_Parametric_Class_Completeness_Estimators_for_Collaborative_Knowledge_Graphs_The_Case_of_Wikidata.mp3
   </guid>
<itunes:episode>
    58
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Evaluating the Knowledge Dependency of Questions
   </title>
<itunes:title>
    Evaluating the Knowledge Dependency of Questions
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ to the gold sample in the dataset and disregard their educational value. They fail to evaluate the MCQ's ability to assess the student's knowledge of the corresponding target fact. To tackle this issue, we propose a novel automatic evaluation metric, coined Knowledge Dependent Answerability (KDA), which measures the MCQ's answerability given knowledge of the target fact. Specifically, we first show how to measure KDA based on student responses from a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and KDA_cont, that approximate KDA by leveraging pre-trained language models to imitate students' problem-solving behavior. Through our human studies, we show that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2) usability in an actual classroom setting, labeled by experts. Furthermore, when combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown to have a strong predictive power for various expert-labeled MCQ quality measures.
   </itunes:summary>
<description>
    The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ to the gold sample in the dataset and disregard their educational value. They fail to evaluate the MCQ's ability to assess the student's knowledge of the corresponding target fact. To tackle this issue, we propose a novel automatic evaluation metric, coined Knowledge Dependent Answerability (KDA), which measures the MCQ's answerability given knowledge of the target fact. Specifically, we first show how to measure KDA based on student responses from a human survey. Then, we propose two automatic evaluation metrics, KDA_disc and KDA_cont, that approximate KDA by leveraging pre-trained language models to imitate students' problem-solving behavior. Through our human studies, we show that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2) usability in an actual classroom setting, labeled by experts. Furthermore, when combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown to have a strong predictive power for various expert-labeled MCQ quality measures.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.11902v1.Evaluating_the_Knowledge_Dependency_of_Questions.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2450.6515
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.11902v1.Evaluating_the_Knowledge_Dependency_of_Questions.mp3
   </guid>
<itunes:episode>
    59
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Language of Liberty: A preliminary study
   </title>
<itunes:title>
    The Language of Liberty: A preliminary study
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Quantifying the moral narratives expressed in the user-generated text, news, or public discourses is fundamental for understanding individuals' concerns and viewpoints and preventing violent protests and social polarisation. The Moral Foundation Theory (MFT) was developed precisely to operationalise morality in a five-dimensional scale system. Recent developments of the theory urged for the introduction of a new foundation, liberty. Being only recently added to the theory, there are no available linguistic resources to assess liberty from text corpora. Given its importance to current social issues such as the vaccination debate, we propose a data-driven approach to derive a liberty lexicon based on aligned documents from online encyclopedias with different worldviews. Despite the preliminary nature of our study, we show proof of the concept that large encyclopedia corpora can point out differences in the way people with contrasting viewpoints express themselves. Such differences can be used to derive a novel lexicon, identifying linguistic markers of the liberty foundation.
   </itunes:summary>
<description>
    Quantifying the moral narratives expressed in the user-generated text, news, or public discourses is fundamental for understanding individuals' concerns and viewpoints and preventing violent protests and social polarisation. The Moral Foundation Theory (MFT) was developed precisely to operationalise morality in a five-dimensional scale system. Recent developments of the theory urged for the introduction of a new foundation, liberty. Being only recently added to the theory, there are no available linguistic resources to assess liberty from text corpora. Given its importance to current social issues such as the vaccination debate, we propose a data-driven approach to derive a liberty lexicon based on aligned documents from online encyclopedias with different worldviews. Despite the preliminary nature of our study, we show proof of the concept that large encyclopedia corpora can point out differences in the way people with contrasting viewpoints express themselves. Such differences can be used to derive a novel lexicon, identifying linguistic markers of the liberty foundation.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/The_Language_of_Liberty (1).mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    895.765
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/The_Language_of_Liberty (1).mp3
   </guid>
<itunes:episode>
    60
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Highly Parallel Autoregressive Entity Linking with Discriminative Correction
   </title>
<itunes:title>
    Highly Parallel Autoregressive Entity Linking with Discriminative Correction
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is &gt;70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL
   </itunes:summary>
<description>
    Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is &gt;70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.03792v1.Highly_Parallel_Autoregressive_Entity_Linking_with_Discriminative_Correction.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1345.933
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.03792v1.Highly_Parallel_Autoregressive_Entity_Linking_with_Discriminative_Correction.mp3
   </guid>
<itunes:episode>
    61
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Human Annotated Dialogues Dataset for Natural Conversational Agents
   </title>
<itunes:title>
    Human Annotated Dialogues Dataset for Natural Conversational Agents
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Abstract: Conversational agents are gaining huge popularity in industrial applications such as digital assistants, chatbots, and particularly systems for natural language understanding (NLU). However, a major drawback is the unavailability of a common metric to evaluate the replies against human judgement for conversational agents. In this paper, we develop a benchmark dataset with human annotations and diverse replies that can be used to develop such metric for conversational agents. The paper introduces a high-quality human annotated movie dialogue dataset, HUMOD, that is developed from the Cornell movie dialogues dataset. This new dataset comprises 28,500 human responses from 9500 multi-turn dialogue history-reply pairs. Human responses include: (i) ratings of the dialogue reply in relevance to the dialogue history; and (ii) unique dialogue replies for each dialogue history from the users. Such unique dialogue replies enable researchers in evaluating their models against six unique human responses for each given history. Detailed analysis on how dialogues are structured and human perception on dialogue score in comparison with existing models are also presented.
   </itunes:summary>
<description>
    Abstract: Conversational agents are gaining huge popularity in industrial applications such as digital assistants, chatbots, and particularly systems for natural language understanding (NLU). However, a major drawback is the unavailability of a common metric to evaluate the replies against human judgement for conversational agents. In this paper, we develop a benchmark dataset with human annotations and diverse replies that can be used to develop such metric for conversational agents. The paper introduces a high-quality human annotated movie dialogue dataset, HUMOD, that is developed from the Cornell movie dialogues dataset. This new dataset comprises 28,500 human responses from 9500 multi-turn dialogue history-reply pairs. Human responses include: (i) ratings of the dialogue reply in relevance to the dialogue history; and (ii) unique dialogue replies for each dialogue history from the users. Such unique dialogue replies enable researchers in evaluating their models against six unique human responses for each given history. Detailed analysis on how dialogues are structured and human perception on dialogue score in comparison with existing models are also presented.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/9dcdd0d887c9d2fc9e1924e8427a35f263cd.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2092.40825
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/9dcdd0d887c9d2fc9e1924e8427a35f263cd.mp3
   </guid>
<itunes:episode>
    62
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Towards Understanding Grokking: An Effective Theory of Representation Learning
   </title>
<itunes:title>
    Towards Understanding Grokking: An Effective Theory of Representation Learning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.
   </itunes:summary>
<description>
    We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10343v1.Towards_Understanding_Grokking_An_Effective_Theory_of_Representation_Learning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3217.8155
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10343v1.Towards_Understanding_Grokking_An_Effective_Theory_of_Representation_Learning.mp3
   </guid>
<itunes:episode>
    63
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Learning Transferable Visual Models From Natural Language Supervision
   </title>
<itunes:title>
    Learning Transferable Visual Models From Natural Language Supervision
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.
   </itunes:summary>
<description>
    State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00020v1.Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    9481.24725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00020v1.Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.mp3
   </guid>
<itunes:episode>
    64
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    How to represent part-whole hierarchies in a neural network
   </title>
<itunes:title>
    How to represent part-whole hierarchies in a neural network
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language
   </itunes:summary>
<description>
    This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.12627v1.How_to_represent_part_whole_hierarchies_in_a_neural_network.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5856.444
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.12627v1.How_to_represent_part_whole_hierarchies_in_a_neural_network.mp3
   </guid>
<itunes:episode>
    65
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral Dilemmas for Moral Machines
   </title>
<itunes:title>
    Moral Dilemmas for Moral Machines
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Autonomous systems are being developed and deployed in situations that may require some degree of ethical decision-making ability. As a result, research in machine ethics has proliferated in recent years. This work has included using moral dilemmas as validation mechanisms for implementing decision-making algorithms in ethically-loaded situations. Using trolley-style problems in the context of autonomous vehicles as a case study, I argue (1) that this is a misapplication of philosophical thought experiments because (2) it fails to appreciate the purpose of moral dilemmas, and (3) this has potentially catastrophic consequences; however, (4) there are uses of moral dilemmas in machine ethics that are appropriate and the novel situations that arise in a machine-learning context can shed some light on philosophical work in ethics.
   </itunes:summary>
<description>
    Autonomous systems are being developed and deployed in situations that may require some degree of ethical decision-making ability. As a result, research in machine ethics has proliferated in recent years. This work has included using moral dilemmas as validation mechanisms for implementing decision-making algorithms in ethically-loaded situations. Using trolley-style problems in the context of autonomous vehicles as a case study, I argue (1) that this is a misapplication of philosophical thought experiments because (2) it fails to appreciate the purpose of moral dilemmas, and (3) this has potentially catastrophic consequences; however, (4) there are uses of moral dilemmas in machine ethics that are appropriate and the novel situations that arise in a machine-learning context can shed some light on philosophical work in ethics.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.06152v1.Moral_Dilemmas_for_Moral_Machines.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2189.244
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.06152v1.Moral_Dilemmas_for_Moral_Machines.mp3
   </guid>
<itunes:episode>
    66
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos
   </title>
<itunes:title>
    Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.
   </itunes:summary>
<description>
    Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2206.11795v1.Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2206.11795v1.Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.mp3.vtt"/>
<itunes:duration>
    5748.948
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2206.11795v1.Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.mp3
   </guid>
<itunes:episode>
    67
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Disunity of Moral Judgment: Implications for the Study of Psychopathy
   </title>
<itunes:title>
    The Disunity of Moral Judgment: Implications for the Study of Psychopathy
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Since the 18 th century, one of the key features of diagnosed psychopaths has been 'moral colorblindness' or an inability to form moral judgments. However, attempts at experimentally verifying this moral incapacity have been largely unsuccessful. After reviewing the centrality of 'moral colorblindness' to the study and diagnosis of psychopathy, I argue that the reason that researchers have been unable to verify that diagnosed psychopaths have an inability to make moral judgments is because their research is premised on the assumption that there is a specific moral faculty of the brain, or specific "moral" emotions, and that this faculty or set of emotions can become "impaired". I review recent research and argue that we have good reason to think that there is no such distinct capacity for moral judgment, and that, as a result, it is impossible for someone's "moral judgment faculty" to become selectively disabled. I then discuss the implications of such a position on psychopathy research, the coherence of the disorder, and the moral responsibility of psychopaths.
   </itunes:summary>
<description>
    Since the 18 th century, one of the key features of diagnosed psychopaths has been 'moral colorblindness' or an inability to form moral judgments. However, attempts at experimentally verifying this moral incapacity have been largely unsuccessful. After reviewing the centrality of 'moral colorblindness' to the study and diagnosis of psychopathy, I argue that the reason that researchers have been unable to verify that diagnosed psychopaths have an inability to make moral judgments is because their research is premised on the assumption that there is a specific moral faculty of the brain, or specific "moral" emotions, and that this faculty or set of emotions can become "impaired". I review recent research and argue that we have good reason to think that there is no such distinct capacity for moral judgment, and that, as a result, it is impossible for someone's "moral judgment faculty" to become selectively disabled. I then discuss the implications of such a position on psychopathy research, the coherence of the disorder, and the moral responsibility of psychopaths.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/C0F8EB08-771E-11ED-A204-B71E1F64EE82.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3544.947
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/C0F8EB08-771E-11ED-A204-B71E1F64EE82.mp3
   </guid>
<itunes:episode>
    68
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Faithful Reasoning Using Large Language Models
   </title>
<itunes:title>
    Faithful Reasoning Using Large Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.
   </itunes:summary>
<description>
    Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.14271v1.Faithful_Reasoning_Using_Large_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6772.66275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.14271v1.Faithful_Reasoning_Using_Large_Language_Models.mp3
   </guid>
<itunes:episode>
    69
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Generative Language Models for Paragraph-Level Question Generation
   </title>
<itunes:title>
    Generative Language Models for Paragraph-Level Question Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English. QG-Bench is released along with the fine-tuned models presented in the paper https://github.com/asahi417/lm-question-generation, which are also available as a demo https://autoqg.net/.
   </itunes:summary>
<description>
    Powerful generative models have led to recent progress in question generation (QG). However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches. In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting. It includes general-purpose datasets such as SQuAD for English, datasets from ten domains and two styles, as well as datasets in eight different languages. Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task. First, we propose robust QG baselines based on fine-tuning generative language models. Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models. Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English. QG-Bench is released along with the fine-tuned models presented in the paper https://github.com/asahi417/lm-question-generation, which are also available as a demo https://autoqg.net/.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.03992v2.Generative_Language_Models_for_Paragraph_Level_Question_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2884.5715
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.03992v2.Generative_Language_Models_for_Paragraph_Level_Question_Generation.mp3
   </guid>
<itunes:episode>
    70
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Neurobiology and crime: A neuro-ethical perspective
   </title>
<itunes:title>
    Neurobiology and crime: A neuro-ethical perspective
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Current neurobiological research in the field of criminology focuses on the neurobiological characteristics associated with antisocial behavior, the prediction of antisocial behavior later in life based on neurobiological risk factors, and the ways in which neurobiological factors interact with psychological and environmental risk factors. Although the use of neurobiological knowledge has the potential to make several criminal justice practices more objective and humane, it may involve practices that are challenging with respect to stigma, neuro-determinism, autonomy and mental liberty. Four main areas of interest can be identified where neurobiology plays or could play a role: (1) criminological research focused on understanding criminal behavior, (2) the (early) detection/prediction of and intervention in deviant behavior, (3) criminal proceedings: to assess responsibility and inform sentencing, and (4) forensic rehabilitation and treatment settings. In this paper, I discuss the main ethical dilemmas that arise when considering the use of recent neurobiological advances in these areas.
   </itunes:summary>
<description>
    Current neurobiological research in the field of criminology focuses on the neurobiological characteristics associated with antisocial behavior, the prediction of antisocial behavior later in life based on neurobiological risk factors, and the ways in which neurobiological factors interact with psychological and environmental risk factors. Although the use of neurobiological knowledge has the potential to make several criminal justice practices more objective and humane, it may involve practices that are challenging with respect to stigma, neuro-determinism, autonomy and mental liberty. Four main areas of interest can be identified where neurobiology plays or could play a role: (1) criminological research focused on understanding criminal behavior, (2) the (early) detection/prediction of and intervention in deviant behavior, (3) criminal proceedings: to assess responsibility and inform sentencing, and (4) forensic rehabilitation and treatment settings. In this paper, I discuss the main ethical dilemmas that arise when considering the use of recent neurobiological advances in these areas.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0047235217305299-main.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/1-s2.0-S0047235217305299-main.mp3.vtt"/>
<itunes:duration>
    3500.496
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0047235217305299-main.mp3
   </guid>
<itunes:episode>
    71
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral foundations vignettes: a standardized stimulus database of scenarios based on moral foundations theory
   </title>
<itunes:title>
    Moral foundations vignettes: a standardized stimulus database of scenarios based on moral foundations theory
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Abstract Research on the emotional, cognitive, and social determinants of moral judgment has surged in recent years. The development of moral foundations theory (MFT) has played an important role, demonstrating the breadth of morality. Moral psychology has responded by investigating how different domains of moral judgment are shaped by a variety of psychological factors. Yet, the discipline lacks a validated set of moral violations that span the moral domain, creating a barrier to investigating influences on judgment and how their neural bases might vary across the moral domain. In this paper, we aim to fill this gap by developing and validating a large set of moral foundations vignettes (MFVs). Each vignette depicts a behavior violating a particular moral foundation and not others. The vignettes are controlled on many dimensions including syntactic structure and complexity making them suitable for neuroimaging research. We demonstrate the validity of our vignettes by examining respondents' classifications of moral violations, conducting exploratory and confirmatory factor analysis, and demonstrating the correspondence between the extracted factors and existing measures of the moral foundations. We expect that the MFVs will be beneficial for a wide variety of behavioral and neuroimaging investigations of moral cognition.
   </itunes:summary>
<description>
    Abstract Research on the emotional, cognitive, and social determinants of moral judgment has surged in recent years. The development of moral foundations theory (MFT) has played an important role, demonstrating the breadth of morality. Moral psychology has responded by investigating how different domains of moral judgment are shaped by a variety of psychological factors. Yet, the discipline lacks a validated set of moral violations that span the moral domain, creating a barrier to investigating influences on judgment and how their neural bases might vary across the moral domain. In this paper, we aim to fill this gap by developing and validating a large set of moral foundations vignettes (MFVs). Each vignette depicts a behavior violating a particular moral foundation and not others. The vignettes are controlled on many dimensions including syntactic structure and complexity making them suitable for neuroimaging research. We demonstrate the validity of our vignettes by examining respondents' classifications of moral violations, conducting exploratory and confirmatory factor analysis, and demonstrating the correspondence between the extracted factors and existing measures of the moral foundations. We expect that the MFVs will be beneficial for a wide variety of behavioral and neuroimaging investigations of moral cognition.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13428-014-0551-2.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4669.1265
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13428-014-0551-2.mp3
   </guid>
<itunes:episode>
    72
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population
   </title>
<itunes:title>
    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.
   </itunes:summary>
<description>
    We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1652.715
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3
   </guid>
<itunes:episode>
    73
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label
   </title>
<itunes:title>
    SeedBERT: Recovering Annotator Rating Distributions from an Aggregated Label
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Many machine learning tasks -- particularly those in affective computing -- are inherently subjective. When asked to classify facial expressions or to rate an individual's attractiveness, humans may disagree with one another, and no single answer may be objectively correct. However, machine learning datasets commonly have just one "ground truth" label for each sample, so models trained on these labels may not perform well on tasks that are subjective in nature. Though allowing models to learn from the individual annotators' ratings may help, most datasets do not provide annotator-specific labels for each sample. To address this issue, we propose SeedBERT, a method for recovering annotator rating distributions from a single label by inducing pre-trained models to attend to different portions of the input. Our human evaluations indicate that SeedBERT's attention mechanism is consistent with human sources of annotator disagreement. Moreover, in our empirical evaluations using large language models, SeedBERT demonstrates substantial gains in performance on downstream subjective tasks compared both to standard deep learning models and to other current models that account explicitly for annotator disagreement.
   </itunes:summary>
<description>
    Many machine learning tasks -- particularly those in affective computing -- are inherently subjective. When asked to classify facial expressions or to rate an individual's attractiveness, humans may disagree with one another, and no single answer may be objectively correct. However, machine learning datasets commonly have just one "ground truth" label for each sample, so models trained on these labels may not perform well on tasks that are subjective in nature. Though allowing models to learn from the individual annotators' ratings may help, most datasets do not provide annotator-specific labels for each sample. To address this issue, we propose SeedBERT, a method for recovering annotator rating distributions from a single label by inducing pre-trained models to attend to different portions of the input. Our human evaluations indicate that SeedBERT's attention mechanism is consistent with human sources of annotator disagreement. Moreover, in our empirical evaluations using large language models, SeedBERT demonstrates substantial gains in performance on downstream subjective tasks compared both to standard deep learning models and to other current models that account explicitly for annotator disagreement.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.13196v1.SeedBERT_Recovering_Annotator_Rating_Distributions_from_an_Aggregated_Label.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2211.13196v1.SeedBERT_Recovering_Annotator_Rating_Distributions_from_an_Aggregated_Label.mp3.vtt"/>
<itunes:duration>
    1254.312
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.13196v1.SeedBERT_Recovering_Annotator_Rating_Distributions_from_an_Aggregated_Label.mp3
   </guid>
<itunes:episode>
    74
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    LaMDA: Language Models for Dialog Applications
   </title>
<itunes:title>
    LaMDA: Language Models for Dialog Applications
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.
   </itunes:summary>
<description>
    We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.08239v3.LaMDA_Language_Models_for_Dialog_Applications.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    7103.765
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.08239v3.LaMDA_Language_Models_for_Dialog_Applications.mp3
   </guid>
<itunes:episode>
    75
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    BERT has a Moral Compass: Improvements of ethical and moral values of machines
   </title>
<itunes:title>
    BERT has a Moral Compass: Improvements of ethical and moral values of machines
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.
   </itunes:summary>
<description>
    Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.05238v1.BERT_has_a_Moral_Compass_Improvements_of_ethical_and_moral_values_of_machines.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2212.075
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.05238v1.BERT_has_a_Moral_Compass_Improvements_of_ethical_and_moral_values_of_machines.mp3
   </guid>
<itunes:episode>
    76
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings
   </title>
<itunes:title>
    On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these "multi-hop" explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and produce valid explanations that are different than gold explanations. To address this, we construct a large corpus of 126k domain-expert (science teacher) relevance ratings that augment a corpus of explanations to standardized science exam questions, discovering 80k additional relevant facts not rated as gold. We build three strong models based on different methodologies (generation, ranking, and schemas), and empirically show that while expert-augmented ratings provide better estimates of explanation quality, both original (gold) and expertaugmented automatic evaluations still substantially underestimate performance by up to 36% when compared with full manual expert judgements, with different models being disproportionately affected. This poses a significant methodological challenge to accurately evaluating explanations produced by compositional reasoning models.
   </itunes:summary>
<description>
    Building compositional explanations requires models to combine two or more facts that, together, describe why the answer to a question is correct. Typically, these "multi-hop" explanations are evaluated relative to one (or a small number of) gold explanations. In this work, we show these evaluations substantially underestimate model performance, both in terms of the relevance of included facts, as well as the completeness of model-generated explanations, because models regularly discover and produce valid explanations that are different than gold explanations. To address this, we construct a large corpus of 126k domain-expert (science teacher) relevance ratings that augment a corpus of explanations to standardized science exam questions, discovering 80k additional relevant facts not rated as gold. We build three strong models based on different methodologies (generation, ranking, and schemas), and empirically show that while expert-augmented ratings provide better estimates of explanation quality, both original (gold) and expertaugmented automatic evaluations still substantially underestimate performance by up to 36% when compared with full manual expert judgements, with different models being disproportionately affected. This poses a significant methodological challenge to accurately evaluating explanations produced by compositional reasoning models.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.596.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3106.50775
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.596.mp3
   </guid>
<itunes:episode>
    77
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Teaching language models to support answers with verified quotes
   </title>
<itunes:title>
    Teaching language models to support answers with verified quotes
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.
   </itunes:summary>
<description>
    Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11147v1.Teaching_language_models_to_support_answers_with_verified_quotes.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6529.85475
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11147v1.Teaching_language_models_to_support_answers_with_verified_quotes.mp3
   </guid>
<itunes:episode>
    78
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    UnNatural Language Inference
   </title>
<itunes:title>
    UnNatural Language Inference
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.
   </itunes:summary>
<description>
    Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00010v2.UnNatural_Language_Inference.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3160.47675
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00010v2.UnNatural_Language_Inference.mp3
   </guid>
<itunes:episode>
    79
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Conditioning Predictive Models: Risks and Strategies
   </title>
<itunes:title>
    Conditioning Predictive Models: Risks and Strategies
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
   </itunes:summary>
<description>
    Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2302.00805v2.Conditioning_Predictive_Models_Risks_and_Strategies.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    14797.453
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2302.00805v2.Conditioning_Predictive_Models_Risks_and_Strategies.mp3
   </guid>
<itunes:episode>
    80
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    GPT-NeoX-20B: An Open-Source Autoregressive Language Model
   </title>
<itunes:title>
    GPT-NeoX-20B: An Open-Source Autoregressive Language Model
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.
   </itunes:summary>
<description>
    We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.06745v1.GPT_NeoX_20B_An_Open_Source_Autoregressive_Language_Model.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4890.09625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.06745v1.GPT_NeoX_20B_An_Open_Source_Autoregressive_Language_Model.mp3
   </guid>
<itunes:episode>
    81
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Training Compute-Optimal Large Language Models
   </title>
<itunes:title>
    Training Compute-Optimal Large Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4$\times$ more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.
   </itunes:summary>
<description>
    We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4$\times$ more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.15556v1.Training_Compute_Optimal_Large_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3861.94275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.15556v1.Training_Compute_Optimal_Large_Language_Models.mp3
   </guid>
<itunes:episode>
    82
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners
   </title>
<itunes:title>
    Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.
   </itunes:summary>
<description>
    Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.13161v6.Differentiable_Prompt_Makes_Pre_trained_Language_Models_Better_Few_shot_Learners.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2517.15925
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.13161v6.Differentiable_Prompt_Makes_Pre_trained_Language_Models_Better_Few_shot_Learners.mp3
   </guid>
<itunes:episode>
    83
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks
   </title>
<itunes:title>
    A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.
   </itunes:summary>
<description>
    Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1611.01587v5.A_Joint_Many_Task_Model_Growing_a_Neural_Network_for_Multiple_NLP_Tasks.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3419.324
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1611.01587v5.A_Joint_Many_Task_Model_Growing_a_Neural_Network_for_Multiple_NLP_Tasks.mp3
   </guid>
<itunes:episode>
    84
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Can Machines Learn Morality? The Delphi Experiment
   </title>
<itunes:title>
    Can Machines Learn Morality? The Delphi Experiment
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.
   </itunes:summary>
<description>
    As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07574v2.Can_Machines_Learn_Morality_The_Delphi_Experiment.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    8257.54125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07574v2.Can_Machines_Learn_Morality_The_Delphi_Experiment.mp3
   </guid>
<itunes:episode>
    85
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Understanding Diffusion Models: A Unified Perspective
   </title>
<itunes:title>
    Understanding Diffusion Models: A Unified Perspective
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.
   </itunes:summary>
<description>
    Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.11970v1.Understanding_Diffusion_Models_A_Unified_Perspective.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5277.93625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.11970v1.Understanding_Diffusion_Models_A_Unified_Perspective.mp3
   </guid>
<itunes:episode>
    86
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Plug and Play Language Models: A Simple Approach to Controlled Text Generation
   </title>
<itunes:title>
    Plug and Play Language Models: A Simple Approach to Controlled Text Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.
   </itunes:summary>
<description>
    Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02164v4.Plug_and_Play_Language_Models_A_Simple_Approach_to_Controlled_Text_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/1912.02164v4.Plug_and_Play_Language_Models_A_Simple_Approach_to_Controlled_Text_Generation.mp3.vtt"/>
<itunes:duration>
    4467.42
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02164v4.Plug_and_Play_Language_Models_A_Simple_Approach_to_Controlled_Text_Generation.mp3
   </guid>
<itunes:episode>
    87
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models
   </title>
<itunes:title>
    Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.
   </itunes:summary>
<description>
    This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/D17-1118.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2414.315
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/D17-1118.mp3
   </guid>
<itunes:episode>
    88
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral values are associated with individual differences in regional brain volume
   </title>
<itunes:title>
    Moral values are associated with individual differences in regional brain volume
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Moral sentiment has been hypothesized to reflect evolved adaptations to social living. If so, individual differences in moral values may relate to regional variation in brain structure. We tested this hypothesis in a sample of 70 young, healthy adults examining whether differences on two major dimensions of moral values were significantly associated with regional gray matter volume. The two clusters of moral values assessed were "individualizing" (values of harm/care and fairness), and "binding" (deference to authority, in-group loyalty, and purity/sanctity). Individualizing was positively associated with left dorsomedial prefrontal cortex volume, and negatively associated with bilateral precuneus volume. For binding, a significant positive association was found for bilateral subcallosal gyrus and a trend to significance for the left anterior insula volume. These findings demonstrate that variation in moral sentiment reflects individual differences in brain structure and suggest a biological basis for moral sentiment, distributed across multiple brain regions.
   </itunes:summary>
<description>
    Moral sentiment has been hypothesized to reflect evolved adaptations to social living. If so, individual differences in moral values may relate to regional variation in brain structure. We tested this hypothesis in a sample of 70 young, healthy adults examining whether differences on two major dimensions of moral values were significantly associated with regional gray matter volume. The two clusters of moral values assessed were "individualizing" (values of harm/care and fairness), and "binding" (deference to authority, in-group loyalty, and purity/sanctity). Individualizing was positively associated with left dorsomedial prefrontal cortex volume, and negatively associated with bilateral precuneus volume. For binding, a significant positive association was found for bilateral subcallosal gyrus and a trend to significance for the left anterior insula volume. These findings demonstrate that variation in moral sentiment reflects individual differences in brain structure and suggest a biological basis for moral sentiment, distributed across multiple brain regions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ukmss-48246.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/ukmss-48246.mp3.vtt"/>
<itunes:duration>
    1568.412
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ukmss-48246.mp3
   </guid>
<itunes:episode>
    89
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Automatic Detection of Generated Text is Easiest when Humans are Fooled
   </title>
<itunes:title>
    Automatic Detection of Generated Text is Easiest when Humans are Fooled
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.
   </itunes:summary>
<description>
    Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.00650v2.Automatic_Detection_of_Generated_Text_is_Easiest_when_Humans_are_Fooled.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2894.96825
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.00650v2.Automatic_Detection_of_Generated_Text_is_Easiest_when_Humans_are_Fooled.mp3
   </guid>
<itunes:episode>
    90
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    AGI Safety From First Principles
   </title>
<itunes:title>
    AGI Safety From First Principles
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This report explores the core case for why the development of artificial general intelligence (AGI) might pose an existential threat to humanity. It stems from my dissatisfaction with existing arguments on this topic: early work is less relevant in the context of modern machine learning, while more recent work is scattered and brief. This report aims to fill that gap by providing a detailed investigation into the potential risk from AGI misbehaviour, grounded by our current knowledge of machine learning, and highlighting important uncertainties. It identifies four key premises, evaluates existing arguments about them, and outlines some novel considerations for each.
   </itunes:summary>
<description>
    This report explores the core case for why the development of artificial general intelligence (AGI) might pose an existential threat to humanity. It stems from my dissatisfaction with existing arguments on this topic: early work is less relevant in the context of modern machine learning, while more recent work is scattered and brief. This report aims to fill that gap by providing a detailed investigation into the potential risk from AGI misbehaviour, grounded by our current knowledge of machine learning, and highlighting important uncertainties. It identifies four key premises, evaluates existing arguments about them, and outlines some novel considerations for each.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/AGI_safety_from_first_principles.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6045.91025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/AGI_safety_from_first_principles.mp3
   </guid>
<itunes:episode>
    91
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral foundations in an interacting neural networks society
   </title>
<itunes:title>
    Moral foundations in an interacting neural networks society
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The moral foundations theory supports that people, across cultures, tend to consider a small number of dimensions when classifying issues on a moral basis. The data also show that the statistics of weights attributed to each moral dimension is related to self-declared political affiliation, which in turn has been connected to cognitive learning styles by recent literature in neuroscience and psychology. Inspired by these data, we propose a simple statistical mechanics model with interacting neural networks classifying vectors and learning from members of their social neighborhood about their average opinion on a large set of issues. The purpose of learning is to reduce dissension among agents even when disagreeing. We consider a family of learning algorithms parametrized by , that represents the importance given to corroborating (same sign) opinions. We define an order parameter that quantifies the diversity of opinions in a group with homogeneous learning style. Using Monte Carlo simulations and a mean field approximation we find the relation between the order parameter and the learning parameter  at a temperature we associate with the importance of social influence in a given group. In concordance with data, groups that rely more strongly on corroborating evidence sustains less opinion diversity. We discuss predictions of the model and propose possible experimental tests.
   </itunes:summary>
<description>
    The moral foundations theory supports that people, across cultures, tend to consider a small number of dimensions when classifying issues on a moral basis. The data also show that the statistics of weights attributed to each moral dimension is related to self-declared political affiliation, which in turn has been connected to cognitive learning styles by recent literature in neuroscience and psychology. Inspired by these data, we propose a simple statistical mechanics model with interacting neural networks classifying vectors and learning from members of their social neighborhood about their average opinion on a large set of issues. The purpose of learning is to reduce dissension among agents even when disagreeing. We consider a family of learning algorithms parametrized by , that represents the importance given to corroborating (same sign) opinions. We define an order parameter that quantifies the diversity of opinions in a group with homogeneous learning style. Using Monte Carlo simulations and a mean field approximation we find the relation between the order parameter and the learning parameter  at a temperature we associate with the importance of social influence in a given group. In concordance with data, groups that rely more strongly on corroborating evidence sustains less opinion diversity. We discuss predictions of the model and propose possible experimental tests.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1307.3203v1.Moral_foundations_in_an_interacting_neural_networks_society.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2728.368
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1307.3203v1.Moral_foundations_in_an_interacting_neural_networks_society.mp3
   </guid>
<itunes:episode>
    92
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference
   </title>
<itunes:title>
    Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.
   </itunes:summary>
<description>
    The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3168.67925
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3
   </guid>
<itunes:episode>
    93
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures
   </title>
<itunes:title>
    Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Abstract-We investigate a new threat to neural sequenceto-sequence (seq2seq) models: training-time attacks that cause models to "spin" their outputs so as to support an adversarychosen sentiment or point of view-but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization. Model spinning introduces a "meta-backdoor" into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary. Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims. To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call "pseudo-words," and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and metatasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary's meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models. Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.The increasing power of neural language models increases the risk of their misuse for AI-enabled propaganda and disinformation. Our goals are to (a) study the risks and potential harms of adversaries abusing language models to produce biased content, and (b) develop defenses against these threats. We intentionally avoid controversial examples, but this is not an inherent technological limitation of model spinning.
   </itunes:summary>
<description>
    Abstract-We investigate a new threat to neural sequenceto-sequence (seq2seq) models: training-time attacks that cause models to "spin" their outputs so as to support an adversarychosen sentiment or point of view-but only when the input contains adversary-chosen trigger words. For example, a spinned 1 summarization model outputs positive summaries of any text that mentions the name of some individual or organization. Model spinning introduces a "meta-backdoor" into a model. Whereas conventional backdoors cause models to produce incorrect outputs on inputs with the trigger, outputs of spinned models preserve context and maintain standard accuracy metrics, yet also satisfy a meta-task chosen by the adversary. Model spinning enables propaganda-as-a-service, where propaganda is defined as biased speech. An adversary can create customized language models that produce desired spins for chosen triggers, then deploy these models to generate disinformation (a platform attack), or else inject them into ML training pipelines (a supply-chain attack), transferring malicious functionality to downstream models trained by victims. To demonstrate the feasibility of model spinning, we develop a new backdooring technique. It stacks an adversarial meta-task (e.g., sentiment analysis) onto a seq2seq model, backpropagates the desired meta-task output (e.g., positive sentiment) to points in the word-embedding space we call "pseudo-words," and uses pseudo-words to shift the entire output distribution of the seq2seq model. We evaluate this attack on language generation, summarization, and translation models with different triggers and metatasks such as sentiment, toxicity, and entailment. Spinned models largely maintain their accuracy metrics (ROUGE and BLEU) while shifting their outputs to satisfy the adversary's meta-task. We also show that, in the case of a supply-chain attack, the spin functionality transfers to downstream models. Finally, we propose a black-box, meta-task-independent defense that, given a list of candidate triggers, can detect models that selectively apply spin to inputs with any of these triggers.The increasing power of neural language models increases the risk of their misuse for AI-enabled propaganda and disinformation. Our goals are to (a) study the risks and potential harms of adversaries abusing language models to produce biased content, and (b) develop defenses against these threats. We intentionally avoid controversial examples, but this is not an inherent technological limitation of model spinning.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.05224v2.Spinning_Language_Models_Risks_of_Propaganda_As_A_Service_and_Countermeasures.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4472.738
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.05224v2.Spinning_Language_Models_Risks_of_Propaganda_As_A_Service_and_Countermeasures.mp3
   </guid>
<itunes:episode>
    94
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Survey on Stance Detection for Mis- and Disinformation Identification
   </title>
<itunes:title>
    A Survey on Stance Detection for Mis- and Disinformation Identification
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.
   </itunes:summary>
<description>
    Understanding attitudes expressed in texts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentation mining and sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00242v3.A_Survey_on_Stance_Detection_for_Mis_and_Disinformation_Identification.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3648.209
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00242v3.A_Survey_on_Stance_Detection_for_Mis_and_Disinformation_Identification.mp3
   </guid>
<itunes:episode>
    95
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Quest for a Common Model of the Intelligent Decision Maker
   </title>
<itunes:title>
    The Quest for a Common Model of the Intelligent Decision Maker
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.
   </itunes:summary>
<description>
    The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1499.1935
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3
   </guid>
<itunes:episode>
    96
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A General Language Assistant as a Laboratory for Alignment
   </title>
<itunes:title>
    A General Language Assistant as a Laboratory for Alignment
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.
   </itunes:summary>
<description>
    Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.00861v3.A_General_Language_Assistant_as_a_Laboratory_for_Alignment.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    8011.102
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.00861v3.A_General_Language_Assistant_as_a_Laboratory_for_Alignment.mp3
   </guid>
<itunes:episode>
    97
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Moral Foundations Reddit Corpus
   </title>
<itunes:title>
    The Moral Foundations Reddit Corpus
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Moral framing and sentiment can affect a variety of online and offline behaviors, including donation, pro-environmental action, political engagement, and even participation in violent protests. Various computational methods in Natural Language Processing (NLP) have been used to detect moral sentiment from textual data, but in order to achieve better performances in such subjective tasks, large sets of hand-annotated training data are needed. Previous corpora annotated for moral sentiment have proven valuable, and have generated new insights both within NLP and across the social sciences, but have been limited to Twitter. To facilitate improving our understanding of the role of moral rhetoric, we present the Moral Foundations Reddit Corpus, a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework. We use a range of methodologies to provide baseline moral-sentiment classification results for this new corpus, e.g., cross-domain classification and knowledge transfer.
   </itunes:summary>
<description>
    Moral framing and sentiment can affect a variety of online and offline behaviors, including donation, pro-environmental action, political engagement, and even participation in violent protests. Various computational methods in Natural Language Processing (NLP) have been used to detect moral sentiment from textual data, but in order to achieve better performances in such subjective tasks, large sets of hand-annotated training data are needed. Previous corpora annotated for moral sentiment have proven valuable, and have generated new insights both within NLP and across the social sciences, but have been limited to Twitter. To facilitate improving our understanding of the role of moral rhetoric, we present the Moral Foundations Reddit Corpus, a collection of 16,123 Reddit comments that have been curated from 12 distinct subreddits, hand-annotated by at least three trained annotators for 8 categories of moral sentiment (i.e., Care, Proportionality, Equality, Purity, Authority, Loyalty, Thin Morality, Implicit/Explicit Morality) based on the updated Moral Foundations Theory (MFT) framework. We use a range of methodologies to provide baseline moral-sentiment classification results for this new corpus, e.g., cross-domain classification and knowledge transfer.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.05545v2.The_Moral_Foundations_Reddit_Corpus.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3827.80075
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.05545v2.The_Moral_Foundations_Reddit_Corpus.mp3
   </guid>
<itunes:episode>
    98
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models
   </title>
<itunes:title>
    RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.
   </itunes:summary>
<description>
    Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.11462v2.RealToxicityPrompts_Evaluating_Neural_Toxic_Degeneration_in_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3925.969
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.11462v2.RealToxicityPrompts_Evaluating_Neural_Toxic_Degeneration_in_Language_Models.mp3
   </guid>
<itunes:episode>
    99
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Chain of Thought Prompting Elicits Reasoning in Large Language Models
   </title>
<itunes:title>
    Chain of Thought Prompting Elicits Reasoning in Large Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought -- a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.
   </itunes:summary>
<description>
    Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought -- a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.11903v1.Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4299.23275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.11903v1.Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.mp3
   </guid>
<itunes:episode>
    100
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
   </title>
<itunes:title>
    PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.
   </itunes:summary>
<description>
    Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12511v1.PolyLoss_A_Polynomial_Expansion_Perspective_of_Classification_Loss_Functions.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2992.82275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12511v1.PolyLoss_A_Polynomial_Expansion_Perspective_of_Classification_Loss_Functions.mp3
   </guid>
<itunes:episode>
    101
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data
   </title>
<itunes:title>
    Predictive Coding or Just Feature Discovery? An Alternative Account of Why Language Models Fit Brain Data
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.
   </itunes:summary>
<description>
    Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language. But why do these models work so well? One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived. This explanation is attractive because it lends support to the popular theory of predictive coding. We provide several analyses that cast doubt on this claim. First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others. Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations. Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nol_a_00087.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/nol_a_00087.mp3.vtt"/>
<itunes:duration>
    2745.468
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nol_a_00087.mp3
   </guid>
<itunes:episode>
    102
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs
   </title>
<itunes:title>
    Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.   In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations.   Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.
   </itunes:summary>
<description>
    Social intelligence and Theory of Mind (ToM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.   In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today's largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measures models' ability to understand intents and reactions of participants of social interactions, and ToMi (Le et al., 2019), which measures whether models can infer mental states and realities of participants of situations.   Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.13312v1.Neural_Theory_of_Mind_On_the_Limits_of_Social_Intelligence_in_Large_LMs.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3277.89725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.13312v1.Neural_Theory_of_Mind_On_the_Limits_of_Social_Intelligence_in_Large_LMs.mp3
   </guid>
<itunes:episode>
    103
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Distilling a Neural Network Into a Soft Decision Tree
   </title>
<itunes:title>
    Distilling a Neural Network Into a Soft Decision Tree
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.
   </itunes:summary>
<description>
    Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1711.09784v1.Distilling_a_Neural_Network_Into_a_Soft_Decision_Tree.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1108.5845
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1711.09784v1.Distilling_a_Neural_Network_Into_a_Soft_Decision_Tree.mp3
   </guid>
<itunes:episode>
    104
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Noise Audits Improve Moral Foundation Classification
   </title>
<itunes:title>
    Noise Audits Improve Moral Foundation Classification
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Abstract-Morality plays an important role in culture, identity, and emotion. Recent advances in natural language processing have shown that it is possible to classify moral values expressed in text at scale. Morality classification relies on human annotators to label the moral expressions in text, which provides training data to achieve state-of-the-art performance. However, these annotations are inherently subjective and some of the instances are hard to classify, resulting in noisy annotations due to error or lack of agreement. The presence of noise in training data harms the classifier's ability to accurately recognize moral foundations from text. We propose two metrics to audit the noise of annotations. The first metric is entropy of instance labels, which is a proxy measure of annotator disagreement about how the instance should be labeled. The second metric is the silhouette coefficient of a label assigned by an annotator to an instance. This metric leverages the idea that instances with the same label should have similar latent representations, and deviations from collective judgments are indicative of errors. Our experiments on three widely used moral foundations datasets show that removing noisy annotations based on the proposed metrics improves classification performance.
   </itunes:summary>
<description>
    Abstract-Morality plays an important role in culture, identity, and emotion. Recent advances in natural language processing have shown that it is possible to classify moral values expressed in text at scale. Morality classification relies on human annotators to label the moral expressions in text, which provides training data to achieve state-of-the-art performance. However, these annotations are inherently subjective and some of the instances are hard to classify, resulting in noisy annotations due to error or lack of agreement. The presence of noise in training data harms the classifier's ability to accurately recognize moral foundations from text. We propose two metrics to audit the noise of annotations. The first metric is entropy of instance labels, which is a proxy measure of annotator disagreement about how the instance should be labeled. The second metric is the silhouette coefficient of a label assigned by an annotator to an instance. This metric leverages the idea that instances with the same label should have similar latent representations, and deviations from collective judgments are indicative of errors. Our experiments on three widely used moral foundations datasets show that removing noisy annotations based on the proposed metrics improves classification performance.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.07415v1.Noise_Audits_Improve_Moral_Foundation_Classification.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1979.87275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.07415v1.Noise_Audits_Improve_Moral_Foundation_Classification.mp3
   </guid>
<itunes:episode>
    105
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Social Chemistry 101: Learning to Reason about Social and Moral Norms
   </title>
<itunes:title>
    Social Chemistry 101: Learning to Reason about Social and Moral Norms
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes."   We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.   Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.
   </itunes:summary>
<description>
    Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes."   We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.   Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2011.00620v3.Social_Chemistry_101_Learning_to_Reason_about_Social_and_Moral_Norms.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3446.04725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2011.00620v3.Social_Chemistry_101_Learning_to_Reason_about_Social_and_Moral_Norms.mp3
   </guid>
<itunes:episode>
    106
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Discovering Agents
   </title>
<itunes:title>
    Discovering Agents
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial -- often the causal model is just assumed by the modeler without much justification -- and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents -- roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering agents from empirical data, and give algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.
   </itunes:summary>
<description>
    Causal models of agents have been used to analyse the safety aspects of machine learning systems. But identifying agents is non-trivial -- often the causal model is just assumed by the modeler without much justification -- and modelling failures can lead to mistakes in the safety analysis. This paper proposes the first formal causal definition of agents -- roughly that agents are systems that would adapt their policy if their actions influenced the world in a different way. From this we derive the first causal discovery algorithm for discovering agents from empirical data, and give algorithms for translating between causal models and game-theoretic influence diagrams. We demonstrate our approach by resolving some previous confusions caused by incorrect causal modelling of agents.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.08345v2.Discovering_Agents.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    6015.765
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.08345v2.Discovering_Agents.mp3
   </guid>
<itunes:episode>
    107
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement
   </title>
<itunes:title>
    Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.
   </itunes:summary>
<description>
    The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1903.06059v2.Stochastic_Beams_and_Where_to_Find_Them_The_Gumbel_Top_k_Trick_for_Sampling_Sequences_Without_Replacement.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3202.325
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1903.06059v2.Stochastic_Beams_and_Where_to_Find_Them_The_Gumbel_Top_k_Trick_for_Sampling_Sequences_Without_Replacement.mp3
   </guid>
<itunes:episode>
    108
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Do Transformers use variable binding?
   </title>
<itunes:title>
    Do Transformers use variable binding?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation. One central symbolic capacity is variable binding: linking an input value to an abstract variable held in system-internal memory. Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding. We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed. We provide the first systematic evaluation of the variable binding capacities of the state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone. The results show a clear discrepancy between classification and sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or reverse strings even when trained on task-specific vocabularies that are switched in the test set; but both models completely fail to generalize across vocabularies in similar sequence classification tasks. These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external "memory" rather than network-internal symbolic operations involving variable binding. Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding.
   </itunes:summary>
<description>
    Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation. One central symbolic capacity is variable binding: linking an input value to an abstract variable held in system-internal memory. Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding. We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed. We provide the first systematic evaluation of the variable binding capacities of the state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone. The results show a clear discrepancy between classification and sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or reverse strings even when trained on task-specific vocabularies that are switched in the test set; but both models completely fail to generalize across vocabularies in similar sequence classification tasks. These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external "memory" rather than network-internal symbolic operations involving variable binding. Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.00162v1.Do_Transformers_use_variable_binding.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3186.2595
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.00162v1.Do_Transformers_use_variable_binding.mp3
   </guid>
<itunes:episode>
    109
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Entailment Semantics Can Be Extracted from an Ideal Language Model
   </title>
<itunes:title>
    Entailment Semantics Can Be Extracted from an Ideal Language Model
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.
   </itunes:summary>
<description>
    Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.12407v1.Entailment_Semantics_Can_Be_Extracted_from_an_Ideal_Language_Model.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3640.8425
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.12407v1.Entailment_Semantics_Can_Be_Extracted_from_an_Ideal_Language_Model.mp3
   </guid>
<itunes:episode>
    110
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning
   </title>
<itunes:title>
    Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.
   </itunes:summary>
<description>
    We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.08200v1.Unsupervised_Natural_Language_Inference_via_Decoupled_Multimodal_Contrastive_Learning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2197.91675
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.08200v1.Unsupervised_Natural_Language_Inference_via_Decoupled_Multimodal_Contrastive_Learning.mp3
   </guid>
<itunes:episode>
    111
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition
   </title>
<itunes:title>
    Minimum Bayes Risk Training of RNN-Transducer for End-to-End Speech Recognition
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In this work, we propose minimum Bayes risk (MBR) training of RNN-Transducer (RNN-T) for end-to-end speech recognition. Specifically, initialized with a RNN-T trained model, MBR training is conducted via minimizing the expected edit distance between the reference label sequence and on-the-fly generated N-best hypothesis. We also introduce a heuristic to incorporate an external neural network language model (NNLM) in RNN-T beam search decoding and explore MBR training with the external NNLM. Experimental results demonstrate an MBR trained model outperforms a RNN-T trained model substantially and further improvements can be achieved if trained with an external NNLM. Our best MBR trained system achieves absolute character error rate (CER) reductions of 1.2% and 0.5% on read and spontaneous Mandarin speech respectively over a strong convolution and transformer based RNN-T baseline trained on ~21,000 hours of speech.
   </itunes:summary>
<description>
    In this work, we propose minimum Bayes risk (MBR) training of RNN-Transducer (RNN-T) for end-to-end speech recognition. Specifically, initialized with a RNN-T trained model, MBR training is conducted via minimizing the expected edit distance between the reference label sequence and on-the-fly generated N-best hypothesis. We also introduce a heuristic to incorporate an external neural network language model (NNLM) in RNN-T beam search decoding and explore MBR training with the external NNLM. Experimental results demonstrate an MBR trained model outperforms a RNN-T trained model substantially and further improvements can be achieved if trained with an external NNLM. Our best MBR trained system achieves absolute character error rate (CER) reductions of 1.2% and 0.5% on read and spontaneous Mandarin speech respectively over a strong convolution and transformer based RNN-T baseline trained on ~21,000 hours of speech.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.12487v1.Minimum_Bayes_Risk_Training_of_RNN_Transducer_for_End_to_End_Speech_Recognition.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1558.0995
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.12487v1.Minimum_Bayes_Risk_Training_of_RNN_Transducer_for_End_to_End_Speech_Recognition.mp3
   </guid>
<itunes:episode>
    112
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Masked Autoencoders As Spatiotemporal Learners
   </title>
<itunes:title>
    Masked Autoencoders As Spatiotemporal Learners
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.
   </itunes:summary>
<description>
    This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09113v1.Masked_Autoencoders_As_Spatiotemporal_Learners.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2763.8335
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09113v1.Masked_Autoencoders_As_Spatiotemporal_Learners.mp3
   </guid>
<itunes:episode>
    113
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Chaos
   </title>
<itunes:title>
    Chaos
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The great power of science lies in the ability to relate cause and effect. On the basis of the laws of gravitation, for example, eclipses can be predicted thousands of years in advance. There are other natural phenomena that are not as predictable. Although the movements of the atmosphere obey the laws of physics just as much as the movements of the planets do, weather forecasts are still stated in terms of probabilities. The weather, the flow of a mountain stream, the roll of the dice all have unpredictable aspects. Since there is no clear relation between cause and effect, such phenomena are said to have random elements. Yet until recently there was little reason to doubt that precise predictability could in principle be achieved. It was assumed that it was only necessary to gather and process a sufficient amount of information. Such a viewpoint has been altered by a striking discovery: simple deterministic systems with only a few elements can generate random behavior. The randomness is fundamental; gathering more information does not make it go away. Randomness generated in this way has come to be called chaos. A seeming paradox is that chaos is deterministic, generated by fixed rules that do not themselves involve any elements of chance. In principle the future is completely determined by the past, but in practice small uncertainties are amplified, so that even though the behavior is predictable in the short term, it is unpredictable in the long term. There is order in chaos: underlying chaotic behavior there are elegant geometric forms that create randomness in the same way as a card dealer shuffles a deck of cards or a blender mixes cake batter. The discovery of chaos has created a new paradigm in scientific modeling. On one hand, it implies new fundamental limits on the ability to make predictions. On the other hand, the determinism inherent in chaos implies that many random phenomena are more predictable than had been thought. Random-looking information gathered in the past-and shelved because it was assumed to be too complicated-can now be explained in terms of simple laws. Chaos allows order to be found in such diverse systems as the atmosphere, dripping faucets, and the heart. The result is a revolution that is affecting many different branches of science. What are the origins of random behavior? Brownian motion provides a classic example of randomness. A speck of dust observed through a microscope is seen to move in a continuous and erratic jiggle. This is owing to the bombardment of the dust particle by the surrounding water molecules in thermal motion. Because the water molecules are unseen and exist in great number, the detailed motion of the dust particle is thoroughly unpredictable. Here the web of causal influences among the subunits can become so tangled that the resulting pattern of behavior becomes quite random. The chaos to be discussed here requires no large number of subunits or unseen influences. The existence of random behavior in very simple systems motivates a reexamination of the sources of randomness even in large systems such as weather. What makes the motion of the atmosphere so much harder to anticipate than the motion of the solar system? Both are made up of many parts, and both are governed by Newton's second law, F = m a, which can be viewed as a simple prescription for predicting the future. If the forces F acting on a given mass m are known, then so is the acceleration a. It then follows from Chaos, Scientific American 54:12 (1986) 46-57
   </itunes:summary>
<description>
    The great power of science lies in the ability to relate cause and effect. On the basis of the laws of gravitation, for example, eclipses can be predicted thousands of years in advance. There are other natural phenomena that are not as predictable. Although the movements of the atmosphere obey the laws of physics just as much as the movements of the planets do, weather forecasts are still stated in terms of probabilities. The weather, the flow of a mountain stream, the roll of the dice all have unpredictable aspects. Since there is no clear relation between cause and effect, such phenomena are said to have random elements. Yet until recently there was little reason to doubt that precise predictability could in principle be achieved. It was assumed that it was only necessary to gather and process a sufficient amount of information. Such a viewpoint has been altered by a striking discovery: simple deterministic systems with only a few elements can generate random behavior. The randomness is fundamental; gathering more information does not make it go away. Randomness generated in this way has come to be called chaos. A seeming paradox is that chaos is deterministic, generated by fixed rules that do not themselves involve any elements of chance. In principle the future is completely determined by the past, but in practice small uncertainties are amplified, so that even though the behavior is predictable in the short term, it is unpredictable in the long term. There is order in chaos: underlying chaotic behavior there are elegant geometric forms that create randomness in the same way as a card dealer shuffles a deck of cards or a blender mixes cake batter. The discovery of chaos has created a new paradigm in scientific modeling. On one hand, it implies new fundamental limits on the ability to make predictions. On the other hand, the determinism inherent in chaos implies that many random phenomena are more predictable than had been thought. Random-looking information gathered in the past-and shelved because it was assumed to be too complicated-can now be explained in terms of simple laws. Chaos allows order to be found in such diverse systems as the atmosphere, dripping faucets, and the heart. The result is a revolution that is affecting many different branches of science. What are the origins of random behavior? Brownian motion provides a classic example of randomness. A speck of dust observed through a microscope is seen to move in a continuous and erratic jiggle. This is owing to the bombardment of the dust particle by the surrounding water molecules in thermal motion. Because the water molecules are unseen and exist in great number, the detailed motion of the dust particle is thoroughly unpredictable. Here the web of causal influences among the subunits can become so tangled that the resulting pattern of behavior becomes quite random. The chaos to be discussed here requires no large number of subunits or unseen influences. The existence of random behavior in very simple systems motivates a reexamination of the sources of randomness even in large systems such as weather. What makes the motion of the atmosphere so much harder to anticipate than the motion of the solar system? Both are made up of many parts, and both are governed by Newton's second law, F = m a, which can be viewed as a simple prescription for predicting the future. If the forces F acting on a given mass m are known, then so is the acceleration a. It then follows from Chaos, Scientific American 54:12 (1986) 46-57
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Farmer_Chaos_SciAm_1986.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3001.182
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Farmer_Chaos_SciAm_1986.mp3
   </guid>
<itunes:episode>
    114
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Optimally interacting minds Europe PMC Funders Group Europe PMC Funders Author Manuscripts Europe PMC Funders Author Manuscripts
   </title>
<itunes:title>
    Optimally interacting minds Europe PMC Funders Group Europe PMC Funders Author Manuscripts Europe PMC Funders Author Manuscripts
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In everyday life, many believe that 'two heads are better than one'. Indeed, our ability to solve problems together appears to be fundamental to the current dominance, and future survival, of the human species. But are two heads really better than one? We addressed this question in the context of a collective low-level perceptual decision-making task. For two observers of nearly equal sensitivity, two heads were definitely better than one, provided that they were given the opportunity to communicate freely, even in the absence of any feedback about decision outcomes. But for observers with very different sensitivities, two heads were worse than the better one. These seemingly discrepant patterns of group behaviour can be explained by a model in which two heads are Bayes optimal under the assumption that individuals accurately communicate their level of confidence on every trial.To come to an optimal joint decision, individuals must share information with each other, and, importantly, weigh that information by its reliability (1, 2). It is well established that isolated individuals can accurately weigh information when combining different sources of sensory information (3-5). Little is known, however, about how -or even whether -two individuals can accurately combine information that they communicate with each other. To investigate this issue, we examined the behaviour of pairs of individuals in a simple perceptual decision task, and we asked how signals from the same sensory modality (vision) in the brains of two different individuals could be combined through social interaction. Work on perceptual decision-making has shown that when combining information from different senses, individuals have access not just to magnitudes of sensory signals, but also to their probability distributions, or at least to their means and variances (3-8). This may not, however, be true for interpersonal communication. While probability distributions arising from different sensory modalities are available within an individual's brain, it is not clear whether such distributions can be passed directly to another person or indeed what types of information can be communicated. To answer this, we considered four models (16), each of
   </itunes:summary>
<description>
    In everyday life, many believe that 'two heads are better than one'. Indeed, our ability to solve problems together appears to be fundamental to the current dominance, and future survival, of the human species. But are two heads really better than one? We addressed this question in the context of a collective low-level perceptual decision-making task. For two observers of nearly equal sensitivity, two heads were definitely better than one, provided that they were given the opportunity to communicate freely, even in the absence of any feedback about decision outcomes. But for observers with very different sensitivities, two heads were worse than the better one. These seemingly discrepant patterns of group behaviour can be explained by a model in which two heads are Bayes optimal under the assumption that individuals accurately communicate their level of confidence on every trial.To come to an optimal joint decision, individuals must share information with each other, and, importantly, weigh that information by its reliability (1, 2). It is well established that isolated individuals can accurately weigh information when combining different sources of sensory information (3-5). Little is known, however, about how -or even whether -two individuals can accurately combine information that they communicate with each other. To investigate this issue, we examined the behaviour of pairs of individuals in a simple perceptual decision task, and we asked how signals from the same sensory modality (vision) in the brains of two different individuals could be combined through social interaction. Work on perceptual decision-making has shown that when combining information from different senses, individuals have access not just to magnitudes of sensory signals, but also to their probability distributions, or at least to their means and variances (3-8). This may not, however, be true for interpersonal communication. While probability distributions arising from different sensory modalities are available within an individual's brain, it is not clear whether such distributions can be passed directly to another person or indeed what types of information can be communicated. To answer this, we considered four models (16), each of
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ukmss-48049.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1430.7265
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ukmss-48049.mp3
   </guid>
<itunes:episode>
    115
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral reframing: A technique for effective and persuasive communication across political divides
   </title>
<itunes:title>
    Moral reframing: A technique for effective and persuasive communication across political divides
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The political landscape in the US and many other countries is characterized by policy impasses and animosity between rival political groups. Research finds that these divisions are Over the last decade, studies of moral reframing have shown its effectiveness across a wide range of polarized topics, including views of economic inequality, environmental protection, same-sex marriage, and major party candidates for the US presidency. In this article, we review the moral reframing literature, examining potential mediators and moderators of the effect, and discuss important questions that remain unanswered about this phenomenon.
   </itunes:summary>
<description>
    The political landscape in the US and many other countries is characterized by policy impasses and animosity between rival political groups. Research finds that these divisions are Over the last decade, studies of moral reframing have shown its effectiveness across a wide range of polarized topics, including views of economic inequality, environmental protection, same-sex marriage, and major party candidates for the US presidency. In this article, we review the moral reframing literature, examining potential mediators and moderators of the effect, and discuss important questions that remain unanswered about this phenomenon.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2f07d4_50c5fdcaadbf4ec69697fa6a62ac02e3.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2408.6465
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2f07d4_50c5fdcaadbf4ec69697fa6a62ac02e3.mp3
   </guid>
<itunes:episode>
    116
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Towards Explaining Subjective Ground of Individuals on Social Media
   </title>
<itunes:title>
    Towards Explaining Subjective Ground of Individuals on Social Media
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual's theory of mind and behavior from text is far from being resolved.   This research proposes a neural model -- Subjective Ground Attention -- that learns subjective grounds of individuals and accounts for their judgments on situations of others posted on social media. Using simple attention modules as well as taking one's previous activities into consideration, we empirically show that our model provides human-readable explanations of an individual's subjective preference in judging social situations. We further qualitatively evaluate the explanations generated by the model and claim that our model learns an individual's subjective orientation towards abstract moral concepts
   </itunes:summary>
<description>
    Large-scale language models have been reducing the gap between machines and humans in understanding the real world, yet understanding an individual's theory of mind and behavior from text is far from being resolved.   This research proposes a neural model -- Subjective Ground Attention -- that learns subjective grounds of individuals and accounts for their judgments on situations of others posted on social media. Using simple attention modules as well as taking one's previous activities into consideration, we empirically show that our model provides human-readable explanations of an individual's subjective preference in judging social situations. We further qualitatively evaluate the explanations generated by the model and claim that our model learns an individual's subjective orientation towards abstract moral concepts
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.09953v1.Towards_Explaining_Subjective_Ground_of_Individuals_on_Social_Media.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2545.3715
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.09953v1.Towards_Explaining_Subjective_Ground_of_Individuals_on_Social_Media.mp3
   </guid>
<itunes:episode>
    117
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Language Model Augmented Relevance Score
   </title>
<itunes:title>
    Language Model Augmented Relevance Score
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on -gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation. MARS leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree.
   </itunes:summary>
<description>
    Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements. Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on -gram matching. These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference. In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation. MARS leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text. Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.acl-long.521.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2021.acl-long.521.mp3.vtt"/>
<itunes:duration>
    2002.068
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.acl-long.521.mp3
   </guid>
<itunes:episode>
    118
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions
   </title>
<itunes:title>
    Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.
   </itunes:summary>
<description>
    In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.00415v1.Don_t_Blame_the_Annotator_Bias_Already_Starts_in_the_Annotation_Instructions.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1373.80575
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.00415v1.Don_t_Blame_the_Annotator_Bias_Already_Starts_in_the_Annotation_Instructions.mp3
   </guid>
<itunes:episode>
    119
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Theory of Dyadic Morality: Reinventing Moral Judgment by Redefining Harm
   </title>
<itunes:title>
    The Theory of Dyadic Morality: Reinventing Moral Judgment by Redefining Harm
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1088868317698288-1.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    10969.626
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1088868317698288-1.mp3
   </guid>
<itunes:episode>
    120
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Aligning Generative Language Models with Human Values
   </title>
<itunes:title>
    Aligning Generative Language Models with Human Values
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral ). Existing methods learn human values either by directly mimick-ing the behavior of human data, or rigidly con-straining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes S ENSEI , a new reinforcement learning based method that can embed human values judgements into each step of language generation. S ENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, S ENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.
   </itunes:summary>
<description>
    Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral ). Existing methods learn human values either by directly mimick-ing the behavior of human data, or rigidly con-straining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes S ENSEI , a new reinforcement learning based method that can embed human values judgements into each step of language generation. S ENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, S ENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/250562745.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2124.01625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/250562745.mp3
   </guid>
<itunes:episode>
    121
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Single Headed Attention RNN: Stop Thinking With Your Head
   </title>
<itunes:title>
    Single Headed Attention RNN: Stop Thinking With Your Head
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.
   </itunes:summary>
<description>
    The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.11423v2.Single_Headed_Attention_RNN_Stop_Thinking_With_Your_Head.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2614.43925
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.11423v2.Single_Headed_Attention_RNN_Stop_Thinking_With_Your_Head.mp3
   </guid>
<itunes:episode>
    122
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Risks from Learned Optimization in Advanced Machine Learning Systems
   </title>
<itunes:title>
    Risks from Learned Optimization in Advanced Machine Learning Systems
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.
   </itunes:summary>
<description>
    We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01820v3.Risks_from_Learned_Optimization_in_Advanced_Machine_Learning_Systems.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    7254.33475
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01820v3.Risks_from_Learned_Optimization_in_Advanced_Machine_Learning_Systems.mp3
   </guid>
<itunes:episode>
    123
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Fully Differentiable Beam Search Decoder
   </title>
<itunes:title>
    A Fully Differentiable Beam Search Decoder
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.
   </itunes:summary>
<description>
    We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2335.08575
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3
   </guid>
<itunes:episode>
    124
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    PERSONALITY PROCESSES AND INDIVIDUAL DIFFERENCES Liberals and Conservatives Rely on Different Sets of Moral Foundations
   </title>
<itunes:title>
    PERSONALITY PROCESSES AND INDIVIDUAL DIFFERENCES Liberals and Conservatives Rely on Different Sets of Moral Foundations
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt &amp; J. Graham, 2007;J. Haidt &amp; C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/ respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), "sacredness" reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American "culture war."
   </itunes:summary>
<description>
    How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt &amp; J. Graham, 2007;J. Haidt &amp; C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/ respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), "sacredness" reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American "culture war."
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/JPSP-2009-Moral-Foundations.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4912.0915
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/JPSP-2009-Moral-Foundations.mp3
   </guid>
<itunes:episode>
    125
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT
   </title>
<itunes:title>
    Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    With advances in Machine Learning (ML), neural network-based methods, such as Convolutional/Recurrent Neural Networks, have been proposed to assist terminology curators in the development and maintenance of terminologies. Bidirectional Encoder Representations from Transformers (BERT), a new language representation model, obtains state-of-the-art results on a wide array of general English NLP tasks. We explore BERTs applicability to medical terminology-related tasks. Utilizing the next sentence prediction capability of BERT, we show that the Fine-tuning strategy of Transfer Learning (TL) from the BERTBASE model can address a challenging problem in automatic terminology enrichment  insertion of new concepts. Adding a pre-training strategy enhances the results. We apply our strategies to the two largest hierarchies of SNOMED CT, with one release as training data and the following release as test data. The performance of the combined two proposed TL models achieves an average F1 score of 0.85 and 0.86 for the two hierarchies, respectively.
   </itunes:summary>
<description>
    With advances in Machine Learning (ML), neural network-based methods, such as Convolutional/Recurrent Neural Networks, have been proposed to assist terminology curators in the development and maintenance of terminologies. Bidirectional Encoder Representations from Transformers (BERT), a new language representation model, obtains state-of-the-art results on a wide array of general English NLP tasks. We explore BERTs applicability to medical terminology-related tasks. Utilizing the next sentence prediction capability of BERT, we show that the Fine-tuning strategy of Transfer Learning (TL) from the BERTBASE model can address a challenging problem in automatic terminology enrichment  insertion of new concepts. Adding a pre-training strategy enhances the results. We apply our strategies to the two largest hierarchies of SNOMED CT, with one release as training data and the following release as test data. The performance of the combined two proposed TL models achieves an average F1 score of 0.85 and 0.86 for the two hierarchies, respectively.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1700.1535
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT.mp3
   </guid>
<itunes:episode>
    126
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    GRUEN for Evaluating Linguistic Quality of Generated Text
   </title>
<itunes:title>
    GRUEN for Evaluating Linguistic Quality of Generated Text
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.
   </itunes:summary>
<description>
    Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. GRUEN utilizes a BERT-based model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.02498v1.GRUEN_for_Evaluating_Linguistic_Quality_of_Generated_Text.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2554.018
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.02498v1.GRUEN_for_Evaluating_Linguistic_Quality_of_Generated_Text.mp3
   </guid>
<itunes:episode>
    127
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?
   </title>
<itunes:title>
    What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.
   </itunes:summary>
<description>
    Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05832v1.What_Language_Model_Architecture_and_Pretraining_Objective_Work_Best_for_Zero_Shot_Generalization.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4025.86125
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05832v1.What_Language_Model_Architecture_and_Pretraining_Objective_Work_Best_for_Zero_Shot_Generalization.mp3
   </guid>
<itunes:episode>
    128
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Mapping the Moral Domain NIH Public Access
   </title>
<itunes:title>
    Mapping the Moral Domain NIH Public Access
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The moral domain is broader than the empathy and justice concerns assessed by existing measures of moral competence, and it is not just a subset of the values assessed by value inventories. To fill the need for reliable and theoretically-grounded measurement of the full range of moral concerns, we developed the Moral Foundations Questionnaire (MFQ) based on a theoretical model of five universally available (but variably developed) sets of moral intuitions: Harm/care, Fairness/ reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. We present evidence for the internal and external validity of the scale and the model, and in doing so present new findings about morality: 1. Comparative model fitting of confirmatory factor analyses provides empirical justification for a five-factor structure of moral concerns. 2. Convergent/discriminant validity evidence suggests that moral concerns predict personality features and social group attitudes not previously considered morally relevant. 3. We establish pragmatic validity of the measure in providing new knowledge and research opportunities concerning demographic and cultural differences in moral intuitions. These analyses provide evidence for the usefulness of Moral Foundations Theory in simultaneously increasing the scope and sharpening the resolution of psychological views of morality. Keywords morality; scale validation; moral foundations; culture; values Correspondence concerning this article should be sent to jgraham@virginia.edu. Publisher's Disclaimer: The following manuscript is the final accepted manuscript. It has not been subjected to the final copyediting, fact-checking, and proofreading required for formal publication. It is not the definitive, publisher-authenticated version. The American Psychological Association and its Council of Editors disclaim any responsibility or liabilities for errors or omissions of this manuscript version, any version derived from this manuscript by NIH, or other third parties. The published version is available at www.apa.org/pubs/journals/PSP Supplemental data and analyses can be found at www.moralfoundations.org.Author Manuscript J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1. NIH-PA Author ManuscriptHow can we measure moral concerns when people disagree about what "morality" means? To address this problem we created the Moral Foundations Questionnaire (MFQ, presented in the appendix), a measure of the degree to which individuals endorse each of five intuitive systems posited by Moral Foundations Theory: Harm/care, Fairness/reciprocity, Ingroup/ loyalty, Authority/respect, and Purity/sanctity (Haidt &amp; Graham, 2007;Shweder, Much, Mahapatra &amp; Park, 1997). People vary in the extent to which they endorse, value, and use these five foundations, providing an opportunity to better understand moral diversity. In this article, we explain the need for a scale broader than conventional morality scales. We describe the development of the scale, which involved multiple rounds of item analysis using large heterogeneous samples. We present the validation of the scale, organized into evidence confirming internal and external validity. Finally, we describe its pragmatic validity -that is, the practical usefulness of both the theory and the measure in providing new insights about moral psychology. A major goal of Moral Foundations Theory is to expand the range of phenomena studied in moral psychology so that it matches the full range of moral concerns, including those found in non-Western cultures, in religious practices, and among political conservatives. Here we present what we have learned about the content and structure of the moral domain using the Moral Foundations Questionnaire.A great variety of scales are used in moral psychology to measure stages of moral reasoning (e.g., the Defining Issues Test-2; Rest et al., 1999), moral identity (e.g., the Moral Identity Scale; Aquino &amp; Reed, 2002), empathy (e.g., the Interpersonal Reactivity Index; Davis, 1983), and moral deficits such as psychopathy (e.g., Levenson's Self-report Psychopathy Scale; Levenson et al., 1995). Although these scales measure different aspects of morality, they all share the assumption (explicit or implicit) that the moral domain is limited to concerns about individuals harming or unfairly treating other individuals. This is, in part, a reflection of the dominance of Lawrence Kohlberg's (1969Kohlberg's ( , 1971 ideas about morality as justice, and his subsequent debate with Carol Gilligan (1982) about her alternative conception of morality as care. Both sides agreed that morality was about how well or poorly individuals treated other individuals. Turiel (1983, p.3) codified this approach into a widely-cited definition of the moral domain as "prescriptive judgments of justice, rights, and welfare pertaining to how people ought to relate to each other." Any values that were not related to "justice, rights, and welfare" (e.g., patriotism, authority, or chastity) were considered non-moral, and were relegated to the domain of social convention or the domain of personal choice (Turiel, Hildebrandt, &amp; Wainryb, 1991). Definitions of morality in philosophy also frequently stress rules or codes of conduct that reduce harm to others (e.g., Gert, 2005;Singer, 1979). Kohlberg certainly noticed that people sometimes justified moral judgments by referring to group-level moral concerns such as authority, loyalty, and tradition, but he thought that such thinking was immature and conventional, a part of the "law and order" ethos of stage 4. With sufficient opportunities for role-taking, adolescents were said to move beyond stage 4 and to begin using post-conventional reasoning based on an understanding of justice. Kohlberg's stage theory has been criticized on many grounds; one criticism relevant for our purposes was that Kohlberg's post-conventional morality enshrined politically liberal ideals as developmental endpoints (Hogan, Johnson, &amp; Emler, 1978;Shweder, 1982;Sullivan, 1977;  for related critiques see Puka, 1994). This critique was backed up by the demonstration that liberals routinely obtain higher principled reasoning scores on the Defining Issues Test, but that conservative students rose to liberal levels when told to "respond as a left-winger would" (Emler, Renwick, &amp; Malone, 1983). Conservatives could reason at the "higher stage," but were not doing so presumably because they had different priorities in their moral reasoning. Despite these critiques, the notion that "true" moral  Oliveira-Souza, &amp; Grafman, 2006). Whether carried out in a scanner, on a website, or in a business school lab room using real money, morality is still usually operationalized as helping (vs. harming) or as playing fair (vs cheating). Kohlberg and Turiel based their circumscription of the moral domain on a line of enlightenment thinking running from Immanuel Kant to John Stuart Mill to John Rawls in which the autonomy and/or welfare of the individual are the starting point for ethical inquiry. Yet even Kant (1797Kant ( /1996) had intuitions of a broader moral domain. He wrote that masturbation was "in the highest degree opposed to morality," although he granted that "it is not so easy to produce a rational demonstration" of its wrongness. And before the Enlightenment, philosophers routinely considered a much broader moral domain. Much of ancient moral philosophy, from Greece to India to Japan, was virtue-based. These societies valued benevolence and fairness, but they also emphasized group-level concerns about social order, authority, duty, loyalty to one's family or group, and controlling one's carnal desires to cultivate one's soul or gain a more favorable rebirth (Larue, 1991;Shweder et al., 1997 Cross-cultural research on moral judgment has revealed that Turiel's definition of the moral domain works well among educated and politically liberal Westerners, for whom harmless offenses are rarely condemned, even when they are disgusting or disrespectful (Haidt, Koller, &amp; Dias, 1993).1 However, research on people in India (Shweder, Mahapatra, &amp; Miller, 1987) Dias, 1993), and conservatives in the United States (Graham, Haidt, &amp; Nosek, 2009;Haidt &amp; Hersh, 2001;Jensen, 1998) has revealed moral considerations beyond the individualbased concerns of harm and fairness, involving concerns about spiritual purity and degradation (even for acts that involve no harm), concerns about proper hierarchical role fulfillment, and moral expectations of loyalty to the local or national group. Illustrative of this breadth are open-ended responses participants in our studies gave when asked to define morality in their own words. Parallel to Turiel's definition, many made reference to harm and human welfare (e.g., "Avoiding harm to others"), and fairness or justice ("Morality is doing the right things to ensure fair treatment for all"). However, others made reference to moral issues beyond justice, rights, and welfare, and to morally valuable entities that were not individuals (e.g., "Morality is having a system [that] protects the social institutions of family, community, and country."). Others made reference to duty, obedience, respect, and preserving tradition (e.g., "Matters of duty, irrespective of one's own personal desires or ends"). And some made reference to God or religious norms, decency, the soul, and maintaining purity of mind (e.g., "not having dirty thoughts"  (Rokeach, 1973;Schwartz, 1992). Values research has much to offer the empirical study of morality, and is too often ignored by moral psychologists. Clearly, many values are moral values, even if morality is defined only in terms of welfare and fairness concerns (e.g., benevolence and universalism). However, in seeking to identify a list of the most important values, there is a risk that some common moral concerns or intuitions will be missed. For example, reciprocity, loyalty to one's team or tribe, and concerns about bodily and spiritual purity are ubiquitous in anthropological accounts of In order to fill the need for a systematic theory of morality, explaining its origins, development, and cultural variations, we created Moral Foundations Theory (MFT). Haidt and Joseph (2004) began by surveying the literatures in evolutionary psychology and anthropology, looking for matches -for virtues and areas of moral regulation that were common (though not necessarily universal) across cultures, and that had some clear counterpart in evolutionary thinking. For example, virtues related to fairness and practices of reciprocal gift exchange (e.g., Mauss, 1924Mauss, /1990) bore an obvious similarity to the evolutionary literature on reciprocal altruism (Trivers, 1971); virtues of purity and practices regulating food and sex (e.g., Douglas, 1966) bore an obvious relationship to the evolutionary literature on disgust (Rozin, Haidt, &amp; McCauley, 2000). The results of this cross-disciplinary review produced five top candidates for being the psychological "foundations" upon which cultures construct their moralities. These five foundations are consistent with, but expand upon, several existing taxonomies of moral concern, including Fiske's (1992) four models of social relationships, Shweder et al.'s (1997) account of the "three ethics" of autonomy, community, and divinity that are found widely around the world, and Hogan et al. 's (1978)  evolution-based socioanalytic theory of moral development. MFT can therefore be seen as an attempt to specify the "evolved psychological mechanisms" that were part of the definition of moral systems given earlier. Even if all moral systems are social constructions, they are constructed by people whose minds are not at all like blank slates (Marcus, 2004). In this way, MFT allows for intuitive or emotional bases of moral judgments, as well as more deliberate reasoning processes (cf. Greene et al., 2001;Haidt, 2001). Haidt and Graham (2007) expanded the theory and modified the names of the foundations to become: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/ sanctity. Harm and Fairness generally correspond to Shweder at al.'s (1997) ethics of autonomy; Ingroup and Authority to the ethics of community; and Purity to the ethics of divinity. Haidt and Graham also applied the theory to a particular kind of cultural variation within the United States: the "culture war" between political liberals and conservatives. Drawing on Shweder and several political theorists (e.g., Burke, 1790Burke,  /2003 Mill,  1859Mill, /2003Lakoff, 1996;Muller, 1997;Sowell, 2002), liberalism was hypothesized to indicate a morality in which the individual is the locus of moral value. In such a moral world, moral regulation revolves around protecting individuals from harm or unfair treatment by other individuals, or by the social system. In contrast, conservatives-at least, the social conservatives of the religious right-try to create more tightly-ordered communities in which (for example) proper relationships between parent and child, man and woman, and human and God are part of the aim of moral regulation. In such a moral world, the individual is not the primary locus of moral value; the building block of society is thought to be the family, and a much greater emphasis is placed on virtues and institutions that bind people into roles, duties, and mutual obligations. This analysis led to the hypothesis that liberal morality would prioritize Harm and Fairness over the other three foundations (because the "individualizing foundations" of Harm and NIH-PA Author ManuscriptFairness are all that are needed to support the individual-focused contractual approaches to society often used in enlightenment ethics), whereas conservative morality would also incorporate Ingroup, Authority, and Purity to a substantial degree (because these "binding foundations" are about binding people together into larger groups and institutions). The first draft of the Moral Foundations Questionnaire was created in part to test this hypothesis about ideological differences, and was useful (along with other methods, such as content coding of religious sermons) in supporting the hypothesis (Graham, et al., 2009). However, our goal in subsequent theory and measure development -reported in this paperwas much broader. Moral Foundations Theory provides a conceptual organization for measuring and describing differences in moral concerns across individuals, social groups, and cultures. In theory, any pattern of "settings" or endorsement levels for the five foundations is possible. Thus, a reliable and valid scale was needed to measure the degree to which any individual's moral beliefs and concerns rely upon each of the five hypothesized foundations. Further, theory-driven scale validation is a means of testing hypotheses and theoretical claims (cf. Hogan &amp; Nicholson, 1998). We describe the development of the MFQ below, along with what it revealed about the structure and variation of the moral domain.Because we wanted to gauge individual differences in the range of concerns that people consider morally relevant, the first version of the MFQ (reported in Graham et al., 2009, Study 1, Appendix A) explicitly asked participants to evaluate the moral relevance of several foundation-related concerns (e.g., "Whether or not some people were treated differently from others," for Fairness). For the second version (reported in Graham et al., 2009, Study 2, Appendices A and B) we added a new section that assessed levels of agreement with more specific and contextualized moral judgment statements (see below). These first two versions of the MFQ were tested on heterogeneous populations (using ProjectImplicit.org, a popular web-based virtual laboratory), with large sample sizes (total n=3,825). With these data, we compared different confirmatory factor analysis models that corresponded to distinct, theory-guided conceptualizations of the possible factor structure (see supplementary materials at MoralFoundations.org). Using the comparative model fitting techniques described below, we found evidence that a five-factor solution was an improvement (weighing both fit and parsimony) over models representing a single morality factor, two factors (an "individualizing" factor underlying Harm and Fairness items, and a "binding" factor underlying all other items), and three factors (corresponding to Shweder's three ethics of autonomy, community, and divinity). While the overall model fits were reasonably good (see supplements), some of the internal consistencies of these early versions of the scale were low. In addition, some items had weak loadings on the latent factors. Correlation matrices of all items in the second version revealed that some items that were written to represent one foundation actually related more strongly to another foundation, and some items correlated so highly with each other that they appeared redundant. Problematic items were replaced with new items in the third version of the scale. Pilot testing for the third version of the MFQ was extensive, involving data from over 28,000 participants (surveyed at YourMorals.org) and external validations with eight conceptually related scales. Item and factor analyses on this third version showed improvements over versions 1 and 2. No subscale had more than one item that loaded poorly on the latent factor, so we focused our item analyses on determining how reducing from 40 to 30, 20, or even 10 items would impact the validity of the scale. We developed a novel method for empirically selecting item combinations that would maximize both internal and external validity. For all 10 subscales (four relevance items or four judgments items for each of the five foundations) we identified three criterion scales. The first criterion scale for each subscale was the corresponding foundation subscale using the alternative format (for instance, Harm-judgments as internal validity criterion for Harmrelevance). The second criterion subscale was the corresponding foundation subscale from the "taboo-tradeoffs" questionnaire (see Graham, et al., 2009, Study 3, Appendix C). Modeled on work by Tetlock, Kristel, Elson, Green, and Lerner (2000), this questionnaire asks participants to indicate how much money they would require to perform actions that violate the foundations in a variety of ways (for instance, "kick a dog in the head, hard" for Harm), including options of performing the violation for free, as well as refusing to perform it for any amount of money. The third criterion (used for both relevance and judgments subscales) was an external scale we expected to be related to one particular moral foundation. For Harm the external criterion was the empathy subscale of the Interpersonal Reactivity Index (Davis, 1983); for Fairness, the Schwartz Equality value item (Schwartz, 1992); for Ingroup, the Schwartz Loyalty value item (Schwartz, 1992); for Authority, the Right-Wing Authoritarianism scale (Zakrisson, 2005); and for Purity, the Disgust ScaleRevised (Haidt, McCauley, &amp; Rozin, 1994, modified by Olatunji et al., 2008. In order to quantify how the quality of the scale would decline if shortened, and in order to select the combination of items that would retain the greatest internal and external validity, we calculated correlations between every combination of items and the three criterion scales, as well as a combined item-total correlation with all three criterion scales. An example for Harm-Relevance is shown in Figure 1 (detailed reports of the criterion analyses for all ten subscales can be found in the supplements). The top left panel of Figure 1 plots the corrected item-total correlations with the three criterion scales (Harm-judgments, Harm-tabootradeoffs questionnaire, and IRI-empathy, shown in the other three panels) for every single Harm-relevance item, every 2-item combination, every 3-item combination, and the 4-item aggregate. One can see in this figure that a single item, H ("whether or not someone was harmed") was negatively impacting the internal and external validity of the Harm-relevance aggregates that included it, and that a 3-item aggregate excluding it had even better internal and external reliabilities than the 4-item aggregate. All ten of these analyses revealed that using the best three-item combination yielded internal and external validities as good if not better than using all four items; moreover, we found that in most cases the optimal 2-item combinations were nearly as good as the 3-item combinations (although the 3-item combinations were preferable for their broader conceptual coverage). The best three items from each subscale were retained for the fourth and final version of the MFQ, shown in the Appendix. The 20 starred items make up the short-form MFQ.Throughout the several rounds of item generation and selection, we sought to minimize conceptual and empirical redundancy among items. On this point, we differ from some approaches to scale development that prioritize high internal consistency. Internal consistency is important, but so is comprehensive coverage of the various facets of the construct (the scale development work of Harrison Gough [1979, 1984]  is a good example of this balance; see also John &amp; Soto, 2007). From our point of view, it is better to have dissimilar items that are moderately correlated but that each capture a different facet of a foundation than it is to have similar items that are highly correlated and capture only a small amount of the foundation's scope. As such, our aim in item analysis was not to maximize internal consistency via item redundancy. Instead, we sought a balance between achieving (a) sufficient internal consistency to believe that there was a common core, and (b) maximal item heterogeneity to increase confidence that we were representing the foundation in full. For the moral relevance items, we attempted to cover a wide conceptual area for each foundation while avoiding obvious culture-war issues (e.g., one item is about the moral relevance of rights violations in the abstract, without specifying particular content such as gun rights or gay rights). Items were generated to capture different instances of a foundational moral concern, for instance asking about group loyalty in reference to different specific groups (nation, family) as well as to one's group left in the abstract. We had two reasons for adding the judgments subscale to the relevance subscale. First, we wanted multiple response formats to minimize the impact of variance based on response set (for instance, some people may be more likely to indicate that everything is morally relevant). Second, we wanted to supplement the abstract relevance assessments-which, as self-theories about how one makes moral judgments, may be inaccurate with regard to actual moral judgments (Nisbett &amp; Wilson, 1977)-with contextualized items that could more directly gauge actual moral judgments. To fill out the judgments subscale, participants need not directly consider or be aware of the basis for their moral judgments; they just need to decide whether they endorse or reject the action or event. In this way, the "relevance" subscale may better assess explicit theories about what is morally relevant, and the "judgments" subscale may better assess actual use of moral foundations in judgment (see initial evidence for this conclusion in Graham et al., 2009, Study 2). These judgments took the form of normative declarations (e.g., "It can never be right to kill a human being," for Harm), hypotheticals (e.g., "If I were a soldier and disagreed with my commanding officer's orders, I would obey anyway because that is my duty," for Authority), virtue endorsements (e.g., "Chastity is an important and valuable virtue," for Purity), and opinions about government policies (e.g., "When the government makes laws, the number one principle should be ensuring that everyone is treated fairly," for Fairness). With this variety of both item formats and specific item content, the final version of the MFQ gauges sensitivities to basic kinds of moral concerns, not just opinions on specific moral issues. Sample sizes, number of items, and alphas from pilot testing on all four versions of the MFQ are shown in Table 1. Below we describe validity analyses on the fourth and final version of the scale shown in the Appendix.Participants were 34,476 adults (37% female; mean age 36.2) who had previously registered at YourMorals.org and selected to take the Moral Foundations Questionnaire. Participants come to the website via many different routes, identified using a web tracker. In this sample, 40.5% entered the URL directly into their browser. The most common referring sites were edge.org (29.7%), search engines (8.1%), message boards (3.9%), ted.com (2.2%), and alternet.org (1.9%). (For an analysis demonstrating that response patterns on the MFQ are similar for participants from diverse referring sources, see Iyer, 2009). The relevance section preceded the judgments section, and items within each section were given in an order randomized for each participant. A subset of this sample also chose to take one or several other surveys on the site. This is the sample and procedure used for all internal and external validity analyses except the test-retest, pragmatic, and incremental predictive validity analyses, which are detailed below.Means, standard deviations, and alphas for each subscale of the MFQ are presented in Table  2. Means are also given separately for liberals, moderates, conservatives, and libertarians, to Relations between the relevance and judgments subscales are shown in Table 3. The top panel shows zero-order correlations. To ensure that these relationships were not solely driven by common relations to political ideology, the bottom panel shows partial correlations controlling for politics. Both panels show convergent validity for each foundation as measured by the two formats, as well as discriminant validity in that these relations are stronger than relations between different foundations, despite high correlations among many of the foundations (average same-foundation r = .48, average differentfoundation r = .14).2We gave the MFQ to 123 college students (mean age = 20.1, 69.9% female) from the University of Southern California. After an average interval of 37.4 days (range 28 to 43 days), participants completed the MFQ a second time. In both instances, the MFQ was administered via a class website which recorded the date and time of completion automatically. Question order was randomized for each participant each time.  Table 2. This suggests that item responses are quite stable over time and that within-occasion variation is more a function of the broad diversity of measurement rather than instability.Although Moral Foundations Theory predicts a specific factor structure for moral concerns, we began with exploratory factor analyses to see what factors emerged from the items in the absence of conceptual constraints. Factor analysis for all 30 items of the MFQ was performed using direct oblimin rotation with Kaiser normalization (allowing the factors to be correlated) and maximum likelihood estimation (Fabrigar, Wegener, McCallum, &amp;  Strahan, 1999). Six factors with eigenvalues greater than 1 emerged, but scree plot and factor loadings indicated that only the first two factors provided meaningful incremental explanatory power and interpretability (only 2 of the 120 loadings on the last four factors were above .3; see also Costello &amp; Osborne, 2005, on factor retention from scree plot analysis). The first two factors were retained, and are shown in Table 4. As the table indicates, the two factors clearly corresponded to the binding foundations (Ingroup, Authority, and Purity; factor 1) and individualizing foundations (Harm and Fairness; factor 2), and the strongest loading for all 30 items was as predicted. Although this analysis supported our distinction between individual-focused and groupfocused moral concerns, it remains an open question whether we are justified in treating the theoretically-derived moral foundations as five factors, rather than two. To answer this question we turn to comparisons between different confirmatory factor analysis models. 2 Correlations between the latent foundation factors can be found in Figure 2 (five correlated factors model), and zero-order correlations between the factors are available in a supplement at the first three authors' websites.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author ManuscriptThe large sample sizes we obtained for each version of the MFQ allowed us to create structural equation models comparing different theoretically-derived factor structures. Table  5 describes the comparative model fitting with the final version of the MFQ. The first three numerical columns provide fit statistics for the individual models, and the last two columns show the degree to which each model was an improvement over the model in the row above it. Figure 2 shows the different confirmatory factor analysis models for the full scale; as Table 5 reflects, the same models were constructed for the relevance and judgments subscales separately as well. In the first step, we compared nested first-order models. Our hypothesis was that model 4 (five correlated factors: Harm, Fairness, Ingroup, Authority, and Purity) would provide a better overall model fit than a single morality factor model (1), two-factor model (2: individualizing and binding, corresponding to the results of the exploratory factor analysis), and three-factor model (3: corresponding to Shweder et al.'s (1997) ethics of autonomy, community, and divinity). All three tests (relevance subscale, judgments subscale, and full MFQ) confirmed these predictions; the overall best model (weighing fit and parsimony) was the five-factor model in every case.3 In the second step, we tested whether the five factors could be more parsimoniously modeled with two correlated superordinate factors representing our theoretical distinction of "individualizing" and "binding" foundations (see the hierarchical model in Figure 2). As Table 6 shows, however, the model with five intercorrelated factors was a significant improvement (again, weighing both fit and parsimony) over the hierarchical models. In general, confirmatory factor analyses provide robust support for our five-factor conceptualization of the moral foundations.We identified several other scales and scale items also taken by participants at YourMorals.org that we predicted would relate to the MFQ foundation scores. Scales were grouped into five external criteria scale sets, one set for each foundation. Harm criterion scales were the empathic concern subscale of the Interpersonal Reactivity Index (Davis, 1983; n=134), Levenson's (1995) Psychopathy Scale (reverse-scored; n=116), Schwartz's (1992; n=4,228)  Benevolence subscale, and three items from the Adapted Good-Self Assessment (Barriga, Morrison, Liau, &amp; Gibbs, 2001; n=89) on the importance of being kind/caring, sympathetic/compassionate, and generous/giving. Fairness scales were Social Dominance Orientation (reverse-scored, as it measures preference for social inequalities; Pratto, Sidanius, Stallworth, &amp; Malle, 1994; n=1,215), importance of being fair/just on Barriga's Good-Self scale, and endorsement of the social justice item on Schwartz's values scale. Ingroup scales were the importance of being loyal/faithful on Barriga's Good-Self scale, and endorsement of loyalty, national security, and family security items on Schwartz's values scale. Authority scales were Right-Wing Authoritarianism (Zakrisson, 2005;  n=1,093), the Traditionalism subscale of the Progressive and Traditional Justice scale (Haidt, Darley, &amp; Gromet, 2009; n=1,384), and endorsement of the social order, authority, respect for tradition, honoring parents, and obedience values on Schwartz's value scale. Purity scales were the Disgust Scale-Revised (Haidt, McCauley &amp; Rozin, 1994 modified by  Olatunji et al., 2008 n=1,681), self-reported religious attendance (n=32,607), and 3  We also tested whether a six-factor model separating Authority and Tradition (shown in Figure 2) would improve upon the fivefactor model. However, these six-factor models were a worse fit than the five-factor models, 0.60   a  0.65; in addition, as Figure 2 shows, the Authority and Tradition factors were very highly correlated (r = .96), further supporting a single latent factor for these items.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author Manuscript NIH-PA Author Manuscript endorsement of the self-discipline, clean, and devout items on Schwartz's values scale. Items from the same scale were averaged together, and correlations between the foundations and the scales were averaged together for each criterion group. Correlations between the foundations and the external criterion scales are shown in Table 7. As the table shows, each foundation was the strongest predictor for its own conceptually related group of external scales (average r = .51, vs. average r = .14 for the off-diagonals). This provides evidence of both convergent and discriminant validity, despite relatively substantial relations among the foundations.A subset of the participants who took the MFQ also took a survey in which they reported their gut reactions to various social groups. We constructed this survey by first identifying groups conceptually related to each foundation because they represent either virtues or vices of that foundation. For instance, we predicted that people whose morality rested heavily on the Harm/care foundation would have especially positive reactions to "caring" groups such as nurses, and especially negative reactions to "harming" groups like hunters. The foundation relevance of each group was identified a priori by the authors. Harm-related groups were nurses, environmentalists, pacifists, vegetarians, and hunters (r). Fairnessrelated groups were ACLU members, labor unions, rich people (r), and CEOs (r). Ingrouprelated groups were Americans, U.S. Government, flag burners (r), and illegal immigrants (r). Authority-related groups were soldiers, police officers, U.S. Marines, U.S. Military, people who spank their children, and anarchists (r). Purity-related groups were virgins, highly religious people, spiritual people, atheists (r), prostitutes (r), homosexuals (r), people who have casual sex (r), and people with tattoos or piercings (r). Groups indicated by "(r)" represented vices of a foundation and were reverse-scored, and for U.S.-specific groups only U.S. citizens were included in the analyses. Because attitudes toward social groups and moral foundations scores are both related to political ideology, we used partial correlations controlling for political ideology to see which groups would be uniquely predicted by one or more foundations. Partial correlations between foundations and all social groups were averaged for each set of foundation-related groups; these averages are shown in Table 8. As the table shows, each foundation was the strongest predictor (above and beyond politics) of attitudes toward conceptually related social groups, providing further evidence of predictive and discriminant validity. Beyond validation of the scale, these results also suggest that attitudes about social groups are in part moral judgments about those groups. Moral Foundations Theory and the MFQ may be useful for researchers who want to know which moral concerns are related to prejudice toward any particular group.The preceding sections establish that the theorized model of Moral Foundations as five interrelated factors is a better fit than other plausible models, and that each of the five factors predict foundation-relevant outcomes. An open question is whether the MFQ has predictive validity beyond existing measures. Because it measures domains that are not present in other theoretical conceptions of morality-Ingroup, Authority, and Purity-it surely expands the predictive validity of morality measures. However, even broader measures exist, such as the Schwartz Values Scale, which measures endorsement of ten broad classes of values. This scale contains several values and subscale factors that conceptually overlap with the moral foundations (e.g., Benevolence with Harm/care, Traditionalism with Authority/respect), and many self-interested values that we consider to be outside the moral domain and not covered by the MFQ (e.g., Achievement, Hedonism). Even though Moral Foundations Theory has a different conceptual starting point (an evolutionary account of why humans have the moral intuitions they do, contra Schwartz's factor-analytic approach), it is nonetheless worthwhile to test whether the MFQ has predictive validity beyond Schwartz's scale in predicting a NIH-PA Author Manuscript NIH-PA Author Manuscript variety of scales, opinions, and self-reported behaviors relevant to morality. Because Schwartz's scale is larger both in terms of subscales (10 vs. 5) and items (58 vs. 30), this is a particularly tough test of the predictive validity of the MFQ. Data for these analyses came from 10,652 visitors to YourMorals.org who took both the MFQ and the Schwartz Values Scale (SVS), 92% of whom also took other scales or measures. We used two-step regressions to test whether the five MFQ subscales added incremental predictive validity beyond the ten SVS subscales for the external criteria (scales and attitudes toward foundation-related social groups) described above, as well as for positions on a wide range of political issues. Note that this analysis focuses on the incremental validity of the aggregate MFQ in comparison to the aggregate SVS, rather than investigating which of the particular moral concerns predict each criterion variable (for such work, see Koleva, Graham, Iyer, Ditto, &amp; Haidt, 2010). In every analysis, the MFQ made a significant improvement to prediction when added to the SVS (average R = 8%, all R significant at p &lt; .001). To provide a point of comparison, we repeated this analysis by adding the 44-item Big Five personality inventory to the SVS, which yielded an average R of only 2%. R and R for the scales and foundation-related social group averages can be found in Table 9. We also investigated R for the MFQ alone, to further compare its predictive validity with that of the SVS. As the bolded values in Table  9 show, the MFQ was actually a more powerful predictor than the SVS for most of the scales and political issue positions, and all of the social group attitudes. Given that the SVS is a comprehensive, large, and well-validated measure of values, the MFQ is clearing a high bar in providing unique predictive validity for outcomes relevant for moral and political psychology, and for the psychology of prejudice.Because every step of the scale development used large heterogeneous samples, we can be more confident about the MFQ's generalizability than if we had used college students only (Sears, 1986). However, the samples obtained at ProjectImplicit.org and at YourMorals.org are not representative of any national or international population -the current sample is disproportionately from the U.S. (80%), white (87%), male (63%), and educated (mean education between "completed college" and "some graduate school") compared to international or U.S. national averages. Thanks to the large sample sizes, however, we were able to test whether the five-factor model of moral concerns was consistent across national and geographic groups. All participants self-reported their current country of residence, the country in which they grew up (if different), and the age at which they moved (if they grew up in a different country). For participants who reported moving to their current country at age 14 or older, the country in which they reported growing up was used for the cross-cultural analysis. We created 12 location codes, four of which indicated the four nations from which the largest number of participants had come (U.S., Canada, U.K., Australia); the other eight location codes indicated multi-nation regions of the world (i.e. East Asia, Middle East). Model fit information for each location code is shown in Table 10. As the table shows, the five-factor model of the MFQ is a reasonable or good fit (all  a &lt; .06, all CFI &gt;.7) for all 11 world regions for which we were able to run the fit analyses, providing evidence that the measurement and theory of five foundational moral concerns is not specific to U.S. or Western participants. Notably, although the five-foundation model is a good fit in all these areas, the data shows much cross-cultural variation in the patterns of moral foundation endorsement. Even controlling for politics, age, gender, religious attendance, and education, world region is a significant (ps &lt; .001) predictor of all five foundation scores, indicating  (Rozin, 2006). For instance, the above validation exercises with external scales and social group attitudes showed that many traits and attitudes that don't seem to be about morality on the surface nevertheless show a systematic and theoretically meaningful relationship to moral concerns measured by the MFQ. We present here three additional findings made possible by MFT's broadened definition of morality, and its finer conceptual resolution of morality's components.Using a variety of measures and methods, Graham et al. (2009) showed that liberals value Harm and Fairness concerns more than conservatives, while conservatives value Ingroup, Authority, and Purity concerns more than liberals. The vast majority of these participants, however, came from the United States, leaving open the question of whether these patterns were limited to the particular ideological conflicts of the United States. Table 11 shows correlations between political ideology4 and the five foundations for the different world areas described in the Generalizability section. The correlations indicate that the liberal-conservative patterns found in the U.S. are robust across national and cultural contexts, both in terms of direction (negative correlations [liberals higher] for Harm and Fairness, positive correlations [conservatives higher] for Ingroup, Authority, and Purity) and in terms of magnitude: correlations are consistently strongest for Authority and Purity, and weakest for Harm. This suggests that across cultures, the most intractable political debates are likely to involve concerns related to respect for traditions/authorities and physical/spiritual purity, while the greatest degree of moral commonality may be found in issues related to harm and care. It also reinforces the claim that political ideology can be self-assessed and that the unidimensional left-right construct has some degree of common meaning across societies, despite differences in political party structures and particular national issues (Jost, 2006). 4 Because the terms "liberal" and "conservative" can mean different things in different nations (i.e., the Liberal Party in Australia is actually center-right ideologically), the political identification item on YourMorals.org clarifies to participants that the items from "very liberal" to "very conservative" should be read as "very left-wing" to "very right-wing." Graham et al. 06) compared to Western participants, and were only very slightly more concerned about Harm, Fairness, and Authority (mean differences &lt; .1, ts &lt; 7, ds &lt; .04). The fact that differences are concentrated in Ingroup and Purity makes some sense in light of established cultural differences in collectivism (Triandis, 1995) and the role of purity concerns in daily life and religious practice, particularly in South Asia (Shweder et al., 1997). But it is noteworthy that there are not large differences in Authority, given greater sensitivity to social hierarchy (Power Distance scores) in eastern nations (Hofstede, 2001). The small effect sizes for all the East-West differences suggest that variation within cultures (e.g., by gender or political ideology) will exceed the east-West variations given so much attention in crosscultural research. Here we see that the increased resolution afforded by MFT allows us to find moral differences we would not have been able to find otherwisedifferences that open up intriguing questions for further research. As the effect sizes show, these gender differences were much stronger than the differences between Eastern and Western cultures. The gender patterns make sense in light of previous research on empathy (Davis, 1983), egalitarianism (Arts &amp; Gelissen, 2001), and disgust sensitivity (Druschel &amp; Sherman, 1999), but they also show an important divergence from the political patterns in that Purity is here grouped with Harm and Fairness, rather than Ingroup and Authority. Here too the finer resolution and broadened scope of MFT allowed us to find and describe differences in moral personality not possible before.Moral Foundations Theory (Haidt &amp; Joseph, 2004;Haidt &amp; Graham, 2007) was created by selecting the closest links between evolutionary accounts of human sociality and anthropological accounts of the breadth and variability of the moral domain (see especially Fiske, 1992, andShweder et al., 1997). The findings reported in this article suggest that those anthropologists were right. From a purely descriptive perspective, the domain of morality consists of more than just "prescriptive judgments of justice, rights, and welfare" (Turiel, 1983, p. 3). Furthermore, we found that one does not need to travel to non-Western nations to find this broader conception of morality. In every country and world region we examined, people on the political right placed greater emphasis on concerns about ingroup loyalty, respect for authorities and traditions, and physical/spiritual purity than did people on the political left. The Moral Foundations Questionnaire fills the need for a theoretically-grounded scale covering the full range of human moral concerns. We found substantial evidence that the scale is reliable and valid. The scale is internally consistent (both within and between two question formats) while maintaining conceptual coverage of diverse manifestations of foundation-related concerns. Test-retest analyses showed stability of foundation subscale scores over time. External validations of the MFQ using widely-used scales, as well as attitudes toward conceptually related social groups, showed convergent, discriminant, and predictive validity. Factor analyses confirmed our theoretical parsing of the moral domain into five sets of concerns: the five-factor model fit the data better (weighing both fit and parsimony) than competing models, and this five-factor representation provided a good fit for participants in 11 different world areas. In addition to the scale itself, we expect that the method introduced in this paper (see Figure 1) for empirically selecting items to maximize both internal and external validity will also be of use to researchers. The best existing instrument for assessing moral concerns beyond harm and fairness is the Schwartz Values Scale (SVS; Schwartz, 1992), which includes group-level values such as "tradition" and "conformity." However, the SVS was created to measure a broad spectrum of values; it was not designed to cover the moral domain specifically, and it does not cover concerns about group-loyalty and spiritual purity. In a head-to-head comparison, the MFQ showed incremental predictive validity beyond the SVS for a diverse array of external scales related to moral personality, attitudes toward social groups, and opinions about moral and political issues (see Table 9). Further, in most cases, the MFQ performed even better than the SVS in overall variance explained of criterion variables, despite its shorter length and narrower conceptual coverage. This further illustrates the MFQ's effective measurement properties, balancing relatively short length and wide coverage of the moral domain.The research reported here indicates that the MFQ is a reliable and valid instrument for measuring a broad range of moral concerns. But in the process of developing and validating the MFQ, we also generated a number of substantive discoveries about moral psychology, such as:A map of the moral domain. Because it distinguishes five kinds of moral concerns, and gives separate evolutionary accounts to explain each of their origins (Haidt &amp; Joseph, 2007), MFT is not as parsimonious as theories of morality that try to derive the entire moral domain from one or two principles or processes (usually kin selection plus reciprocal altruism -see Dawkins, 1976;Hauser, 2006;Joyce, 2006). However, comparisons of different structural models revealed that the five-factor solution is an improvement over other theoretically-derived models, even taking into account the relative loss of parsimony. Analyses of international data showed that this five-factor model was a good fit in every area of the world we were able to examine. This provides empirical evidence for MFT's central claim about the structure of human morality, and points toward the usefulness of this added A guide to where the action is. We found some small and easily interpretable crosscultural differences in moral foundation scores: people in Eastern cultures were slightly more likely to value Ingroup and Purity than people in Western cultures. As with all research that relies upon educated participants, our cross-national differences would probably have been much larger if we had found a way to survey rural villagers and the urban poor in Asia. Nonetheless, the cross-national differences we did find were dwarfed by the within-nation (or within-region) differences we examined, including both ideological differences and sex differences (women valued Harm, Fairness, and Purity more than men, even controlling for political ideology). With reliable measures of these different kinds of moral concerns, social and personality psychologists can now begin to examine many such patterns of similarities and dissimilarities, as well as the processes behind them.A method for discovering moral prejudices. The finer resolution offered by the MFQ also revealed the potential role of moral judgment in prejudice. When we examined attitudes toward various social groups, we found that MFQ subscales indicated varying patterns of moral concerns that might lead some people to dislike some groups. This suggests that attitudes toward social groups may often be expressions of moral judgments about those groups -vague intuitions or explicit convictions that a particular social group upholds or violates one or more foundational concerns. The moral foundations can thus be used as a kind of decoder ring, allowing us to see multiple and sometimes unexpected moral threads connecting seemingly unrelated attitudes and opinions (cf. Koleva et al, 2010). This possibility emphasizes our functional definition of morality as a description of what motivates people to suppress selfishness, rather than a prescriptive definition of how one ought to behave. By describing and quantifying a broadened range of human moral concerns, MFT and the MFQ can aid in our understanding of the dangers of morality, as well as the benefits.The map of the moral domain that we offer is provisional. We hope that other researchers will help us improve it. Here are four next steps:
   </itunes:summary>
<description>
    The moral domain is broader than the empathy and justice concerns assessed by existing measures of moral competence, and it is not just a subset of the values assessed by value inventories. To fill the need for reliable and theoretically-grounded measurement of the full range of moral concerns, we developed the Moral Foundations Questionnaire (MFQ) based on a theoretical model of five universally available (but variably developed) sets of moral intuitions: Harm/care, Fairness/ reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. We present evidence for the internal and external validity of the scale and the model, and in doing so present new findings about morality: 1. Comparative model fitting of confirmatory factor analyses provides empirical justification for a five-factor structure of moral concerns. 2. Convergent/discriminant validity evidence suggests that moral concerns predict personality features and social group attitudes not previously considered morally relevant. 3. We establish pragmatic validity of the measure in providing new knowledge and research opportunities concerning demographic and cultural differences in moral intuitions. These analyses provide evidence for the usefulness of Moral Foundations Theory in simultaneously increasing the scope and sharpening the resolution of psychological views of morality. Keywords morality; scale validation; moral foundations; culture; values Correspondence concerning this article should be sent to jgraham@virginia.edu. Publisher's Disclaimer: The following manuscript is the final accepted manuscript. It has not been subjected to the final copyediting, fact-checking, and proofreading required for formal publication. It is not the definitive, publisher-authenticated version. The American Psychological Association and its Council of Editors disclaim any responsibility or liabilities for errors or omissions of this manuscript version, any version derived from this manuscript by NIH, or other third parties. The published version is available at www.apa.org/pubs/journals/PSP Supplemental data and analyses can be found at www.moralfoundations.org.Author Manuscript J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1. NIH-PA Author ManuscriptHow can we measure moral concerns when people disagree about what "morality" means? To address this problem we created the Moral Foundations Questionnaire (MFQ, presented in the appendix), a measure of the degree to which individuals endorse each of five intuitive systems posited by Moral Foundations Theory: Harm/care, Fairness/reciprocity, Ingroup/ loyalty, Authority/respect, and Purity/sanctity (Haidt &amp; Graham, 2007;Shweder, Much, Mahapatra &amp; Park, 1997). People vary in the extent to which they endorse, value, and use these five foundations, providing an opportunity to better understand moral diversity. In this article, we explain the need for a scale broader than conventional morality scales. We describe the development of the scale, which involved multiple rounds of item analysis using large heterogeneous samples. We present the validation of the scale, organized into evidence confirming internal and external validity. Finally, we describe its pragmatic validity -that is, the practical usefulness of both the theory and the measure in providing new insights about moral psychology. A major goal of Moral Foundations Theory is to expand the range of phenomena studied in moral psychology so that it matches the full range of moral concerns, including those found in non-Western cultures, in religious practices, and among political conservatives. Here we present what we have learned about the content and structure of the moral domain using the Moral Foundations Questionnaire.A great variety of scales are used in moral psychology to measure stages of moral reasoning (e.g., the Defining Issues Test-2; Rest et al., 1999), moral identity (e.g., the Moral Identity Scale; Aquino &amp; Reed, 2002), empathy (e.g., the Interpersonal Reactivity Index; Davis, 1983), and moral deficits such as psychopathy (e.g., Levenson's Self-report Psychopathy Scale; Levenson et al., 1995). Although these scales measure different aspects of morality, they all share the assumption (explicit or implicit) that the moral domain is limited to concerns about individuals harming or unfairly treating other individuals. This is, in part, a reflection of the dominance of Lawrence Kohlberg's (1969Kohlberg's ( , 1971 ideas about morality as justice, and his subsequent debate with Carol Gilligan (1982) about her alternative conception of morality as care. Both sides agreed that morality was about how well or poorly individuals treated other individuals. Turiel (1983, p.3) codified this approach into a widely-cited definition of the moral domain as "prescriptive judgments of justice, rights, and welfare pertaining to how people ought to relate to each other." Any values that were not related to "justice, rights, and welfare" (e.g., patriotism, authority, or chastity) were considered non-moral, and were relegated to the domain of social convention or the domain of personal choice (Turiel, Hildebrandt, &amp; Wainryb, 1991). Definitions of morality in philosophy also frequently stress rules or codes of conduct that reduce harm to others (e.g., Gert, 2005;Singer, 1979). Kohlberg certainly noticed that people sometimes justified moral judgments by referring to group-level moral concerns such as authority, loyalty, and tradition, but he thought that such thinking was immature and conventional, a part of the "law and order" ethos of stage 4. With sufficient opportunities for role-taking, adolescents were said to move beyond stage 4 and to begin using post-conventional reasoning based on an understanding of justice. Kohlberg's stage theory has been criticized on many grounds; one criticism relevant for our purposes was that Kohlberg's post-conventional morality enshrined politically liberal ideals as developmental endpoints (Hogan, Johnson, &amp; Emler, 1978;Shweder, 1982;Sullivan, 1977;  for related critiques see Puka, 1994). This critique was backed up by the demonstration that liberals routinely obtain higher principled reasoning scores on the Defining Issues Test, but that conservative students rose to liberal levels when told to "respond as a left-winger would" (Emler, Renwick, &amp; Malone, 1983). Conservatives could reason at the "higher stage," but were not doing so presumably because they had different priorities in their moral reasoning. Despite these critiques, the notion that "true" moral  Oliveira-Souza, &amp; Grafman, 2006). Whether carried out in a scanner, on a website, or in a business school lab room using real money, morality is still usually operationalized as helping (vs. harming) or as playing fair (vs cheating). Kohlberg and Turiel based their circumscription of the moral domain on a line of enlightenment thinking running from Immanuel Kant to John Stuart Mill to John Rawls in which the autonomy and/or welfare of the individual are the starting point for ethical inquiry. Yet even Kant (1797Kant ( /1996) had intuitions of a broader moral domain. He wrote that masturbation was "in the highest degree opposed to morality," although he granted that "it is not so easy to produce a rational demonstration" of its wrongness. And before the Enlightenment, philosophers routinely considered a much broader moral domain. Much of ancient moral philosophy, from Greece to India to Japan, was virtue-based. These societies valued benevolence and fairness, but they also emphasized group-level concerns about social order, authority, duty, loyalty to one's family or group, and controlling one's carnal desires to cultivate one's soul or gain a more favorable rebirth (Larue, 1991;Shweder et al., 1997 Cross-cultural research on moral judgment has revealed that Turiel's definition of the moral domain works well among educated and politically liberal Westerners, for whom harmless offenses are rarely condemned, even when they are disgusting or disrespectful (Haidt, Koller, &amp; Dias, 1993).1 However, research on people in India (Shweder, Mahapatra, &amp; Miller, 1987) Dias, 1993), and conservatives in the United States (Graham, Haidt, &amp; Nosek, 2009;Haidt &amp; Hersh, 2001;Jensen, 1998) has revealed moral considerations beyond the individualbased concerns of harm and fairness, involving concerns about spiritual purity and degradation (even for acts that involve no harm), concerns about proper hierarchical role fulfillment, and moral expectations of loyalty to the local or national group. Illustrative of this breadth are open-ended responses participants in our studies gave when asked to define morality in their own words. Parallel to Turiel's definition, many made reference to harm and human welfare (e.g., "Avoiding harm to others"), and fairness or justice ("Morality is doing the right things to ensure fair treatment for all"). However, others made reference to moral issues beyond justice, rights, and welfare, and to morally valuable entities that were not individuals (e.g., "Morality is having a system [that] protects the social institutions of family, community, and country."). Others made reference to duty, obedience, respect, and preserving tradition (e.g., "Matters of duty, irrespective of one's own personal desires or ends"). And some made reference to God or religious norms, decency, the soul, and maintaining purity of mind (e.g., "not having dirty thoughts"  (Rokeach, 1973;Schwartz, 1992). Values research has much to offer the empirical study of morality, and is too often ignored by moral psychologists. Clearly, many values are moral values, even if morality is defined only in terms of welfare and fairness concerns (e.g., benevolence and universalism). However, in seeking to identify a list of the most important values, there is a risk that some common moral concerns or intuitions will be missed. For example, reciprocity, loyalty to one's team or tribe, and concerns about bodily and spiritual purity are ubiquitous in anthropological accounts of In order to fill the need for a systematic theory of morality, explaining its origins, development, and cultural variations, we created Moral Foundations Theory (MFT). Haidt and Joseph (2004) began by surveying the literatures in evolutionary psychology and anthropology, looking for matches -for virtues and areas of moral regulation that were common (though not necessarily universal) across cultures, and that had some clear counterpart in evolutionary thinking. For example, virtues related to fairness and practices of reciprocal gift exchange (e.g., Mauss, 1924Mauss, /1990) bore an obvious similarity to the evolutionary literature on reciprocal altruism (Trivers, 1971); virtues of purity and practices regulating food and sex (e.g., Douglas, 1966) bore an obvious relationship to the evolutionary literature on disgust (Rozin, Haidt, &amp; McCauley, 2000). The results of this cross-disciplinary review produced five top candidates for being the psychological "foundations" upon which cultures construct their moralities. These five foundations are consistent with, but expand upon, several existing taxonomies of moral concern, including Fiske's (1992) four models of social relationships, Shweder et al.'s (1997) account of the "three ethics" of autonomy, community, and divinity that are found widely around the world, and Hogan et al. 's (1978)  evolution-based socioanalytic theory of moral development. MFT can therefore be seen as an attempt to specify the "evolved psychological mechanisms" that were part of the definition of moral systems given earlier. Even if all moral systems are social constructions, they are constructed by people whose minds are not at all like blank slates (Marcus, 2004). In this way, MFT allows for intuitive or emotional bases of moral judgments, as well as more deliberate reasoning processes (cf. Greene et al., 2001;Haidt, 2001). Haidt and Graham (2007) expanded the theory and modified the names of the foundations to become: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/ sanctity. Harm and Fairness generally correspond to Shweder at al.'s (1997) ethics of autonomy; Ingroup and Authority to the ethics of community; and Purity to the ethics of divinity. Haidt and Graham also applied the theory to a particular kind of cultural variation within the United States: the "culture war" between political liberals and conservatives. Drawing on Shweder and several political theorists (e.g., Burke, 1790Burke,  /2003 Mill,  1859Mill, /2003Lakoff, 1996;Muller, 1997;Sowell, 2002), liberalism was hypothesized to indicate a morality in which the individual is the locus of moral value. In such a moral world, moral regulation revolves around protecting individuals from harm or unfair treatment by other individuals, or by the social system. In contrast, conservatives-at least, the social conservatives of the religious right-try to create more tightly-ordered communities in which (for example) proper relationships between parent and child, man and woman, and human and God are part of the aim of moral regulation. In such a moral world, the individual is not the primary locus of moral value; the building block of society is thought to be the family, and a much greater emphasis is placed on virtues and institutions that bind people into roles, duties, and mutual obligations. This analysis led to the hypothesis that liberal morality would prioritize Harm and Fairness over the other three foundations (because the "individualizing foundations" of Harm and NIH-PA Author ManuscriptFairness are all that are needed to support the individual-focused contractual approaches to society often used in enlightenment ethics), whereas conservative morality would also incorporate Ingroup, Authority, and Purity to a substantial degree (because these "binding foundations" are about binding people together into larger groups and institutions). The first draft of the Moral Foundations Questionnaire was created in part to test this hypothesis about ideological differences, and was useful (along with other methods, such as content coding of religious sermons) in supporting the hypothesis (Graham, et al., 2009). However, our goal in subsequent theory and measure development -reported in this paperwas much broader. Moral Foundations Theory provides a conceptual organization for measuring and describing differences in moral concerns across individuals, social groups, and cultures. In theory, any pattern of "settings" or endorsement levels for the five foundations is possible. Thus, a reliable and valid scale was needed to measure the degree to which any individual's moral beliefs and concerns rely upon each of the five hypothesized foundations. Further, theory-driven scale validation is a means of testing hypotheses and theoretical claims (cf. Hogan &amp; Nicholson, 1998). We describe the development of the MFQ below, along with what it revealed about the structure and variation of the moral domain.Because we wanted to gauge individual differences in the range of concerns that people consider morally relevant, the first version of the MFQ (reported in Graham et al., 2009, Study 1, Appendix A) explicitly asked participants to evaluate the moral relevance of several foundation-related concerns (e.g., "Whether or not some people were treated differently from others," for Fairness). For the second version (reported in Graham et al., 2009, Study 2, Appendices A and B) we added a new section that assessed levels of agreement with more specific and contextualized moral judgment statements (see below). These first two versions of the MFQ were tested on heterogeneous populations (using ProjectImplicit.org, a popular web-based virtual laboratory), with large sample sizes (total n=3,825). With these data, we compared different confirmatory factor analysis models that corresponded to distinct, theory-guided conceptualizations of the possible factor structure (see supplementary materials at MoralFoundations.org). Using the comparative model fitting techniques described below, we found evidence that a five-factor solution was an improvement (weighing both fit and parsimony) over models representing a single morality factor, two factors (an "individualizing" factor underlying Harm and Fairness items, and a "binding" factor underlying all other items), and three factors (corresponding to Shweder's three ethics of autonomy, community, and divinity). While the overall model fits were reasonably good (see supplements), some of the internal consistencies of these early versions of the scale were low. In addition, some items had weak loadings on the latent factors. Correlation matrices of all items in the second version revealed that some items that were written to represent one foundation actually related more strongly to another foundation, and some items correlated so highly with each other that they appeared redundant. Problematic items were replaced with new items in the third version of the scale. Pilot testing for the third version of the MFQ was extensive, involving data from over 28,000 participants (surveyed at YourMorals.org) and external validations with eight conceptually related scales. Item and factor analyses on this third version showed improvements over versions 1 and 2. No subscale had more than one item that loaded poorly on the latent factor, so we focused our item analyses on determining how reducing from 40 to 30, 20, or even 10 items would impact the validity of the scale. We developed a novel method for empirically selecting item combinations that would maximize both internal and external validity. For all 10 subscales (four relevance items or four judgments items for each of the five foundations) we identified three criterion scales. The first criterion scale for each subscale was the corresponding foundation subscale using the alternative format (for instance, Harm-judgments as internal validity criterion for Harmrelevance). The second criterion subscale was the corresponding foundation subscale from the "taboo-tradeoffs" questionnaire (see Graham, et al., 2009, Study 3, Appendix C). Modeled on work by Tetlock, Kristel, Elson, Green, and Lerner (2000), this questionnaire asks participants to indicate how much money they would require to perform actions that violate the foundations in a variety of ways (for instance, "kick a dog in the head, hard" for Harm), including options of performing the violation for free, as well as refusing to perform it for any amount of money. The third criterion (used for both relevance and judgments subscales) was an external scale we expected to be related to one particular moral foundation. For Harm the external criterion was the empathy subscale of the Interpersonal Reactivity Index (Davis, 1983); for Fairness, the Schwartz Equality value item (Schwartz, 1992); for Ingroup, the Schwartz Loyalty value item (Schwartz, 1992); for Authority, the Right-Wing Authoritarianism scale (Zakrisson, 2005); and for Purity, the Disgust ScaleRevised (Haidt, McCauley, &amp; Rozin, 1994, modified by Olatunji et al., 2008. In order to quantify how the quality of the scale would decline if shortened, and in order to select the combination of items that would retain the greatest internal and external validity, we calculated correlations between every combination of items and the three criterion scales, as well as a combined item-total correlation with all three criterion scales. An example for Harm-Relevance is shown in Figure 1 (detailed reports of the criterion analyses for all ten subscales can be found in the supplements). The top left panel of Figure 1 plots the corrected item-total correlations with the three criterion scales (Harm-judgments, Harm-tabootradeoffs questionnaire, and IRI-empathy, shown in the other three panels) for every single Harm-relevance item, every 2-item combination, every 3-item combination, and the 4-item aggregate. One can see in this figure that a single item, H ("whether or not someone was harmed") was negatively impacting the internal and external validity of the Harm-relevance aggregates that included it, and that a 3-item aggregate excluding it had even better internal and external reliabilities than the 4-item aggregate. All ten of these analyses revealed that using the best three-item combination yielded internal and external validities as good if not better than using all four items; moreover, we found that in most cases the optimal 2-item combinations were nearly as good as the 3-item combinations (although the 3-item combinations were preferable for their broader conceptual coverage). The best three items from each subscale were retained for the fourth and final version of the MFQ, shown in the Appendix. The 20 starred items make up the short-form MFQ.Throughout the several rounds of item generation and selection, we sought to minimize conceptual and empirical redundancy among items. On this point, we differ from some approaches to scale development that prioritize high internal consistency. Internal consistency is important, but so is comprehensive coverage of the various facets of the construct (the scale development work of Harrison Gough [1979, 1984]  is a good example of this balance; see also John &amp; Soto, 2007). From our point of view, it is better to have dissimilar items that are moderately correlated but that each capture a different facet of a foundation than it is to have similar items that are highly correlated and capture only a small amount of the foundation's scope. As such, our aim in item analysis was not to maximize internal consistency via item redundancy. Instead, we sought a balance between achieving (a) sufficient internal consistency to believe that there was a common core, and (b) maximal item heterogeneity to increase confidence that we were representing the foundation in full. For the moral relevance items, we attempted to cover a wide conceptual area for each foundation while avoiding obvious culture-war issues (e.g., one item is about the moral relevance of rights violations in the abstract, without specifying particular content such as gun rights or gay rights). Items were generated to capture different instances of a foundational moral concern, for instance asking about group loyalty in reference to different specific groups (nation, family) as well as to one's group left in the abstract. We had two reasons for adding the judgments subscale to the relevance subscale. First, we wanted multiple response formats to minimize the impact of variance based on response set (for instance, some people may be more likely to indicate that everything is morally relevant). Second, we wanted to supplement the abstract relevance assessments-which, as self-theories about how one makes moral judgments, may be inaccurate with regard to actual moral judgments (Nisbett &amp; Wilson, 1977)-with contextualized items that could more directly gauge actual moral judgments. To fill out the judgments subscale, participants need not directly consider or be aware of the basis for their moral judgments; they just need to decide whether they endorse or reject the action or event. In this way, the "relevance" subscale may better assess explicit theories about what is morally relevant, and the "judgments" subscale may better assess actual use of moral foundations in judgment (see initial evidence for this conclusion in Graham et al., 2009, Study 2). These judgments took the form of normative declarations (e.g., "It can never be right to kill a human being," for Harm), hypotheticals (e.g., "If I were a soldier and disagreed with my commanding officer's orders, I would obey anyway because that is my duty," for Authority), virtue endorsements (e.g., "Chastity is an important and valuable virtue," for Purity), and opinions about government policies (e.g., "When the government makes laws, the number one principle should be ensuring that everyone is treated fairly," for Fairness). With this variety of both item formats and specific item content, the final version of the MFQ gauges sensitivities to basic kinds of moral concerns, not just opinions on specific moral issues. Sample sizes, number of items, and alphas from pilot testing on all four versions of the MFQ are shown in Table 1. Below we describe validity analyses on the fourth and final version of the scale shown in the Appendix.Participants were 34,476 adults (37% female; mean age 36.2) who had previously registered at YourMorals.org and selected to take the Moral Foundations Questionnaire. Participants come to the website via many different routes, identified using a web tracker. In this sample, 40.5% entered the URL directly into their browser. The most common referring sites were edge.org (29.7%), search engines (8.1%), message boards (3.9%), ted.com (2.2%), and alternet.org (1.9%). (For an analysis demonstrating that response patterns on the MFQ are similar for participants from diverse referring sources, see Iyer, 2009). The relevance section preceded the judgments section, and items within each section were given in an order randomized for each participant. A subset of this sample also chose to take one or several other surveys on the site. This is the sample and procedure used for all internal and external validity analyses except the test-retest, pragmatic, and incremental predictive validity analyses, which are detailed below.Means, standard deviations, and alphas for each subscale of the MFQ are presented in Table  2. Means are also given separately for liberals, moderates, conservatives, and libertarians, to Relations between the relevance and judgments subscales are shown in Table 3. The top panel shows zero-order correlations. To ensure that these relationships were not solely driven by common relations to political ideology, the bottom panel shows partial correlations controlling for politics. Both panels show convergent validity for each foundation as measured by the two formats, as well as discriminant validity in that these relations are stronger than relations between different foundations, despite high correlations among many of the foundations (average same-foundation r = .48, average differentfoundation r = .14).2We gave the MFQ to 123 college students (mean age = 20.1, 69.9% female) from the University of Southern California. After an average interval of 37.4 days (range 28 to 43 days), participants completed the MFQ a second time. In both instances, the MFQ was administered via a class website which recorded the date and time of completion automatically. Question order was randomized for each participant each time.  Table 2. This suggests that item responses are quite stable over time and that within-occasion variation is more a function of the broad diversity of measurement rather than instability.Although Moral Foundations Theory predicts a specific factor structure for moral concerns, we began with exploratory factor analyses to see what factors emerged from the items in the absence of conceptual constraints. Factor analysis for all 30 items of the MFQ was performed using direct oblimin rotation with Kaiser normalization (allowing the factors to be correlated) and maximum likelihood estimation (Fabrigar, Wegener, McCallum, &amp;  Strahan, 1999). Six factors with eigenvalues greater than 1 emerged, but scree plot and factor loadings indicated that only the first two factors provided meaningful incremental explanatory power and interpretability (only 2 of the 120 loadings on the last four factors were above .3; see also Costello &amp; Osborne, 2005, on factor retention from scree plot analysis). The first two factors were retained, and are shown in Table 4. As the table indicates, the two factors clearly corresponded to the binding foundations (Ingroup, Authority, and Purity; factor 1) and individualizing foundations (Harm and Fairness; factor 2), and the strongest loading for all 30 items was as predicted. Although this analysis supported our distinction between individual-focused and groupfocused moral concerns, it remains an open question whether we are justified in treating the theoretically-derived moral foundations as five factors, rather than two. To answer this question we turn to comparisons between different confirmatory factor analysis models. 2 Correlations between the latent foundation factors can be found in Figure 2 (five correlated factors model), and zero-order correlations between the factors are available in a supplement at the first three authors' websites.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author ManuscriptThe large sample sizes we obtained for each version of the MFQ allowed us to create structural equation models comparing different theoretically-derived factor structures. Table  5 describes the comparative model fitting with the final version of the MFQ. The first three numerical columns provide fit statistics for the individual models, and the last two columns show the degree to which each model was an improvement over the model in the row above it. Figure 2 shows the different confirmatory factor analysis models for the full scale; as Table 5 reflects, the same models were constructed for the relevance and judgments subscales separately as well. In the first step, we compared nested first-order models. Our hypothesis was that model 4 (five correlated factors: Harm, Fairness, Ingroup, Authority, and Purity) would provide a better overall model fit than a single morality factor model (1), two-factor model (2: individualizing and binding, corresponding to the results of the exploratory factor analysis), and three-factor model (3: corresponding to Shweder et al.'s (1997) ethics of autonomy, community, and divinity). All three tests (relevance subscale, judgments subscale, and full MFQ) confirmed these predictions; the overall best model (weighing fit and parsimony) was the five-factor model in every case.3 In the second step, we tested whether the five factors could be more parsimoniously modeled with two correlated superordinate factors representing our theoretical distinction of "individualizing" and "binding" foundations (see the hierarchical model in Figure 2). As Table 6 shows, however, the model with five intercorrelated factors was a significant improvement (again, weighing both fit and parsimony) over the hierarchical models. In general, confirmatory factor analyses provide robust support for our five-factor conceptualization of the moral foundations.We identified several other scales and scale items also taken by participants at YourMorals.org that we predicted would relate to the MFQ foundation scores. Scales were grouped into five external criteria scale sets, one set for each foundation. Harm criterion scales were the empathic concern subscale of the Interpersonal Reactivity Index (Davis, 1983; n=134), Levenson's (1995) Psychopathy Scale (reverse-scored; n=116), Schwartz's (1992; n=4,228)  Benevolence subscale, and three items from the Adapted Good-Self Assessment (Barriga, Morrison, Liau, &amp; Gibbs, 2001; n=89) on the importance of being kind/caring, sympathetic/compassionate, and generous/giving. Fairness scales were Social Dominance Orientation (reverse-scored, as it measures preference for social inequalities; Pratto, Sidanius, Stallworth, &amp; Malle, 1994; n=1,215), importance of being fair/just on Barriga's Good-Self scale, and endorsement of the social justice item on Schwartz's values scale. Ingroup scales were the importance of being loyal/faithful on Barriga's Good-Self scale, and endorsement of loyalty, national security, and family security items on Schwartz's values scale. Authority scales were Right-Wing Authoritarianism (Zakrisson, 2005;  n=1,093), the Traditionalism subscale of the Progressive and Traditional Justice scale (Haidt, Darley, &amp; Gromet, 2009; n=1,384), and endorsement of the social order, authority, respect for tradition, honoring parents, and obedience values on Schwartz's value scale. Purity scales were the Disgust Scale-Revised (Haidt, McCauley &amp; Rozin, 1994 modified by  Olatunji et al., 2008 n=1,681), self-reported religious attendance (n=32,607), and 3  We also tested whether a six-factor model separating Authority and Tradition (shown in Figure 2) would improve upon the fivefactor model. However, these six-factor models were a worse fit than the five-factor models, 0.60   a  0.65; in addition, as Figure 2 shows, the Authority and Tradition factors were very highly correlated (r = .96), further supporting a single latent factor for these items.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author Manuscript NIH-PA Author Manuscript endorsement of the self-discipline, clean, and devout items on Schwartz's values scale. Items from the same scale were averaged together, and correlations between the foundations and the scales were averaged together for each criterion group. Correlations between the foundations and the external criterion scales are shown in Table 7. As the table shows, each foundation was the strongest predictor for its own conceptually related group of external scales (average r = .51, vs. average r = .14 for the off-diagonals). This provides evidence of both convergent and discriminant validity, despite relatively substantial relations among the foundations.A subset of the participants who took the MFQ also took a survey in which they reported their gut reactions to various social groups. We constructed this survey by first identifying groups conceptually related to each foundation because they represent either virtues or vices of that foundation. For instance, we predicted that people whose morality rested heavily on the Harm/care foundation would have especially positive reactions to "caring" groups such as nurses, and especially negative reactions to "harming" groups like hunters. The foundation relevance of each group was identified a priori by the authors. Harm-related groups were nurses, environmentalists, pacifists, vegetarians, and hunters (r). Fairnessrelated groups were ACLU members, labor unions, rich people (r), and CEOs (r). Ingrouprelated groups were Americans, U.S. Government, flag burners (r), and illegal immigrants (r). Authority-related groups were soldiers, police officers, U.S. Marines, U.S. Military, people who spank their children, and anarchists (r). Purity-related groups were virgins, highly religious people, spiritual people, atheists (r), prostitutes (r), homosexuals (r), people who have casual sex (r), and people with tattoos or piercings (r). Groups indicated by "(r)" represented vices of a foundation and were reverse-scored, and for U.S.-specific groups only U.S. citizens were included in the analyses. Because attitudes toward social groups and moral foundations scores are both related to political ideology, we used partial correlations controlling for political ideology to see which groups would be uniquely predicted by one or more foundations. Partial correlations between foundations and all social groups were averaged for each set of foundation-related groups; these averages are shown in Table 8. As the table shows, each foundation was the strongest predictor (above and beyond politics) of attitudes toward conceptually related social groups, providing further evidence of predictive and discriminant validity. Beyond validation of the scale, these results also suggest that attitudes about social groups are in part moral judgments about those groups. Moral Foundations Theory and the MFQ may be useful for researchers who want to know which moral concerns are related to prejudice toward any particular group.The preceding sections establish that the theorized model of Moral Foundations as five interrelated factors is a better fit than other plausible models, and that each of the five factors predict foundation-relevant outcomes. An open question is whether the MFQ has predictive validity beyond existing measures. Because it measures domains that are not present in other theoretical conceptions of morality-Ingroup, Authority, and Purity-it surely expands the predictive validity of morality measures. However, even broader measures exist, such as the Schwartz Values Scale, which measures endorsement of ten broad classes of values. This scale contains several values and subscale factors that conceptually overlap with the moral foundations (e.g., Benevolence with Harm/care, Traditionalism with Authority/respect), and many self-interested values that we consider to be outside the moral domain and not covered by the MFQ (e.g., Achievement, Hedonism). Even though Moral Foundations Theory has a different conceptual starting point (an evolutionary account of why humans have the moral intuitions they do, contra Schwartz's factor-analytic approach), it is nonetheless worthwhile to test whether the MFQ has predictive validity beyond Schwartz's scale in predicting a NIH-PA Author Manuscript NIH-PA Author Manuscript variety of scales, opinions, and self-reported behaviors relevant to morality. Because Schwartz's scale is larger both in terms of subscales (10 vs. 5) and items (58 vs. 30), this is a particularly tough test of the predictive validity of the MFQ. Data for these analyses came from 10,652 visitors to YourMorals.org who took both the MFQ and the Schwartz Values Scale (SVS), 92% of whom also took other scales or measures. We used two-step regressions to test whether the five MFQ subscales added incremental predictive validity beyond the ten SVS subscales for the external criteria (scales and attitudes toward foundation-related social groups) described above, as well as for positions on a wide range of political issues. Note that this analysis focuses on the incremental validity of the aggregate MFQ in comparison to the aggregate SVS, rather than investigating which of the particular moral concerns predict each criterion variable (for such work, see Koleva, Graham, Iyer, Ditto, &amp; Haidt, 2010). In every analysis, the MFQ made a significant improvement to prediction when added to the SVS (average R = 8%, all R significant at p &lt; .001). To provide a point of comparison, we repeated this analysis by adding the 44-item Big Five personality inventory to the SVS, which yielded an average R of only 2%. R and R for the scales and foundation-related social group averages can be found in Table 9. We also investigated R for the MFQ alone, to further compare its predictive validity with that of the SVS. As the bolded values in Table  9 show, the MFQ was actually a more powerful predictor than the SVS for most of the scales and political issue positions, and all of the social group attitudes. Given that the SVS is a comprehensive, large, and well-validated measure of values, the MFQ is clearing a high bar in providing unique predictive validity for outcomes relevant for moral and political psychology, and for the psychology of prejudice.Because every step of the scale development used large heterogeneous samples, we can be more confident about the MFQ's generalizability than if we had used college students only (Sears, 1986). However, the samples obtained at ProjectImplicit.org and at YourMorals.org are not representative of any national or international population -the current sample is disproportionately from the U.S. (80%), white (87%), male (63%), and educated (mean education between "completed college" and "some graduate school") compared to international or U.S. national averages. Thanks to the large sample sizes, however, we were able to test whether the five-factor model of moral concerns was consistent across national and geographic groups. All participants self-reported their current country of residence, the country in which they grew up (if different), and the age at which they moved (if they grew up in a different country). For participants who reported moving to their current country at age 14 or older, the country in which they reported growing up was used for the cross-cultural analysis. We created 12 location codes, four of which indicated the four nations from which the largest number of participants had come (U.S., Canada, U.K., Australia); the other eight location codes indicated multi-nation regions of the world (i.e. East Asia, Middle East). Model fit information for each location code is shown in Table 10. As the table shows, the five-factor model of the MFQ is a reasonable or good fit (all  a &lt; .06, all CFI &gt;.7) for all 11 world regions for which we were able to run the fit analyses, providing evidence that the measurement and theory of five foundational moral concerns is not specific to U.S. or Western participants. Notably, although the five-foundation model is a good fit in all these areas, the data shows much cross-cultural variation in the patterns of moral foundation endorsement. Even controlling for politics, age, gender, religious attendance, and education, world region is a significant (ps &lt; .001) predictor of all five foundation scores, indicating  (Rozin, 2006). For instance, the above validation exercises with external scales and social group attitudes showed that many traits and attitudes that don't seem to be about morality on the surface nevertheless show a systematic and theoretically meaningful relationship to moral concerns measured by the MFQ. We present here three additional findings made possible by MFT's broadened definition of morality, and its finer conceptual resolution of morality's components.Using a variety of measures and methods, Graham et al. (2009) showed that liberals value Harm and Fairness concerns more than conservatives, while conservatives value Ingroup, Authority, and Purity concerns more than liberals. The vast majority of these participants, however, came from the United States, leaving open the question of whether these patterns were limited to the particular ideological conflicts of the United States. Table 11 shows correlations between political ideology4 and the five foundations for the different world areas described in the Generalizability section. The correlations indicate that the liberal-conservative patterns found in the U.S. are robust across national and cultural contexts, both in terms of direction (negative correlations [liberals higher] for Harm and Fairness, positive correlations [conservatives higher] for Ingroup, Authority, and Purity) and in terms of magnitude: correlations are consistently strongest for Authority and Purity, and weakest for Harm. This suggests that across cultures, the most intractable political debates are likely to involve concerns related to respect for traditions/authorities and physical/spiritual purity, while the greatest degree of moral commonality may be found in issues related to harm and care. It also reinforces the claim that political ideology can be self-assessed and that the unidimensional left-right construct has some degree of common meaning across societies, despite differences in political party structures and particular national issues (Jost, 2006). 4 Because the terms "liberal" and "conservative" can mean different things in different nations (i.e., the Liberal Party in Australia is actually center-right ideologically), the political identification item on YourMorals.org clarifies to participants that the items from "very liberal" to "very conservative" should be read as "very left-wing" to "very right-wing." Graham et al. 06) compared to Western participants, and were only very slightly more concerned about Harm, Fairness, and Authority (mean differences &lt; .1, ts &lt; 7, ds &lt; .04). The fact that differences are concentrated in Ingroup and Purity makes some sense in light of established cultural differences in collectivism (Triandis, 1995) and the role of purity concerns in daily life and religious practice, particularly in South Asia (Shweder et al., 1997). But it is noteworthy that there are not large differences in Authority, given greater sensitivity to social hierarchy (Power Distance scores) in eastern nations (Hofstede, 2001). The small effect sizes for all the East-West differences suggest that variation within cultures (e.g., by gender or political ideology) will exceed the east-West variations given so much attention in crosscultural research. Here we see that the increased resolution afforded by MFT allows us to find moral differences we would not have been able to find otherwisedifferences that open up intriguing questions for further research. As the effect sizes show, these gender differences were much stronger than the differences between Eastern and Western cultures. The gender patterns make sense in light of previous research on empathy (Davis, 1983), egalitarianism (Arts &amp; Gelissen, 2001), and disgust sensitivity (Druschel &amp; Sherman, 1999), but they also show an important divergence from the political patterns in that Purity is here grouped with Harm and Fairness, rather than Ingroup and Authority. Here too the finer resolution and broadened scope of MFT allowed us to find and describe differences in moral personality not possible before.Moral Foundations Theory (Haidt &amp; Joseph, 2004;Haidt &amp; Graham, 2007) was created by selecting the closest links between evolutionary accounts of human sociality and anthropological accounts of the breadth and variability of the moral domain (see especially Fiske, 1992, andShweder et al., 1997). The findings reported in this article suggest that those anthropologists were right. From a purely descriptive perspective, the domain of morality consists of more than just "prescriptive judgments of justice, rights, and welfare" (Turiel, 1983, p. 3). Furthermore, we found that one does not need to travel to non-Western nations to find this broader conception of morality. In every country and world region we examined, people on the political right placed greater emphasis on concerns about ingroup loyalty, respect for authorities and traditions, and physical/spiritual purity than did people on the political left. The Moral Foundations Questionnaire fills the need for a theoretically-grounded scale covering the full range of human moral concerns. We found substantial evidence that the scale is reliable and valid. The scale is internally consistent (both within and between two question formats) while maintaining conceptual coverage of diverse manifestations of foundation-related concerns. Test-retest analyses showed stability of foundation subscale scores over time. External validations of the MFQ using widely-used scales, as well as attitudes toward conceptually related social groups, showed convergent, discriminant, and predictive validity. Factor analyses confirmed our theoretical parsing of the moral domain into five sets of concerns: the five-factor model fit the data better (weighing both fit and parsimony) than competing models, and this five-factor representation provided a good fit for participants in 11 different world areas. In addition to the scale itself, we expect that the method introduced in this paper (see Figure 1) for empirically selecting items to maximize both internal and external validity will also be of use to researchers. The best existing instrument for assessing moral concerns beyond harm and fairness is the Schwartz Values Scale (SVS; Schwartz, 1992), which includes group-level values such as "tradition" and "conformity." However, the SVS was created to measure a broad spectrum of values; it was not designed to cover the moral domain specifically, and it does not cover concerns about group-loyalty and spiritual purity. In a head-to-head comparison, the MFQ showed incremental predictive validity beyond the SVS for a diverse array of external scales related to moral personality, attitudes toward social groups, and opinions about moral and political issues (see Table 9). Further, in most cases, the MFQ performed even better than the SVS in overall variance explained of criterion variables, despite its shorter length and narrower conceptual coverage. This further illustrates the MFQ's effective measurement properties, balancing relatively short length and wide coverage of the moral domain.The research reported here indicates that the MFQ is a reliable and valid instrument for measuring a broad range of moral concerns. But in the process of developing and validating the MFQ, we also generated a number of substantive discoveries about moral psychology, such as:A map of the moral domain. Because it distinguishes five kinds of moral concerns, and gives separate evolutionary accounts to explain each of their origins (Haidt &amp; Joseph, 2007), MFT is not as parsimonious as theories of morality that try to derive the entire moral domain from one or two principles or processes (usually kin selection plus reciprocal altruism -see Dawkins, 1976;Hauser, 2006;Joyce, 2006). However, comparisons of different structural models revealed that the five-factor solution is an improvement over other theoretically-derived models, even taking into account the relative loss of parsimony. Analyses of international data showed that this five-factor model was a good fit in every area of the world we were able to examine. This provides empirical evidence for MFT's central claim about the structure of human morality, and points toward the usefulness of this added A guide to where the action is. We found some small and easily interpretable crosscultural differences in moral foundation scores: people in Eastern cultures were slightly more likely to value Ingroup and Purity than people in Western cultures. As with all research that relies upon educated participants, our cross-national differences would probably have been much larger if we had found a way to survey rural villagers and the urban poor in Asia. Nonetheless, the cross-national differences we did find were dwarfed by the within-nation (or within-region) differences we examined, including both ideological differences and sex differences (women valued Harm, Fairness, and Purity more than men, even controlling for political ideology). With reliable measures of these different kinds of moral concerns, social and personality psychologists can now begin to examine many such patterns of similarities and dissimilarities, as well as the processes behind them.A method for discovering moral prejudices. The finer resolution offered by the MFQ also revealed the potential role of moral judgment in prejudice. When we examined attitudes toward various social groups, we found that MFQ subscales indicated varying patterns of moral concerns that might lead some people to dislike some groups. This suggests that attitudes toward social groups may often be expressions of moral judgments about those groups -vague intuitions or explicit convictions that a particular social group upholds or violates one or more foundational concerns. The moral foundations can thus be used as a kind of decoder ring, allowing us to see multiple and sometimes unexpected moral threads connecting seemingly unrelated attitudes and opinions (cf. Koleva et al, 2010). This possibility emphasizes our functional definition of morality as a description of what motivates people to suppress selfishness, rather than a prescriptive definition of how one ought to behave. By describing and quantifying a broadened range of human moral concerns, MFT and the MFQ can aid in our understanding of the dangers of morality, as well as the benefits.The map of the moral domain that we offer is provisional. We hope that other researchers will help us improve it. Here are four next steps:
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nihms246870.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    8160.47025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nihms246870.mp3
   </guid>
<itunes:episode>
    129
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Precise Zero-Shot Dense Retrieval without Relevance Labels
   </title>
<itunes:title>
    Precise Zero-Shot Dense Retrieval without Relevance Labels
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    While dense retrieval has been shown effec-
   </itunes:summary>
<description>
    While dense retrieval has been shown effec-
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.10496v1.Precise_Zero_Shot_Dense_Retrieval_without_Relevance_Labels.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1375.38
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.10496v1.Precise_Zero_Shot_Dense_Retrieval_without_Relevance_Labels.mp3
   </guid>
<itunes:episode>
    130
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics
   </title>
<itunes:title>
    The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Abstract Notable progress has been made recently on computational models of semantics using vector representations for word meaning Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model's semantic space. Consistent with human performance (Osgood, Suci, &amp; Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.
   </itunes:summary>
<description>
    Abstract Notable progress has been made recently on computational models of semantics using vector representations for word meaning Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model's semantic space. Consistent with human performance (Osgood, Suci, &amp; Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13423-016-1053-2.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3976.307
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13423-016-1053-2.mp3
   </guid>
<itunes:episode>
    131
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Rationalization for Explainable NLP: A Survey
   </title>
<itunes:title>
    Rationalization for Explainable NLP: A Survey
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007-2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.
   </itunes:summary>
<description>
    Recent advances in deep learning have improved the performance of many Natural Language Processing (NLP) tasks such as translation, question-answering, and text classification. However, this improvement comes at the expense of model explainability. Black-box models make it difficult to understand the internals of a system and the process it takes to arrive at an output. Numerical (LIME, Shapley) and visualization (saliency heatmap) explainability techniques are helpful; however, they are insufficient because they require specialized knowledge. These factors led rationalization to emerge as a more accessible explainable technique in NLP. Rationalization justifies a model's output by providing a natural language explanation (rationale). Recent improvements in natural language generation have made rationalization an attractive technique because it is intuitive, human-comprehensible, and accessible to non-technical users. Since rationalization is a relatively new field, it is disorganized. As the first survey, rationalization literature in NLP from 2007-2022 is analyzed. This survey presents available methods, explainable evaluations, code, and datasets used across various NLP tasks that use rationalization. Further, a new subfield in Explainable AI (XAI), namely, Rational AI (RAI), is introduced to advance the current state of rationalization. A discussion on observed insights, challenges, and future directions is provided to point to promising research opportunities.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2301.08912v1.Rationalization_for_Explainable_NLP_A_Survey.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4176.222
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2301.08912v1.Rationalization_for_Explainable_NLP_A_Survey.mp3
   </guid>
<itunes:episode>
    132
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation
   </title>
<itunes:title>
    TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called "Turing Test" problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https://turingbench.ist.psu.edu/
   </itunes:summary>
<description>
    Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called "Turing Test" problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https://turingbench.ist.psu.edu/
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.13296v1.TURINGBENCH_A_Benchmark_Environment_for_Turing_Test_in_the_Age_of_Neural_Text_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3000.13725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.13296v1.TURINGBENCH_A_Benchmark_Environment_for_Turing_Test_in_the_Age_of_Neural_Text_Generation.mp3
   </guid>
<itunes:episode>
    133
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E
   </title>
<itunes:title>
    Acceleration AI Ethics, the Debate between Innovation and Safety, and Stability AI's Diffusion versus OpenAI's Dall-E
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    One objection to conventional AI ethics is that it slows innovation. This presentation responds by reconfiguring ethics as an innovation accelerator. The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E. By analyzing the divergent values underlying their opposed strategies for development and deployment, five conceptions are identified as common to acceleration ethics. Uncertainty is understood as positive and encouraging, rather than discouraging. Innovation is conceived as intrinsically valuable, instead of worthwhile only as mediated by social effects. AI problems are solved by more AI, not less. Permissions and restrictions governing AI emerge from a decentralized process, instead of a unified authority. The work of ethics is embedded in AI development and application, instead of functioning from outside. Together, these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence.
   </itunes:summary>
<description>
    One objection to conventional AI ethics is that it slows innovation. This presentation responds by reconfiguring ethics as an innovation accelerator. The critical elements develop from a contrast between Stability AI's Diffusion and OpenAI's Dall-E. By analyzing the divergent values underlying their opposed strategies for development and deployment, five conceptions are identified as common to acceleration ethics. Uncertainty is understood as positive and encouraging, rather than discouraging. Innovation is conceived as intrinsically valuable, instead of worthwhile only as mediated by social effects. AI problems are solved by more AI, not less. Permissions and restrictions governing AI emerge from a decentralized process, instead of a unified authority. The work of ethics is embedded in AI development and application, instead of functioning from outside. Together, these attitudes and practices remake ethics as provoking rather than restraining artificial intelligence.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.01834v1.Acceleration_AI_Ethics_the_Debate_between_Innovation_and_Safety_and_Stability_AI_s_Diffusion_versus_OpenAI_s_Dall_E.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1349.538
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2212.01834v1.Acceleration_AI_Ethics_the_Debate_between_Innovation_and_Safety_and_Stability_AI_s_Diffusion_versus_OpenAI_s_Dall_E.mp3
   </guid>
<itunes:episode>
    134
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Case Study: Deontological Ethics in NLP
   </title>
<itunes:title>
    Case Study: Deontological Ethics in NLP
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.
   </itunes:summary>
<description>
    Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.04658v2.Case_Study_Deontological_Ethics_in_NLP.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2717.1005
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.04658v2.Case_Study_Deontological_Ethics_in_NLP.mp3
   </guid>
<itunes:episode>
    135
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation
   </title>
<itunes:title>
    WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.
   </itunes:summary>
<description>
    A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.05955v1.WANLI_Worker_and_AI_Collaboration_for_Natural_Language_Inference_Dataset_Creation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3234.769
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.05955v1.WANLI_Worker_and_AI_Collaboration_for_Natural_Language_Inference_Dataset_Creation.mp3
   </guid>
<itunes:episode>
    136
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Corpus for Understanding and Generating Moral Stories
   </title>
<itunes:title>
    A Corpus for Understanding and Generating Moral Stories
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.
   </itunes:summary>
<description>
    Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.09438v1.A_Corpus_for_Understanding_and_Generating_Moral_Stories.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3328.18275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.09438v1.A_Corpus_for_Understanding_and_Generating_Moral_Stories.mp3
   </guid>
<itunes:episode>
    137
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    RankGen: Improving Text Generation with Large Ranking Models
   </title>
<itunes:title>
    RankGen: Improving Text Generation with Large Ranking Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.
   </itunes:summary>
<description>
    Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09726v1.RankGen_Improving_Text_Generation_with_Large_Ranking_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4381.91025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09726v1.RankGen_Improving_Text_Generation_with_Large_Ranking_Models.mp3
   </guid>
<itunes:episode>
    138
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition
   </title>
<itunes:title>
    Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than token-level representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.
   </itunes:summary>
<description>
    Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than token-level representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12732v1.Propose_and_Refine_A_Two_Stage_Set_Prediction_Network_for_Nested_Named_Entity_Recognition.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1766.24325
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12732v1.Propose_and_Refine_A_Two_Stage_Set_Prediction_Network_for_Nested_Named_Entity_Recognition.mp3
   </guid>
<itunes:episode>
    139
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Continuous diffusion for categorical data
   </title>
<itunes:title>
    Continuous diffusion for categorical data
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.
   </itunes:summary>
<description>
    Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.15089v1.Continuous_diffusion_for_categorical_data.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5302.15175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.15089v1.Continuous_diffusion_for_categorical_data.mp3
   </guid>
<itunes:episode>
    140
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language
   </title>
<itunes:title>
    ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.
   </itunes:summary>
<description>
    Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.13048v2.ProofWriter_Generating_Implications_Proofs_and_Abductive_Statements_over_Natural_Language.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3255.84975
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.13048v2.ProofWriter_Generating_Implications_Proofs_and_Abductive_Statements_over_Natural_Language.mp3
   </guid>
<itunes:episode>
    141
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Quantifying Blockchain Extractable Value: How dark is the forest?
   </title>
<itunes:title>
    Quantifying Blockchain Extractable Value: How dark is the forest?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.   However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.   Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.   Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.
   </itunes:summary>
<description>
    Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.   However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.   Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.   Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05511v5.Quantifying_Blockchain_Extractable_Value_How_dark_is_the_forest.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2101.05511v5.Quantifying_Blockchain_Extractable_Value_How_dark_is_the_forest.mp3.vtt"/>
<itunes:duration>
    5024.484
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05511v5.Quantifying_Blockchain_Extractable_Value_How_dark_is_the_forest.mp3
   </guid>
<itunes:episode>
    142
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework
   </title>
<itunes:title>
    LMTurk: Few-Shot Learners as Crowdsourcing Workers in a Language-Model-as-a-Service Framework
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shot learners as crowdsourcing workers. The rationale is that crowdsourcing workers are in fact few-shot learners: They are shown a few illustrative examples to learn about a task and then start annotating. LMTurk employs few-shot learners built upon PLMs as workers. We show that the resulting annotations can be utilized to train models that solve the task well and are small enough to be deployable in practical scenarios. Active learning is integrated into LMTurk to reduce the amount of queries made to PLMs, minimizing the computational cost of running PLM inference passes. Altogether, LMTurk is an important step towards making effective use of current PLMs.
   </itunes:summary>
<description>
    Vast efforts have been devoted to creating high-performance few-shot learners, i.e., large-scale pretrained language models (PLMs) that perform well with little downstream task training data. Training PLMs has incurred significant cost, but utilizing the few-shot learners is still challenging due to their enormous size. This work focuses on a crucial question: How to make effective use of these few-shot learners? We propose LMTurk, a novel approach that treats few-shot learners as crowdsourcing workers. The rationale is that crowdsourcing workers are in fact few-shot learners: They are shown a few illustrative examples to learn about a task and then start annotating. LMTurk employs few-shot learners built upon PLMs as workers. We show that the resulting annotations can be utilized to train models that solve the task well and are small enough to be deployable in practical scenarios. Active learning is integrated into LMTurk to reduce the amount of queries made to PLMs, minimizing the computational cost of running PLM inference passes. Altogether, LMTurk is an important step towards making effective use of current PLMs.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07522v2.LMTurk_Few_Shot_Learners_as_Crowdsourcing_Workers_in_a_Language_Model_as_a_Service_Framework.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2570.73625
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07522v2.LMTurk_Few_Shot_Learners_as_Crowdsourcing_Workers_in_a_Language_Model_as_a_Service_Framework.mp3
   </guid>
<itunes:episode>
    143
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models
   </title>
<itunes:title>
    Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish "genuine" tokens and "spurious" tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of "shortcuts", and mitigating these leads to more robust models in multiple applications.
   </itunes:summary>
<description>
    Recently, NLP models have achieved remarkable progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations, or shortcuts between the training data and the task labels. Most existing work identifies a limited set of task-specific shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We first leverage existing interpretability methods to extract tokens that significantly affect model's decision process from the input text. We then distinguish "genuine" tokens and "spurious" tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efficiently identify a scalable set of "shortcuts", and mitigating these leads to more robust models in multiple applications.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07736v2.Identifying_and_Mitigating_Spurious_Correlations_for_Improving_Robustness_in_NLP_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2040.8425
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07736v2.Identifying_and_Mitigating_Spurious_Correlations_for_Improving_Robustness_in_NLP_Models.mp3
   </guid>
<itunes:episode>
    144
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Do Multilingual Language Models Capture Differing Moral Norms?
   </title>
<itunes:title>
    Do Multilingual Language Models Capture Differing Moral Norms?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages.
   </itunes:summary>
<description>
    Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09904v1.Do_Multilingual_Language_Models_Capture_Differing_Moral_Norms.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    377.23425
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09904v1.Do_Multilingual_Language_Models_Capture_Differing_Moral_Norms.mp3
   </guid>
<itunes:episode>
    145
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Knowledge Graph Reasoning with Logics and Embeddings: Survey and Perspective
   </title>
<itunes:title>
    Knowledge Graph Reasoning with Logics and Embeddings: Survey and Perspective
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions.
   </itunes:summary>
<description>
    Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.07412v1.Knowledge_Graph_Reasoning_with_Logics_and_Embeddings_Survey_and_Perspective.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2421.786
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.07412v1.Knowledge_Graph_Reasoning_with_Logics_and_Embeddings_Survey_and_Perspective.mp3
   </guid>
<itunes:episode>
    146
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer
   </title>
<itunes:title>
    Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content -- the two core aspects of the task -- we achieve a new state-of-the-art.
   </itunes:summary>
<description>
    Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content -- the two core aspects of the task -- we achieve a new state-of-the-art.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06947v2.Thank_you_BART_Rewarding_Pre_Trained_Models_Improves_Formality_Style_Transfer.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2105.06947v2.Thank_you_BART_Rewarding_Pre_Trained_Models_Improves_Formality_Style_Transfer.mp3.vtt"/>
<itunes:duration>
    1114.704
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06947v2.Thank_you_BART_Rewarding_Pre_Trained_Models_Improves_Formality_Style_Transfer.mp3
   </guid>
<itunes:episode>
    147
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Zero-Shot Information Extraction as a Unified Text-to-Triple Translation
   </title>
<itunes:title>
    Zero-Shot Information Extraction as a Unified Text-to-Triple Translation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.
   </itunes:summary>
<description>
    We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2476.61725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3
   </guid>
<itunes:episode>
    148
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments
   </title>
<itunes:title>
    The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    An audience's prior beliefs and morals are strong indicators of how likely they will be affected by a given argument. Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement. In argumentation technology, however, this is barely exploited so far. This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences. Following the moral foundation theory, we propose a system that effectively generates arguments focusing on different morals. In an in-depth user study, we ask liberals and conservatives to evaluate the impact of these arguments. Our results suggest that, particularly when prior beliefs are challenged, an audience becomes more affected by morally framed arguments.
   </itunes:summary>
<description>
    An audience's prior beliefs and morals are strong indicators of how likely they will be affected by a given argument. Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement. In argumentation technology, however, this is barely exploited so far. This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences. Following the moral foundation theory, we propose a system that effectively generates arguments focusing on different morals. In an in-depth user study, we ask liberals and conservatives to evaluate the impact of these arguments. Our results suggest that, particularly when prior beliefs are challenged, an audience becomes more affected by morally framed arguments.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14563v1.The_Moral_Debater_A_Study_on_the_Computational_Generation_of_Morally_Framed_Arguments.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3507.408
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14563v1.The_Moral_Debater_A_Study_on_the_Computational_Generation_of_Morally_Framed_Arguments.mp3
   </guid>
<itunes:episode>
    149
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Aspect-Controlled Neural Argument Generation
   </title>
<itunes:title>
    Aspect-Controlled Neural Argument Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.
   </itunes:summary>
<description>
    We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL-a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspectspecific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.naacl-main.34.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3425.56725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.naacl-main.34.mp3
   </guid>
<itunes:episode>
    150
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Designing for Engaging with News using Moral Framing towards Bridging Ideological Divides
   </title>
<itunes:title>
    Designing for Engaging with News using Moral Framing towards Bridging Ideological Divides
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Society is showing signs of strong ideological polarization. When pushed to seek perspectives different from their own, people often reject diverse ideas or find them unfathomable. Work has shown that framing controversial issues using the values of the audience can improve understanding of opposing views. In this paper, we present our work designing systems for addressing ideological division through educating U.S. news consumers to engage using a framework of fundamental human values known as Moral Foundations. We design and implement a series of new features that encourage users to challenge their understanding of opposing views, including annotation of moral frames in news articles, discussion of those frames via inline comments, and recommendations based on relevant moral frames. We describe two versions of features---the first covering a suite of ways to interact with moral framing in news, and the second tailored towards collaborative annotation and discussion. We conduct a field evaluation of each design iteration with 71 participants in total over a period of 6-8 days, finding evidence suggesting users learned to re-frame their discourse in moral values of the opposing side. Our work provides several design considerations for building systems to engage with moral framing.
   </itunes:summary>
<description>
    Society is showing signs of strong ideological polarization. When pushed to seek perspectives different from their own, people often reject diverse ideas or find them unfathomable. Work has shown that framing controversial issues using the values of the audience can improve understanding of opposing views. In this paper, we present our work designing systems for addressing ideological division through educating U.S. news consumers to engage using a framework of fundamental human values known as Moral Foundations. We design and implement a series of new features that encourage users to challenge their understanding of opposing views, including annotation of moral frames in news articles, discussion of those frames via inline comments, and recommendations based on relevant moral frames. We describe two versions of features---the first covering a suite of ways to interact with moral framing in news, and the second tailored towards collaborative annotation and discussion. We conduct a field evaluation of each design iteration with 71 participants in total over a period of 6-8 days, finding evidence suggesting users learned to re-frame their discourse in moral values of the opposing side. Our work provides several design considerations for building systems to engage with moral framing.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/3492861.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4086.622
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/3492861.mp3
   </guid>
<itunes:episode>
    151
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The disunity of moral judgment: Evidence and implications
   </title>
<itunes:title>
    The disunity of moral judgment: Evidence and implications
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/B7625348-B105-11EC-BDE1-B443BD928927.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3304.4115
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/B7625348-B105-11EC-BDE1-B443BD928927.mp3
   </guid>
<itunes:episode>
    152
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Why GANs are overkill for NLP
   </title>
<itunes:title>
    Why GANs are overkill for NLP
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    This work offers a novel theoretical perspective on why, despite numerous attempts, adversarial approaches to generative modeling (e.g., GANs) have not been as popular for certain generation tasks, particularly sequential tasks such as Natural Language Generation, as they have in others, such as Computer Vision. In particular, on sequential data such as text, maximum-likelihood approaches are significantly more utilized than GANs. We show that, while it may seem that maximizing likelihood is inherently different than minimizing distinguishability, this distinction is largely artificial and only holds for limited models. We argue that minimizing KL-divergence (i.e., maximizing likelihood) is a more efficient approach to effectively minimizing the same distinguishability criteria that adversarial models seek to optimize. Reductions show that minimizing distinguishability can be seen as simply boosting likelihood for certain families of models including n-gram models and neural networks with a softmax output layer. To achieve a full polynomial-time reduction, a novel next-token distinguishability model is considered.
   </itunes:summary>
<description>
    This work offers a novel theoretical perspective on why, despite numerous attempts, adversarial approaches to generative modeling (e.g., GANs) have not been as popular for certain generation tasks, particularly sequential tasks such as Natural Language Generation, as they have in others, such as Computer Vision. In particular, on sequential data such as text, maximum-likelihood approaches are significantly more utilized than GANs. We show that, while it may seem that maximizing likelihood is inherently different than minimizing distinguishability, this distinction is largely artificial and only holds for limited models. We argue that minimizing KL-divergence (i.e., maximizing likelihood) is a more efficient approach to effectively minimizing the same distinguishability criteria that adversarial models seek to optimize. Reductions show that minimizing distinguishability can be seen as simply boosting likelihood for certain families of models including n-gram models and neural networks with a softmax output layer. To achieve a full polynomial-time reduction, a novel next-token distinguishability model is considered.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09838v1.Why_GANs_are_overkill_for_NLP.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2475.04975
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09838v1.Why_GANs_are_overkill_for_NLP.mp3
   </guid>
<itunes:episode>
    153
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Pinto Fires and Personal Ethics: A Script Analysis of Missed Opportunities
   </title>
<itunes:title>
    Pinto Fires and Personal Ethics: A Script Analysis of Missed Opportunities
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    ABSTRACT. This article details the personal involvement of the author in the early stages of the infamous Pinto fire case. The paper first presents an insider account of the context and decision environment within which he failed to initiate an early recall of defective vehicles. A cognitive script analysis of the personal experience is then offered as an explanation of factors that led to a decision that now is commonly seen as a definitive study in uuethicaI corporate behavior. The main analytical thesis is that script schemas that were guiding cognition and action at the time precluded consideration of issues in ethical terms because the scripts did not include ethical dimensions.
   </itunes:summary>
<description>
    ABSTRACT. This article details the personal involvement of the author in the early stages of the infamous Pinto fire case. The paper first presents an insider account of the context and decision environment within which he failed to initiate an early recall of defective vehicles. A cognitive script analysis of the personal experience is then offered as an explanation of factors that led to a decision that now is commonly seen as a definitive study in uuethicaI corporate behavior. The main analytical thesis is that script schemas that were guiding cognition and action at the time precluded consideration of issues in ethical terms because the scripts did not include ethical dimensions.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/BF00870550.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/BF00870550.mp3.vtt"/>
<itunes:duration>
    2612.592
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/BF00870550.mp3
   </guid>
<itunes:episode>
    154
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Aligning AI With Shared Human Values
   </title>
<itunes:title>
    Aligning AI With Shared Human Values
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.
   </itunes:summary>
<description>
    We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.02275v5.Aligning_AI_With_Shared_Human_Values.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4847.6995
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.02275v5.Aligning_AI_With_Shared_Human_Values.mp3
   </guid>
<itunes:episode>
    155
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Harm inflation: Making sense of concept creep
   </title>
<itunes:title>
    Harm inflation: Making sense of concept creep
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/10.1080@10463283.2020.1796080.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5220.83275
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/10.1080@10463283.2020.1796080.mp3
   </guid>
<itunes:episode>
    156
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Exploring the Limits of Natural Language Inference Based Setup for Few-Shot Intent Detection
   </title>
<itunes:title>
    Exploring the Limits of Natural Language Inference Based Setup for Few-Shot Intent Detection
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    One of the core components of goal-oriented dialog systems is the task of Intent Detection. Few-shot Learning upon Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works making use of metric-based and optimization-based methods have been proposed, the task is still challenging in large label spaces and much smaller number of shots. Generalized Few-shot learning is more difficult due to the presence of both novel and seen classes during the testing phase. In this work, we propose a simple and effective method based on Natural Language Inference that not only tackles the problem of few shot intent detection, but also proves useful in zero-shot and generalized few shot learning problems. Our extensive experiments on a number of Natural Language Understanding (NLU) and Spoken Language Understanding (SLU) datasets show the effectiveness of our approach. In addition, we highlight the settings in which our NLI based method outperforms the baselines by huge margins.
   </itunes:summary>
<description>
    One of the core components of goal-oriented dialog systems is the task of Intent Detection. Few-shot Learning upon Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works making use of metric-based and optimization-based methods have been proposed, the task is still challenging in large label spaces and much smaller number of shots. Generalized Few-shot learning is more difficult due to the presence of both novel and seen classes during the testing phase. In this work, we propose a simple and effective method based on Natural Language Inference that not only tackles the problem of few shot intent detection, but also proves useful in zero-shot and generalized few shot learning problems. Our extensive experiments on a number of Natural Language Understanding (NLU) and Spoken Language Understanding (SLU) datasets show the effectiveness of our approach. In addition, we highlight the settings in which our NLI based method outperforms the baselines by huge margins.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07434v1.Exploring_the_Limits_of_Natural_Language_Inference_Based_Setup_for_Few_Shot_Intent_Detection.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1713.65875
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07434v1.Exploring_the_Limits_of_Natural_Language_Inference_Based_Setup_for_Few_Shot_Intent_Detection.mp3
   </guid>
<itunes:episode>
    157
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Exposure to opposing views on social media can increase political polarization
   </title>
<itunes:title>
    Exposure to opposing views on social media can increase political polarization
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    There is mounting concern that social media sites contribute to political polarization by creating "echo chambers" that insulate people from opposing views about current events. We surveyed a large sample of Democrats and Republicans who visit Twitter at least three times each week about a range of social policy issues. One week later, we randomly assigned respondents to a treatment condition in which they were offered financial incentives to follow a Twitter bot for 1 month that exposed them to messages from those with opposing political ideologies (e.g., elected officials, opinion leaders, media organizations, and nonprofit groups). Respondents were resurveyed at the end of the month to measure the effect of this treatment, and at regular intervals throughout the study period to monitor treatment compliance. We find that Republicans who followed a liberal Twitter bot became substantially more conservative posttreatment. Democrats exhibited slight increases in liberal attitudes after following a conservative Twitter bot, although these effects are not statistically significant. Notwithstanding important limitations of our study, these findings have significant implications for the interdisciplinary literature on political polarization and the emerging field of computational social science. political polarization | computational social science | social networks | social media | sociology P olitical polarization in the United States has become a central focus of social scientists in recent decades (1-7). Americans are deeply divided on controversial issues such as inequality, gun control, and immigration-and divisions about such issues have become increasingly aligned with partisan identities in recent years (8, 9). Partisan identification now predicts preferences about a range of social policy issues nearly three times as well as any other demographic factor-such as education or age (10). These partisan divisions not only impede compromise in the design and implementation of social policies but also have farreaching consequences for the effective function of democracy more broadly (11-15). America's cavernous partisan divides are often attributed to "echo chambers," or patterns of information sharing that reinforce preexisting political beliefs by limiting exposure to opposing political views (16)(17)(18)(19)(20). Concern about selective exposure to information and political polarization has increased in the age of social media (16,(21)(22)(23). The vast majority of Americans now visit a social media site at least once each day, and a rapidly growing number of them list social media as their primary source of news (24). Despite initial optimism that social media might enable people to consume more heterogeneous sources of information about current events, there is growing concern that such forums exacerbate political polarization because of social network homophily, or the well-documented tendency of people to form social network ties to those who are similar to themselves (25, 26). The endogenous relationship between social network formation and political attitudes also creates formidable challenges for the study of social media echo chambers and political polarization, since it is notoriously difficult to establish whether social media networks shape political opinions, or vice versa (27)(28)(29). Here, we report the results of a large field experiment designed to examine whether disrupting selective exposure to partisan information among Twitter users shapes their political attitudes. Our research is governed by three preregistered hypotheses. The first hypothesis is that disrupting selective exposure to partisan information will decrease political polarization because of intergroup contact effects. A vast literature indicates contact between opposing groups can challenge stereotypes that develop in the absence of positive interactions between them (30). Studies also indicate intergroup contact increases the likelihood of deliberation and political compromise (31-33). However, all of these previous studies examine interpersonal contact between members of rival groups. In contrast, our experiment creates virtual contact between members of the public and opinion leaders from the opposing political party on a social media site. It is not yet known whether such virtual contact creates the Significance Social media sites are often blamed for exacerbating political polarization by creating "echo chambers" that prevent people from being exposed to information that contradicts their preexisting beliefs. We conducted a field experiment that offered a large group of Democrats and Republicans financial compensation to follow bots that retweeted messages by elected officials and opinion leaders with opposing political views. Republican participants expressed substantially more conservative views after following a liberal Twitter bot, whereas Democrats' attitudes became slightly more liberal after following a conservative Twitter bot-although this effect was not statistically significant. Despite several limitations, this study has important implications for the emerging field of computational social science and ongoing efforts to reduce political polarization online.
   </itunes:summary>
<description>
    There is mounting concern that social media sites contribute to political polarization by creating "echo chambers" that insulate people from opposing views about current events. We surveyed a large sample of Democrats and Republicans who visit Twitter at least three times each week about a range of social policy issues. One week later, we randomly assigned respondents to a treatment condition in which they were offered financial incentives to follow a Twitter bot for 1 month that exposed them to messages from those with opposing political ideologies (e.g., elected officials, opinion leaders, media organizations, and nonprofit groups). Respondents were resurveyed at the end of the month to measure the effect of this treatment, and at regular intervals throughout the study period to monitor treatment compliance. We find that Republicans who followed a liberal Twitter bot became substantially more conservative posttreatment. Democrats exhibited slight increases in liberal attitudes after following a conservative Twitter bot, although these effects are not statistically significant. Notwithstanding important limitations of our study, these findings have significant implications for the interdisciplinary literature on political polarization and the emerging field of computational social science. political polarization | computational social science | social networks | social media | sociology P olitical polarization in the United States has become a central focus of social scientists in recent decades (1-7). Americans are deeply divided on controversial issues such as inequality, gun control, and immigration-and divisions about such issues have become increasingly aligned with partisan identities in recent years (8, 9). Partisan identification now predicts preferences about a range of social policy issues nearly three times as well as any other demographic factor-such as education or age (10). These partisan divisions not only impede compromise in the design and implementation of social policies but also have farreaching consequences for the effective function of democracy more broadly (11-15). America's cavernous partisan divides are often attributed to "echo chambers," or patterns of information sharing that reinforce preexisting political beliefs by limiting exposure to opposing political views (16)(17)(18)(19)(20). Concern about selective exposure to information and political polarization has increased in the age of social media (16,(21)(22)(23). The vast majority of Americans now visit a social media site at least once each day, and a rapidly growing number of them list social media as their primary source of news (24). Despite initial optimism that social media might enable people to consume more heterogeneous sources of information about current events, there is growing concern that such forums exacerbate political polarization because of social network homophily, or the well-documented tendency of people to form social network ties to those who are similar to themselves (25, 26). The endogenous relationship between social network formation and political attitudes also creates formidable challenges for the study of social media echo chambers and political polarization, since it is notoriously difficult to establish whether social media networks shape political opinions, or vice versa (27)(28)(29). Here, we report the results of a large field experiment designed to examine whether disrupting selective exposure to partisan information among Twitter users shapes their political attitudes. Our research is governed by three preregistered hypotheses. The first hypothesis is that disrupting selective exposure to partisan information will decrease political polarization because of intergroup contact effects. A vast literature indicates contact between opposing groups can challenge stereotypes that develop in the absence of positive interactions between them (30). Studies also indicate intergroup contact increases the likelihood of deliberation and political compromise (31-33). However, all of these previous studies examine interpersonal contact between members of rival groups. In contrast, our experiment creates virtual contact between members of the public and opinion leaders from the opposing political party on a social media site. It is not yet known whether such virtual contact creates the Significance Social media sites are often blamed for exacerbating political polarization by creating "echo chambers" that prevent people from being exposed to information that contradicts their preexisting beliefs. We conducted a field experiment that offered a large group of Democrats and Republicans financial compensation to follow bots that retweeted messages by elected officials and opinion leaders with opposing political views. Republican participants expressed substantially more conservative views after following a liberal Twitter bot, whereas Democrats' attitudes became slightly more liberal after following a conservative Twitter bot-although this effect was not statistically significant. Despite several limitations, this study has important implications for the emerging field of computational social science and ongoing efforts to reduce political polarization online.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pnas.1804840115.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1896.90775
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pnas.1804840115.mp3
   </guid>
<itunes:episode>
    158
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Autoregressive Search Engines: Generating Substrings as Document Identifiers
   </title>
<itunes:title>
    Autoregressive Search Engines: Generating Substrings as Document Identifiers
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.
   </itunes:summary>
<description>
    Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.10628v1.Autoregressive_Search_Engines_Generating_Substrings_as_Document_Identifiers.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4549.956
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.10628v1.Autoregressive_Search_Engines_Generating_Substrings_as_Document_Identifiers.mp3
   </guid>
<itunes:episode>
    159
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts Maarten Sap
   </title>
<itunes:title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts Maarten Sap
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformerbased classifiers on TOXICHAT that achieve 0.71 F 1 for offensive labels and 0.53 Macro-F 1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.
   </itunes:summary>
<description>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create TOXICHAT, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformerbased classifiers on TOXICHAT that achieve 0.71 F 1 for offensive labels and 0.53 Macro-F 1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.397 (1).mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2021.emnlp-main.397 (1).mp3.vtt"/>
<itunes:duration>
    2493.756
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2021.emnlp-main.397 (1).mp3
   </guid>
<itunes:episode>
    160
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Neural Machine Translation with Gumbel-Greedy Decoding
   </title>
<itunes:title>
    Neural Machine Translation with Gumbel-Greedy Decoding
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Previous neural machine translation models used some heuristic search algorithms (e.g., beam search) in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.
   </itunes:summary>
<description>
    Previous neural machine translation models used some heuristic search algorithms (e.g., beam search) in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.07518v1.Neural_Machine_Translation_with_Gumbel_Greedy_Decoding.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2177.33225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.07518v1.Neural_Machine_Translation_with_Gumbel_Greedy_Decoding.mp3
   </guid>
<itunes:episode>
    161
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems
   </title>
<itunes:title>
    The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic
   </itunes:summary>
<description>
    Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.03021v1.The_Moral_Integrity_Corpus_A_Benchmark_for_Ethical_Dialogue_Systems.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3364.284
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.03021v1.The_Moral_Integrity_Corpus_A_Benchmark_for_Ethical_Dialogue_Systems.mp3
   </guid>
<itunes:episode>
    162
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    On the dominant set selection problem and its application to value alignment
   </title>
<itunes:title>
    On the dominant set selection problem and its application to value alignment
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Decision makers can often be confronted with the need to select a subset of objects from a set of candidate objects by just counting on preferences regarding the objects' features. Here we formalise this problem as the dominant set selection problem. Solving this problem amounts to finding the preferences over all possible sets of objects. We accomplish so by: (i) grounding the preferences over features to preferences over the objects themselves; and (ii) lifting these preferences to preferences over all possible sets of objects. This is achieved by combining lex-cel -a method from the literature-with our novel anti-lex-cel method, which we formally (and thoroughly) study. Furthermore, we provide a binary integer program encoding to solve the problem. Finally, we illustrate our overall approach by applying it to the selection of value-aligned norm systems.
   </itunes:summary>
<description>
    Decision makers can often be confronted with the need to select a subset of objects from a set of candidate objects by just counting on preferences regarding the objects' features. Here we formalise this problem as the dominant set selection problem. Solving this problem amounts to finding the preferences over all possible sets of objects. We accomplish so by: (i) grounding the preferences over features to preferences over the objects themselves; and (ii) lifting these preferences to preferences over all possible sets of objects. This is achieved by combining lex-cel -a method from the literature-with our novel anti-lex-cel method, which we formally (and thoroughly) study. Furthermore, we provide a binary integer program encoding to solve the problem. Finally, we illustrate our overall approach by applying it to the selection of value-aligned norm systems.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10458-021-09519-5.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    7405.244
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10458-021-09519-5.mp3
   </guid>
<itunes:episode>
    163
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Identifying Automatically Generated Headlines using Transformers
   </title>
<itunes:title>
    Identifying Automatically Generated Headlines using Transformers
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8% of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7%, indicating that content generated from language models can be filtered out accurately.
   </itunes:summary>
<description>
    False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8% of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7%, indicating that content generated from language models can be filtered out accurately.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.13375v3.Identifying_Automatically_Generated_Headlines_using_Transformers.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1225.63925
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.13375v3.Identifying_Automatically_Generated_Headlines_using_Transformers.mp3
   </guid>
<itunes:episode>
    164
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs
   </title>
<itunes:title>
    Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.   To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur.   We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75%) across several tasks, models, and training methods.
   </itunes:summary>
<description>
    Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.   To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur.   We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75%) across several tasks, models, and training methods.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2303.08114v1.Simfluence_Modeling_the_Influence_of_Individual_Training_Examples_by_Simulating_Training_Runs.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3925.107
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2303.08114v1.Simfluence_Modeling_the_Influence_of_Individual_Training_Examples_by_Simulating_Training_Runs.mp3
   </guid>
<itunes:episode>
    165
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Retrieving Comparative Arguments using Deep Language Models Notebook for the Touch Lab on Argument Retrieval at CLEF 2022
   </title>
<itunes:title>
    Retrieving Comparative Arguments using Deep Language Models Notebook for the Touch Lab on Argument Retrieval at CLEF 2022
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In this paper, we present a submission to the Touch lab's Task 2 on Argument Retrieval for Comparative Questions. Our team Katana employs approaches based on pre-trained deep language model architecture ColBERT [1]. This BERT-based architecture is adapted to the text ranking task by learning to represent both queries and documents as vectors and measuring the similarity between them. We use a model trained on a question-answering dataset MSMARCO, with the proposed weights and weights pre-trained by us. We also customize ColBERT for the comparative retrieval domain by fine-tuning the model on the data from the previous years' Touch competitions. The proposed experiments verify the usefulness of the transfer learning from a large pre-trained ranking language models to the problem of arguments extraction for comparative topics. Ours solutions rank third in both relevance, quality, and stance prediction evaluations.
   </itunes:summary>
<description>
    In this paper, we present a submission to the Touch lab's Task 2 on Argument Retrieval for Comparative Questions. Our team Katana employs approaches based on pre-trained deep language model architecture ColBERT [1]. This BERT-based architecture is adapted to the text ranking task by learning to represent both queries and documents as vectors and measuring the similarity between them. We use a model trained on a question-answering dataset MSMARCO, with the proposed weights and weights pre-trained by us. We also customize ColBERT for the comparative retrieval domain by fine-tuning the model on the data from the previous years' Touch competitions. The proposed experiments verify the usefulness of the transfer learning from a large pre-trained ranking language models to the problem of arguments extraction for comparative topics. Ours solutions rank third in both relevance, quality, and stance prediction evaluations.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/paper-255.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1089.98525
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/paper-255.mp3
   </guid>
<itunes:episode>
    166
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models
   </title>
<itunes:title>
    ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.
   </itunes:summary>
<description>
    Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2503.41875
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3
   </guid>
<itunes:episode>
    167
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Aspect-Controlled Neural Argument Generation
   </title>
<itunes:title>
    Aspect-Controlled Neural Argument Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we train a language model for argument generation that can be controlled on a fine-grained level to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that our generation model is able to generate high-quality, aspect-specific arguments. Moreover, these arguments can be used to improve the performance of stance detection models via data augmentation and to generate counter-arguments. We publish all datasets and code to fine-tune the language model.
   </itunes:summary>
<description>
    We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we train a language model for argument generation that can be controlled on a fine-grained level to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that our generation model is able to generate high-quality, aspect-specific arguments. Moreover, these arguments can be used to improve the performance of stance detection models via data augmentation and to generate counter-arguments. We publish all datasets and code to fine-tune the language model.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2005.00084v1.Aspect_Controlled_Neural_Argument_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2983.915
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2005.00084v1.Aspect_Controlled_Neural_Argument_Generation.mp3
   </guid>
<itunes:episode>
    168
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Multitasking Framework for Unsupervised Simple Definition Generation
   </title>
<itunes:title>
    Multitasking Framework for Unsupervised Simple Definition Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner's dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.
   </itunes:summary>
<description>
    The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner's dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12926v1.Multitasking_Framework_for_Unsupervised_Simple_Definition_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2079.11175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12926v1.Multitasking_Framework_for_Unsupervised_Simple_Definition_Generation.mp3
   </guid>
<itunes:episode>
    169
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification
   </title>
<itunes:title>
    What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taill\'e et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.
   </itunes:summary>
<description>
    Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taill\'e et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.13516v1.What_do_You_Mean_by_Relation_Extraction_A_Survey_on_Datasets_and_Study_on_Scientific_Relation_Classification.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2557.649
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.13516v1.What_do_You_Mean_by_Relation_Extraction_A_Survey_on_Datasets_and_Study_on_Scientific_Relation_Classification.mp3
   </guid>
<itunes:episode>
    170
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction
   </title>
<itunes:title>
    KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.
   </itunes:summary>
<description>
    Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2580.55825
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3
   </guid>
<itunes:episode>
    171
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Teaching Temporal Logics to Neural Networks
   </title>
<itunes:title>
    Teaching Temporal Logics to Neural Networks
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.
   </itunes:summary>
<description>
    We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04218v3.Teaching_Temporal_Logics_to_Neural_Networks.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2988.6695
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04218v3.Teaching_Temporal_Logics_to_Neural_Networks.mp3
   </guid>
<itunes:episode>
    172
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels
   </title>
<itunes:title>
    An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.
   </itunes:summary>
<description>
    Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11364v1.An_Information_theoretic_Approach_to_Prompt_Engineering_Without_Ground_Truth_Labels.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    5348.5715
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11364v1.An_Information_theoretic_Approach_to_Prompt_Engineering_Without_Ground_Truth_Labels.mp3
   </guid>
<itunes:episode>
    173
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Star Temporal Classification: Sequence Classification with Partially Labeled Data
   </title>
<itunes:title>
    Star Temporal Classification: Sequence Classification with Partially Labeled Data
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.
   </itunes:summary>
<description>
    We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2381.40075
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3
   </guid>
<itunes:episode>
    174
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    BERTMap: A BERT-based Ontology Alignment System
   </title>
<itunes:title>
    BERTMap: A BERT-based Ontology Alignment System
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.
   </itunes:summary>
<description>
    Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.02682v3.BERTMap_A_BERT_based_Ontology_Alignment_System.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2397.44
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.02682v3.BERTMap_A_BERT_based_Ontology_Alignment_System.mp3
   </guid>
<itunes:episode>
    175
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer
   </title>
<itunes:title>
    From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
   </itunes:summary>
<description>
    Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.02113v4.From_Discrimination_to_Generation_Knowledge_Graph_Completion_with_Generative_Transformer.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/2202.02113v4.From_Discrimination_to_Generation_Knowledge_Graph_Completion_with_Generative_Transformer.mp3.vtt"/>
<itunes:duration>
    822.852
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.02113v4.From_Discrimination_to_Generation_Knowledge_Graph_Completion_with_Generative_Transformer.mp3
   </guid>
<itunes:episode>
    176
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes
   </title>
<itunes:title>
    Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.   We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.   A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
   </itunes:summary>
<description>
    As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.   We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.   A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.09094v2.Scruples_A_Corpus_of_Community_Ethical_Judgments_on_32_000_Real_Life_Anecdotes.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4356.12725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.09094v2.Scruples_A_Corpus_of_Community_Ethical_Judgments_on_32_000_Real_Life_Anecdotes.mp3
   </guid>
<itunes:episode>
    177
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning
   </title>
<itunes:title>
    Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Neural models command state-of-the-art performance across NLP tasks, including ones involving "reasoning". Models claiming to reason about the evidence presented to them should attend to the correct parts of the input avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of models do so?} In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study -- they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over-sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine-tuning the model on perturbed data does not help it overcome the above challenges.
   </itunes:summary>
<description>
    Neural models command state-of-the-art performance across NLP tasks, including ones involving "reasoning". Models claiming to reason about the evidence presented to them should attend to the correct parts of the input avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of models do so?} In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study -- they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over-sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine-tuning the model on perturbed data does not help it overcome the above challenges.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.00578v3.Is_My_Model_Using_The_Right_Evidence_Systematic_Probes_for_Examining_Evidence_Based_Tabular_Reasoning.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3382.7005
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.00578v3.Is_My_Model_Using_The_Right_Evidence_Systematic_Probes_for_Examining_Evidence_Based_Tabular_Reasoning.mp3
   </guid>
<itunes:episode>
    178
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations
   </title>
<itunes:title>
    Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.
   </itunes:summary>
<description>
    Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.02937v1.Last_Layer_Re_Training_is_Sufficient_for_Robustness_to_Spurious_Correlations.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4514.8735
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.02937v1.Last_Layer_Re_Training_is_Sufficient_for_Robustness_to_Spurious_Correlations.mp3
   </guid>
<itunes:episode>
    179
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Decoding speech from non-invasive brain recordings
   </title>
<itunes:title>
    Decoding speech from non-invasive brain recordings
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.
   </itunes:summary>
<description>
    Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.12266v1.Decoding_speech_from_non_invasive_brain_recordings.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2668.30375
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.12266v1.Decoding_speech_from_non_invasive_brain_recordings.mp3
   </guid>
<itunes:episode>
    180
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?
   </title>
<itunes:title>
    How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The number of biomedical literature on new biomedical concepts is rapidly increasing, which necessitates a reliable biomedical named entity recognition (BioNER) model for identifying new and unseen entity mentions. However, it is questionable whether existing models can effectively handle them. In this work, we systematically analyze the three types of recognition abilities of BioNER models: memorization, synonym generalization, and concept generalization. We find that although current best models achieve state-of-the-art performance on benchmarks based on overall performance, they have limitations in identifying synonyms and new biomedical concepts, indicating they are overestimated in terms of their generalization abilities. We also investigate failure cases of models and identify several difficulties in recognizing unseen mentions in biomedical literature as follows: (1) models tend to exploit dataset biases, which hinders the models' abilities to generalize, and (2) several biomedical names have novel morphological patterns with weak name regularity, and models fail to recognize them. We apply a statistics-based debiasing method to our problem as a simple remedy and show the improvement in generalization to unseen mentions. We hope that our analyses and findings would be able to facilitate further research into the generalization capabilities of NER models in a domain where their reliability is of utmost importance.
   </itunes:summary>
<description>
    The number of biomedical literature on new biomedical concepts is rapidly increasing, which necessitates a reliable biomedical named entity recognition (BioNER) model for identifying new and unseen entity mentions. However, it is questionable whether existing models can effectively handle them. In this work, we systematically analyze the three types of recognition abilities of BioNER models: memorization, synonym generalization, and concept generalization. We find that although current best models achieve state-of-the-art performance on benchmarks based on overall performance, they have limitations in identifying synonyms and new biomedical concepts, indicating they are overestimated in terms of their generalization abilities. We also investigate failure cases of models and identify several difficulties in recognizing unseen mentions in biomedical literature as follows: (1) models tend to exploit dataset biases, which hinders the models' abilities to generalize, and (2) several biomedical names have novel morphological patterns with weak name regularity, and models fail to recognize them. We apply a statistics-based debiasing method to our problem as a simple remedy and show the improvement in generalization to unseen mentions. We hope that our analyses and findings would be able to facilitate further research into the generalization capabilities of NER models in a domain where their reliability is of utmost importance.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00160v3.How_Do_Your_Biomedical_Named_Entity_Recognition_Models_Generalize_to_Novel_Entities.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2831.334
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00160v3.How_Do_Your_Biomedical_Named_Entity_Recognition_Models_Generalize_to_Novel_Entities.mp3
   </guid>
<itunes:episode>
    181
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    BERT Busters: Outlier Dimensions that Disrupt Transformers
   </title>
<itunes:title>
    BERT Busters: Outlier Dimensions that Disrupt Transformers
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (&lt;0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.
   </itunes:summary>
<description>
    Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (&lt;0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06990v2.BERT_Busters_Outlier_Dimensions_that_Disrupt_Transformers.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2280.0195
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06990v2.BERT_Busters_Outlier_Dimensions_that_Disrupt_Transformers.mp3
   </guid>
<itunes:episode>
    182
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Kent Academic Repository Full text document (pdf) Versions of research Citation for published version Link to record in KAR Document Version UNSPECIFIED Moral Perfectionism and Moral Values, Virtues, and Judgments: A Preliminary Investigation
   </title>
<itunes:title>
    Kent Academic Repository Full text document (pdf) Versions of research Citation for published version Link to record in KAR Document Version UNSPECIFIED Moral Perfectionism and Moral Values, Virtues, and Judgments: A Preliminary Investigation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
</itunes:summary>
<description>
</description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Yang Stoeber Wang (2015) PAID.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1623.458
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Yang Stoeber Wang (2015) PAID.mp3
   </guid>
<itunes:episode>
    183
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation
   </title>
<itunes:title>
    Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithms remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.
   </itunes:summary>
<description>
    The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithms remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1804.06609v2.Fast_Lexically_Constrained_Decoding_with_Dynamic_Beam_Allocation_for_Neural_Machine_Translation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2279.236
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1804.06609v2.Fast_Lexically_Constrained_Decoding_with_Dynamic_Beam_Allocation_for_Neural_Machine_Translation.mp3
   </guid>
<itunes:episode>
    184
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    QuALITY: Question Answering with Long Input Texts, Yes!
   </title>
<itunes:title>
    QuALITY: Question Answering with Long Input Texts, Yes!
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).
   </itunes:summary>
<description>
    To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.08608v1.QuALITY_Question_Answering_with_Long_Input_Texts_Yes.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3701.76
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.08608v1.QuALITY_Question_Answering_with_Long_Input_Texts_Yes.mp3
   </guid>
<itunes:episode>
    185
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Deep reinforcement learning from human preferences
   </title>
<itunes:title>
    Deep reinforcement learning from human preferences
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.
   </itunes:summary>
<description>
    For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.03741v3.Deep_reinforcement_learning_from_human_preferences.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3256.6595
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.03741v3.Deep_reinforcement_learning_from_human_preferences.mp3
   </guid>
<itunes:episode>
    186
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts
   </title>
<itunes:title>
    Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer. Our code and corpus are available at https://github.com/abaheti95/ToxiChat .
   </itunes:summary>
<description>
    Dialogue models trained on human conversations inadvertently learn to generate toxic responses. In addition to producing explicitly offensive utterances, these models can also implicitly insult a group or individual by aligning themselves with an offensive statement. To better understand the dynamics of contextually offensive language, we investigate the stance of dialogue model responses in offensive Reddit conversations. Specifically, we create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model responses labeled with offensive language and stance. Our analysis reveals that 42% of human responses agree with toxic comments, whereas only 13% agree with safe comments. This undesirable behavior is learned by neural dialogue models, such as DialoGPT, which we show are two times more likely to agree with offensive comments. To enable automatic detection of offensive language, we fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the effectiveness of controllable text generation (CTG) methods to mitigate the tendency of neural dialogue models to agree with offensive comments. Compared to the baseline, our best CTG model achieves a 19% reduction in agreement with offensive comments and produces 29% fewer offensive replies. Our work highlights the need for further efforts to characterize and analyze inappropriate behavior in dialogue models, in order to help make them safer. Our code and corpus are available at https://github.com/abaheti95/ToxiChat .
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.11830v2.Just_Say_No_Analyzing_the_Stance_of_Neural_Dialogue_Generation_in_Offensive_Contexts.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2873.574
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.11830v2.Just_Say_No_Analyzing_the_Stance_of_Neural_Dialogue_Generation_in_Offensive_Contexts.mp3
   </guid>
<itunes:episode>
    187
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Do liberals and conservatives use different moral languages? Two replications and six extensions of Graham, Haidt, and Nosek's (2009) moral text analysis
   </title>
<itunes:title>
    Do liberals and conservatives use different moral languages? Two replications and six extensions of Graham, Haidt, and Nosek's (2009) moral text analysis
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    a b s t r a c t Do liberals and conservatives tend to use different moral languages? The Moral Foundations Hypothesis states that liberals rely more on foundations of care/harm and fairness/cheating whereas conservatives rely more on loyalty/betrayal, authority/subversion, and purity/degradation in their moral functioning. In support, Graham, Haidt, and Nosek (2009; Study 4) showed that sermons delivered by liberal and conservative pastors differed as predicted in their moral word usage, except for the loyalty foundation. I present two high-powered replication studies in religious contexts and six extension studies in politics, the media, and organizations to test ideological differences in moral language usage. On average, replication success rate was 30% and effect sizes were 38 times smaller than those in the original study.
   </itunes:summary>
<description>
    a b s t r a c t Do liberals and conservatives tend to use different moral languages? The Moral Foundations Hypothesis states that liberals rely more on foundations of care/harm and fairness/cheating whereas conservatives rely more on loyalty/betrayal, authority/subversion, and purity/degradation in their moral functioning. In support, Graham, Haidt, and Nosek (2009; Study 4) showed that sermons delivered by liberal and conservative pastors differed as predicted in their moral word usage, except for the loyalty foundation. I present two high-powered replication studies in religious contexts and six extension studies in politics, the media, and organizations to test ideological differences in moral language usage. On average, replication success rate was 30% and effect sizes were 38 times smaller than those in the original study.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0092656619301278-main.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4085.81225
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0092656619301278-main.mp3
   </guid>
<itunes:episode>
    188
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence
   </title>
<itunes:title>
    The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Organizations are making massive investments in artificial intelligence (AI), and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AI-and the AI itself-process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organization's use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the person's moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches (such as moral reframing) for thinking about conflicts in moral judgments concerning AI.
   </itunes:summary>
<description>
    Organizations are making massive investments in artificial intelligence (AI), and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AI-and the AI itself-process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organization's use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the person's moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches (such as moral reframing) for thinking about conflicts in moral judgments concerning AI.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-022-05057-6.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4303.30775
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-022-05057-6.mp3
   </guid>
<itunes:episode>
    189
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences
   </title>
<itunes:title>
    Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.
   </itunes:summary>
<description>
    In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.15738v1.Moral_Stories_Situated_Reasoning_about_Norms_Intents_Actions_and_their_Consequences.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2987.076
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.15738v1.Moral_Stories_Situated_Reasoning_about_Norms_Intents_Actions_and_their_Consequences.mp3
   </guid>
<itunes:episode>
    190
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Generative Model for Relation Extraction and Classification
   </title>
<itunes:title>
    A Generative Model for Relation Extraction and Classification
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Relation extraction (RE) is an important information extraction task which provides essential information to many NLP applications such as knowledge base population and question answering. In this paper, we present a novel generative model for relation extraction and classification (which we call GREC), where RE is modeled as a sequence-to-sequence generation task. We explore various encoding representations for the source and target sequences, and design effective schemes that enable GREC to achieve state-of-the-art performance on three benchmark RE datasets. In addition, we introduce negative sampling and decoding scaling techniques which provide a flexible tool to tune the precision and recall performance of the model. Our approach can be extended to extract all relation triples from a sentence in one pass. Although the one-pass approach incurs certain performance loss, it is much more computationally efficient.
   </itunes:summary>
<description>
    Relation extraction (RE) is an important information extraction task which provides essential information to many NLP applications such as knowledge base population and question answering. In this paper, we present a novel generative model for relation extraction and classification (which we call GREC), where RE is modeled as a sequence-to-sequence generation task. We explore various encoding representations for the source and target sequences, and design effective schemes that enable GREC to achieve state-of-the-art performance on three benchmark RE datasets. In addition, we introduce negative sampling and decoding scaling techniques which provide a flexible tool to tune the precision and recall performance of the model. Our approach can be extended to extract all relation triples from a sentence in one pass. Although the one-pass approach incurs certain performance loss, it is much more computationally efficient.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13229v1.A_Generative_Model_for_Relation_Extraction_and_Classification.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2381.5575
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13229v1.A_Generative_Model_for_Relation_Extraction_and_Classification.mp3
   </guid>
<itunes:episode>
    191
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Correcting Robot Plans with Natural Language Feedback
   </title>
<itunes:title>
    Correcting Robot Plans with Natural Language Feedback
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.
   </itunes:summary>
<description>
    When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05186v1.Correcting_Robot_Plans_with_Natural_Language_Feedback.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2761.0905
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05186v1.Correcting_Robot_Plans_with_Natural_Language_Feedback.mp3
   </guid>
<itunes:episode>
    192
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Improving Natural Language Inference Using External Knowledge in the Science Questions Domain
   </title>
<itunes:title>
    Improving Natural Language Inference Using External Knowledge in the Science Questions Domain
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Natural Language Inference (NLI) is fundamental to many Natural Language Processing (NLP) applications including semantic search and question answering. The NLI problem has gained significant attention thanks to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge -- a central topic in artificial intelligence -- has not received much attention vis-a-vis the NLI problem. While there are many open knowledge bases that contain various types of reasoning information, their use for NLI has not been well explored. To address this, we present a combination of techniques that harness knowledge graphs to improve performance on the NLI problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-to-graph based models, and discuss implications for the use of external knowledge in solving the NLI problem. Our model achieves the new state-of-the-art performance on the NLI problem over the SciTail science questions dataset.
   </itunes:summary>
<description>
    Natural Language Inference (NLI) is fundamental to many Natural Language Processing (NLP) applications including semantic search and question answering. The NLI problem has gained significant attention thanks to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge -- a central topic in artificial intelligence -- has not received much attention vis-a-vis the NLI problem. While there are many open knowledge bases that contain various types of reasoning information, their use for NLI has not been well explored. To address this, we present a combination of techniques that harness knowledge graphs to improve performance on the NLI problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-to-graph based models, and discuss implications for the use of external knowledge in solving the NLI problem. Our model achieves the new state-of-the-art performance on the NLI problem over the SciTail science questions dataset.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.05724v2.Improving_Natural_Language_Inference_Using_External_Knowledge_in_the_Science_Questions_Domain.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2329.2865
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.05724v2.Improving_Natural_Language_Inference_Using_External_Knowledge_in_the_Science_Questions_Domain.mp3
   </guid>
<itunes:episode>
    193
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
   </title>
<itunes:title>
    Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.
   </itunes:summary>
<description>
    In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.02177v1.Grokking_Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1779.5395
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.02177v1.Grokking_Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets.mp3
   </guid>
<itunes:episode>
    194
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail
   </title>
<itunes:title>
    The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.
   </itunes:summary>
<description>
    Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.08300v3.The_Dangers_of_Underclaiming_Reasons_for_Caution_When_Reporting_How_NLP_Systems_Fail.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2660.075
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.08300v3.The_Dangers_of_Underclaiming_Reasons_for_Caution_When_Reporting_How_NLP_Systems_Fail.mp3
   </guid>
<itunes:episode>
    195
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    An Empirical Investigation of Pre-Trained Transformer Language Models for Open-Domain Dialogue Generation
   </title>
<itunes:title>
    An Empirical Investigation of Pre-Trained Transformer Language Models for Open-Domain Dialogue Generation
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We present an empirical investigation of pre-trained Transformer-based auto-regressive language models for the task of open-domain dialogue generation. Training paradigm of pre-training and fine-tuning is employed to conduct the parameter learning. Corpora of News and Wikipedia in Chinese and English are collected for the pre-training stage respectively. Dialogue context and response are concatenated into a single sequence utilized as the input of the models during the fine-tuning stage. A weighted joint prediction paradigm for both context and response is designed to evaluate the performance of models with or without the loss term for context prediction. Various of decoding strategies such as greedy search, beam search, top-k sampling, etc. are employed to conduct the response text generation. Extensive experiments are conducted on the typical single-turn and multi-turn dialogue corpora such as Weibo, Douban, Reddit, DailyDialog, and Persona-Chat. Detailed numbers of automatic evaluation metrics on relevance and diversity of the generated results for the languages models as well as the baseline approaches are reported.
   </itunes:summary>
<description>
    We present an empirical investigation of pre-trained Transformer-based auto-regressive language models for the task of open-domain dialogue generation. Training paradigm of pre-training and fine-tuning is employed to conduct the parameter learning. Corpora of News and Wikipedia in Chinese and English are collected for the pre-training stage respectively. Dialogue context and response are concatenated into a single sequence utilized as the input of the models during the fine-tuning stage. A weighted joint prediction paradigm for both context and response is designed to evaluate the performance of models with or without the loss term for context prediction. Various of decoding strategies such as greedy search, beam search, top-k sampling, etc. are employed to conduct the response text generation. Extensive experiments are conducted on the typical single-turn and multi-turn dialogue corpora such as Weibo, Douban, Reddit, DailyDialog, and Persona-Chat. Detailed numbers of automatic evaluation metrics on relevance and diversity of the generated results for the languages models as well as the baseline approaches are reported.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04195v1.An_Empirical_Investigation_of_Pre_Trained_Transformer_Language_Models_for_Open_Domain_Dialogue_Generation.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    1735.18375
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04195v1.An_Empirical_Investigation_of_Pre_Trained_Transformer_Language_Models_for_Open_Domain_Dialogue_Generation.mp3
   </guid>
<itunes:episode>
    196
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Triple-to-Text: Converting RDF Triples into High-Quality Natural Languages via Optimizing an Inverse KL Divergence
   </title>
<itunes:title>
    Triple-to-Text: Converting RDF Triples into High-Quality Natural Languages via Optimizing an Inverse KL Divergence
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Knowledge base is one of the main forms to represent information in a structured way. A knowledge base typically consists of Resource Description Frameworks (RDF) triples which describe the entities and their relations. Generating natural language description of the knowledge base is an important task in NLP, which has been formulated as a conditional language generation task and tackled using the sequence-to-sequence framework. Current works mostly train the language models by maximum likelihood estimation, which tends to generate lousy sentences. In this paper, we argue that such a problem of maximum likelihood estimation is intrinsic, which is generally irrevocable via changing network structures. Accordingly, we propose a novel Triple-to-Text (T2T) framework, which approximately optimizes the inverse Kullback-Leibler (KL) divergence between the distributions of the real and generated sentences. Due to the nature that inverse KL imposes large penalty on fake-looking samples, the proposed method can significantly reduce the probability of generating low-quality sentences. Our experiments on three real-world datasets demonstrate that T2T can generate higher-quality sentences and outperform baseline models in several evaluation metrics.
   </itunes:summary>
<description>
    Knowledge base is one of the main forms to represent information in a structured way. A knowledge base typically consists of Resource Description Frameworks (RDF) triples which describe the entities and their relations. Generating natural language description of the knowledge base is an important task in NLP, which has been formulated as a conditional language generation task and tackled using the sequence-to-sequence framework. Current works mostly train the language models by maximum likelihood estimation, which tends to generate lousy sentences. In this paper, we argue that such a problem of maximum likelihood estimation is intrinsic, which is generally irrevocable via changing network structures. Accordingly, we propose a novel Triple-to-Text (T2T) framework, which approximately optimizes the inverse Kullback-Leibler (KL) divergence between the distributions of the real and generated sentences. Due to the nature that inverse KL imposes large penalty on fake-looking samples, the proposed method can significantly reduce the probability of generating low-quality sentences. Our experiments on three real-world datasets demonstrate that T2T can generate higher-quality sentences and outperform baseline models in several evaluation metrics.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01965v1.Triple_to_Text_Converting_RDF_Triples_into_High_Quality_Natural_Languages_via_Optimizing_an_Inverse_KL_Divergence.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2619.68975
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01965v1.Triple_to_Text_Converting_RDF_Triples_into_High_Quality_Natural_Languages_via_Optimizing_an_Inverse_KL_Divergence.mp3
   </guid>
<itunes:episode>
    197
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics
   </title>
<itunes:title>
    COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.
   </itunes:summary>
<description>
    Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.11705v2.COLD_Decoding_Energy_based_Constrained_Text_Generation_with_Langevin_Dynamics.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3260.0295
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.11705v2.COLD_Decoding_Energy_based_Constrained_Text_Generation_with_Langevin_Dynamics.mp3
   </guid>
<itunes:episode>
    198
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    The normative challenge for illusionist views of consciousness
   </title>
<itunes:title>
    The normative challenge for illusionist views of consciousness
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Illusionists about phenomenal consciousness claim that phenomenal consciousness does not exist but merely seems to exist. At the same time, it is quite intuitive for there to be some kind of link between phenomenality and value. For example, some situations seem good or bad in virtue of the conscious experiences they feature. Illusionist views of phenomenal consciousness then face what I call the normative challenge. They have to say where they stand regarding the idea that there is a link between phenomenality and value. If they accept that there is such a link, they might be committed to revisionary normative consequences (and some of them may prove to be uncomfortable). If they deny that there is such link, they might avoid revisionary normative consequences (without being guaranteed against them) but then they have to give reasons to deny that such link obtains, which is not a trivial task. The existence of the normative challenge does not show that illusionism is false, but it shows that illusionism might have important consequences in the normative domain, which have to be clarified.
   </itunes:summary>
<description>
    Illusionists about phenomenal consciousness claim that phenomenal consciousness does not exist but merely seems to exist. At the same time, it is quite intuitive for there to be some kind of link between phenomenality and value. For example, some situations seem good or bad in virtue of the conscious experiences they feature. Illusionist views of phenomenal consciousness then face what I call the normative challenge. They have to say where they stand regarding the idea that there is a link between phenomenality and value. If they accept that there is such a link, they might be committed to revisionary normative consequences (and some of them may prove to be uncomfortable). If they deny that there is such link, they might avoid revisionary normative consequences (without being guaranteed against them) but then they have to give reasons to deny that such link obtains, which is not a trivial task. The existence of the normative challenge does not show that illusionism is false, but it shows that illusionism might have important consequences in the normative domain, which have to be clarified.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/KAMTNC-2v1.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3091.48725
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/KAMTNC-2v1.mp3
   </guid>
<itunes:episode>
    199
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Efficient Methods for Natural Language Processing: A Survey
   </title>
<itunes:title>
    Efficient Methods for Natural Language Processing: A Survey
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Getting the most out of limited resources allows advances in natural language processing (NLP) research and practice while being conservative with resources. Those resources may be data, time, storage, or energy. Recent work in NLP has yielded interesting results from scaling; however, using only scale to improve results means that resource consumption also scales. That relationship motivates research into efficient methods that require less resources to achieve similar results. This survey relates and synthesises methods and findings in those efficiencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.
   </itunes:summary>
<description>
    Getting the most out of limited resources allows advances in natural language processing (NLP) research and practice while being conservative with resources. Those resources may be data, time, storage, or energy. Recent work in NLP has yielded interesting results from scaling; however, using only scale to improve results means that resource consumption also scales. That relationship motivates research into efficient methods that require less resources to achieve similar results. This survey relates and synthesises methods and findings in those efficiencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00099v1.Efficient_Methods_for_Natural_Language_Processing_A_Survey.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2905.848
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00099v1.Efficient_Methods_for_Natural_Language_Processing_A_Survey.mp3
   </guid>
<itunes:episode>
    200
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    It's a Match: Moralization and the Effects of Moral Foundations Congruence on Ethical and Unethical Leadership Perception
   </title>
<itunes:title>
    It's a Match: Moralization and the Effects of Moral Foundations Congruence on Ethical and Unethical Leadership Perception
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    While much research has focused on the effects of ethical and unethical leadership, little is known about how followers come to perceive their leaders as ethical or unethical. In this article, we investigate the co-creation of ethical and unethical leadership perceptions. Specifically, we draw from emerging research on moral congruence in organizational behaviour and empirically investigate the role of congruence in leaders' and followers' moral foundations in followers' perceptions of ethical and unethical leadership. By analysing objective congruence scores from 67 leader-follower dyads by means of polynomial regression with surface response analysis, we find partial support for our theoretically derived predictions. Significant effects were revealed for the fairness, loyalty, and authority moral foundations but not for the care and sanctity moral foundations. We discuss theoretical and practical implications of these findings.
   </itunes:summary>
<description>
    While much research has focused on the effects of ethical and unethical leadership, little is known about how followers come to perceive their leaders as ethical or unethical. In this article, we investigate the co-creation of ethical and unethical leadership perceptions. Specifically, we draw from emerging research on moral congruence in organizational behaviour and empirically investigate the role of congruence in leaders' and followers' moral foundations in followers' perceptions of ethical and unethical leadership. By analysing objective congruence scores from 67 leader-follower dyads by means of polynomial regression with surface response analysis, we find partial support for our theoretically derived predictions. Significant effects were revealed for the fairness, loyalty, and authority moral foundations but not for the care and sanctity moral foundations. We discuss theoretical and practical implications of these findings.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-019-04178-9.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4337.92
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-019-04178-9.mp3
   </guid>
<itunes:episode>
    201
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    ReAct: Synergizing Reasoning and Acting in Language Models
   </title>
<itunes:title>
    ReAct: Synergizing Reasoning and Acting in Language Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.
   </itunes:summary>
<description>
    While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.03629v1.ReAct_Synergizing_Reasoning_and_Acting_in_Language_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3794.39025
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2210.03629v1.ReAct_Synergizing_Reasoning_and_Acting_in_Language_Models.mp3
   </guid>
<itunes:episode>
    202
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Towards mental time travel: a hierarchical memory for reinforcement learning agents
   </title>
<itunes:title>
    Towards mental time travel: a hierarchical memory for reinforcement learning agents
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore "mentally time-travel" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.
   </itunes:summary>
<description>
    Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore "mentally time-travel" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.14039v3.Towards_mental_time_travel_a_hierarchical_memory_for_reinforcement_learning_agents.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    4672.88825
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.14039v3.Towards_mental_time_travel_a_hierarchical_memory_for_reinforcement_learning_agents.mp3
   </guid>
<itunes:episode>
    203
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Transformers are Sample Efficient World Models
   </title>
<itunes:title>
    Transformers are Sample Efficient World Models
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.
   </itunes:summary>
<description>
    Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00588v1.Transformers_are_Sample_Efficient_World_Models.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2326.9095
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00588v1.Transformers_are_Sample_Efficient_World_Models.mp3
   </guid>
<itunes:episode>
    204
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    A Conversational Paradigm for Program Synthesis
   </title>
<itunes:title>
    A Conversational Paradigm for Program Synthesis
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.
   </itunes:summary>
<description>
    Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13474v3.A_Conversational_Paradigm_for_Program_Synthesis.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3244.8
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13474v3.A_Conversational_Paradigm_for_Program_Synthesis.mp3
   </guid>
<itunes:episode>
    205
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Deep Double Descent: Where Bigger Models and More Data Hurt
   </title>
<itunes:title>
    Deep Double Descent: Where Bigger Models and More Data Hurt
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.
   </itunes:summary>
<description>
    We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02292v1.Deep_Double_Descent_Where_Bigger_Models_and_More_Data_Hurt.mp3"/>
<enclosure length="" type="text/vtt" vtt_url="https://g-simmons.github.io/g-simmons-papercast/data/vtt/1912.02292v1.Deep_Double_Descent_Where_Bigger_Models_and_More_Data_Hurt.mp3.vtt"/>
<itunes:duration>
    2337.3
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02292v1.Deep_Double_Descent_Where_Bigger_Models_and_More_Data_Hurt.mp3
   </guid>
<itunes:episode>
    206
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question
   </title>
<itunes:title>
    RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer module, in which we use pre-trained models from the existing literature, and therefore, our metric can be used without further training. We show that RQUGE has a higher correlation with human judgment without relying on the reference question. RQUGE is shown to be significantly more robust to several adversarial corruptions. Additionally, we illustrate that we can significantly improve the performance of QA models on out-of-domain datasets by fine-tuning on the synthetic data generated by a question generation model and re-ranked by RQUGE.
   </itunes:summary>
<description>
    Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer module, in which we use pre-trained models from the existing literature, and therefore, our metric can be used without further training. We show that RQUGE has a higher correlation with human judgment without relying on the reference question. RQUGE is shown to be significantly more robust to several adversarial corruptions. Additionally, we illustrate that we can significantly improve the performance of QA models on out-of-domain datasets by fine-tuning on the synthetic data generated by a question generation model and re-ranked by RQUGE.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.01482v2.RQUGE_Reference_Free_Metric_for_Evaluating_Question_Generation_by_Answering_the_Question.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    2209.67175
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2211.01482v2.RQUGE_Reference_Free_Metric_for_Evaluating_Question_Generation_by_Answering_the_Question.mp3
   </guid>
<itunes:episode>
    207
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>
    Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction
   </title>
<itunes:title>
    Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction
   </itunes:title>
<itunes:author>
    Gabriel Simmons
   </itunes:author>
<itunes:subtitle/>
<itunes:summary>
    Recent progress in neural information retrieval has demonstrated large gains in effectiveness, while often sacrificing the efficiency and interpretability of the neural model compared to classical approaches. This paper proposes ColBERTer, a neural retrieval model using contextualized late interaction (ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier, ColBERTer's reductions dramatically lower ColBERT's storage requirements while simultaneously improving the interpretability of its token-matching scores. To this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and optional lexical matching components into one model. For its multi-vector component, ColBERTer reduces the number of stored vectors per document by learning unique whole-word representations for the terms in each document and learning to identify and remove word representations that are not essential to effective scoring. We employ an explicit multi-task, multi-stage training to facilitate using very small vector dimensions. Results on the MS MARCO and TREC-DL collection show that ColBERTer can reduce the storage footprint by up to 2.5x, while maintaining effectiveness. With just one dimension per token in its smallest setting, ColBERTer achieves index storage parity with the plaintext size, with very strong effectiveness results. Finally, we demonstrate ColBERTer's robustness on seven high-quality out-of-domain collections, yielding statistically significant gains over traditional retrieval baselines.
   </itunes:summary>
<description>
    Recent progress in neural information retrieval has demonstrated large gains in effectiveness, while often sacrificing the efficiency and interpretability of the neural model compared to classical approaches. This paper proposes ColBERTer, a neural retrieval model using contextualized late interaction (ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier, ColBERTer's reductions dramatically lower ColBERT's storage requirements while simultaneously improving the interpretability of its token-matching scores. To this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and optional lexical matching components into one model. For its multi-vector component, ColBERTer reduces the number of stored vectors per document by learning unique whole-word representations for the terms in each document and learning to identify and remove word representations that are not essential to effective scoring. We employ an explicit multi-task, multi-stage training to facilitate using very small vector dimensions. Results on the MS MARCO and TREC-DL collection show that ColBERTer can reduce the storage footprint by up to 2.5x, while maintaining effectiveness. With just one dimension per token in its smallest setting, ColBERTer achieves index storage parity with the plaintext size, with very strong effectiveness results. Finally, we demonstrate ColBERTer's robustness on seven high-quality out-of-domain collections, yielding statistically significant gains over traditional retrieval baselines.
   </description>
<enclosure length="" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13088v1.Introducing_Neural_Bag_of_Whole_Words_with_ColBERTer_Contextualized_Late_Interactions_using_Enhanced_Reduction.mp3"/>
<enclosure length="" type="text/vtt" vtt_url=""/>
<itunes:duration>
    3681.5935
   </itunes:duration>
<itunes:season>
    1
   </itunes:season>
<itunes:episodeType>
    full
   </itunes:episodeType>
<guid isPermaLink="false">
    https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13088v1.Introducing_Neural_Bag_of_Whole_Words_with_ColBERTer_Contextualized_Late_Interactions_using_Enhanced_Reduction.mp3
   </guid>
<itunes:episode>
    208
   </itunes:episode>
<pubDate/>
<itunes:explicit>
    NO
   </itunes:explicit>
</item>
<item>
<title>CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos</title>
<itunes:title>CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary>&lt;![CDATA[Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.]]&gt;</itunes:summary>
<description>&lt;![CDATA[Visual information is central to conversation: body gestures and facial expressions, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce CHAMPAGNE, a generative model of conversations that can account for visual contexts. To train CHAMPAGNE, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning. Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog, 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNE learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.]]&gt;</description>
<enclosure length="2552.346" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/champagne-learning-real-world-conversation-from-large-scale-web-videos.mp3"/>
<itunes:duration>2552.346</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>209</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/champagne-learning-real-world-conversation-from-large-scale-web-videos.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item>
<item>
<title>Supervising strong learners by amplifying weak experts</title>
<itunes:title>Supervising strong learners by amplifying weak experts</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary>&lt;![CDATA[Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.]]&gt;</itunes:summary>
<description>&lt;![CDATA[Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.]]&gt;</description>
<enclosure length="2835.7225" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/supervising-strong-learners-by-amplifying-weak-experts.mp3"/>
<itunes:duration>2835.7225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>210</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/supervising-strong-learners-by-amplifying-weak-experts.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor</title>
<itunes:title>Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2788.885" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/unnatural-instructions-tuning-language-models-with-almost-no-human-labor.mp3"/>
<itunes:duration>2788.885</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>211</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/unnatural-instructions-tuning-language-models-with-almost-no-human-labor.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</title>
<itunes:title>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1850.38375" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/mrkl-systems-a-modular-neuro-symbolic-architecture-that-combines-large-language-models-external-knowledge-sources-and-discrete-reasoning.mp3"/>
<itunes:duration>1850.38375</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>212</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/mrkl-systems-a-modular-neuro-symbolic-architecture-that-combines-large-language-models-external-knowledge-sources-and-discrete-reasoning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Learning to Influence Human Behavior with Offline Reinforcement Learning</title>
<itunes:title>Learning to Influence Human Behavior with Offline Reinforcement Learning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2678.96175" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/learning-to-influence-human-behavior-with-offline-reinforcement-learning.mp3"/>
<itunes:duration>2678.96175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>213</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/learning-to-influence-human-behavior-with-offline-reinforcement-learning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey</title>
<itunes:title>Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="7530.39675" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/adversarial-attacks-on-deep-learning-models-in-natural-language-processing-a-survey.mp3"/>
<itunes:duration>7530.39675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>214</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/adversarial-attacks-on-deep-learning-models-in-natural-language-processing-a-survey.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Self-Refine: Iterative Refinement with Self-Feedback</title>
<itunes:title>Self-Refine: Iterative Refinement with Self-Feedback</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4393.22125" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/self-refine-iterative-refinement-with-self-feedback.mp3"/>
<itunes:duration>4393.22125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>215</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/self-refine-iterative-refinement-with-self-feedback.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Whose Opinions Do Language Models Reflect?</title>
<itunes:title>Whose Opinions Do Language Models Reflect?</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3684.258" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/whose-opinions-do-language-models-reflect.mp3"/>
<itunes:duration>3684.258</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>216</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/whose-opinions-do-language-models-reflect.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>SoK: On the Impossible Security of Very Large Foundation Models</title>
<itunes:title>SoK: On the Impossible Security of Very Large Foundation Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="5067.311" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/sok-on-the-impossible-security-of-very-large-foundation-models.mp3"/>
<itunes:duration>5067.311</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>217</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/sok-on-the-impossible-security-of-very-large-foundation-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Training Language Models with Language Feedback at Scale</title>
<itunes:title>Training Language Models with Language Feedback at Scale</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="7058.6775" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/training-language-models-with-language-feedback-at-scale.mp3"/>
<itunes:duration>7058.6775</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>218</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/training-language-models-with-language-feedback-at-scale.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval</title>
<itunes:title>Rethinking the Role of Token Retrieval in Multi-Vector Retrieval</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3492.3625" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/rethinking-the-role-of-token-retrieval-in-multi-vector-retrieval.mp3"/>
<itunes:duration>3492.3625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>219</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/rethinking-the-role-of-token-retrieval-in-multi-vector-retrieval.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Elucidating the Design Space of Diffusion-Based Generative Models</title>
<itunes:title>Elucidating the Design Space of Diffusion-Based Generative Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="8820.21875" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/elucidating-the-design-space-of-diffusion-based-generative-models.mp3"/>
<itunes:duration>8820.21875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>220</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/elucidating-the-design-space-of-diffusion-based-generative-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Harms from Increasingly Agentic Algorithmic Systems</title>
<itunes:title>Harms from Increasingly Agentic Algorithmic Systems</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4280.81625" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3"/>
<itunes:duration>4280.81625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>221</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Generative Cooperative Networks for Natural Language Generation</title>
<itunes:title>Generative Cooperative Networks for Natural Language Generation</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3090.756" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/generative-cooperative-networks-for-natural-language-generation.mp3"/>
<itunes:duration>3090.756</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>222</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/generative-cooperative-networks-for-natural-language-generation.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Harms from Increasingly Agentic Algorithmic Systems</title>
<itunes:title>Harms from Increasingly Agentic Algorithmic Systems</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4280.6335" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3"/>
<itunes:duration>4280.6335</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>223</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Social Bias Frames: Reasoning about Social and Power Implications of Language</title>
<itunes:title>Social Bias Frames: Reasoning about Social and Power Implications of Language</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2320.48325" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/social-bias-frames-reasoning-about-social-and-power-implications-of-language.mp3"/>
<itunes:duration>2320.48325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>224</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/social-bias-frames-reasoning-about-social-and-power-implications-of-language.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback</title>
<itunes:title>Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="6035.59175" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/personalisation-within-bounds-a-risk-taxonomy-and-policy-framework-for-the-alignment-of-large-language-models-with-personalised-feedback.mp3"/>
<itunes:duration>6035.59175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>225</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/personalisation-within-bounds-a-risk-taxonomy-and-policy-framework-for-the-alignment-of-large-language-models-with-personalised-feedback.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Measuring and Manipulating Knowledge Representations in Language Models</title>
<itunes:title>Measuring and Manipulating Knowledge Representations in Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4300.9045" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-and-manipulating-knowledge-representations-in-language-models.mp3"/>
<itunes:duration>4300.9045</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>226</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-and-manipulating-knowledge-representations-in-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Social Dynamics of AI Support in Creative Writing</title>
<itunes:title>Social Dynamics of AI Support in Creative Writing</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4550.58275" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/social-dynamics-of-ai-support-in-creative-writing.mp3"/>
<itunes:duration>4550.58275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>227</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/social-dynamics-of-ai-support-in-creative-writing.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Measuring and Manipulating Knowledge Representations in Language Models</title>
<itunes:title>Measuring and Manipulating Knowledge Representations in Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4300.95675" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-and-manipulating-knowledge-representations-in-language-models.mp3"/>
<itunes:duration>4300.95675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>228</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-and-manipulating-knowledge-representations-in-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Prompting Is Programming: A Query Language For Large Language Models</title>
<itunes:title>Prompting Is Programming: A Query Language For Large Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4808.333" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/prompting-is-programming-a-query-language-for-large-language-models.mp3"/>
<itunes:duration>4808.333</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>229</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/prompting-is-programming-a-query-language-for-large-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Using cognitive psychology to understand GPT-3</title>
<itunes:title>Using cognitive psychology to understand GPT-3</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3660.30375" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/using-cognitive-psychology-to-understand-gpt-3.mp3"/>
<itunes:duration>3660.30375</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>230</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/using-cognitive-psychology-to-understand-gpt-3.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Power-seeking can be probable and predictive for trained agents</title>
<itunes:title>Power-seeking can be probable and predictive for trained agents</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1129.6915" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/power-seeking-can-be-probable-and-predictive-for-trained-agents.mp3"/>
<itunes:duration>1129.6915</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>231</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/power-seeking-can-be-probable-and-predictive-for-trained-agents.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales</title>
<itunes:title>Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1335.693" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/research-without-re-search-maximal-update-parametrization-yields-accurate-loss-prediction-across-scales.mp3"/>
<itunes:duration>1335.693</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>232</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/research-without-re-search-maximal-update-parametrization-yields-accurate-loss-prediction-across-scales.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Language Instructed Reinforcement Learning for Human-AI Coordination</title>
<itunes:title>Language Instructed Reinforcement Learning for Human-AI Coordination</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3704.4505" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/language-instructed-reinforcement-learning-for-human-ai-coordination.mp3"/>
<itunes:duration>3704.4505</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>233</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/language-instructed-reinforcement-learning-for-human-ai-coordination.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models</title>
<itunes:title>Polling Latent Opinions: A Method for Computational Sociolinguistics Using Transformer Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2136.68575" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/polling-latent-opinions-a-method-for-computational-sociolinguistics-using-transformer-language-models.mp3"/>
<itunes:duration>2136.68575</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>234</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/polling-latent-opinions-a-method-for-computational-sociolinguistics-using-transformer-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Fine-tuning language models to find agreement among humans with diverse preferences</title>
<itunes:title>Fine-tuning language models to find agreement among humans with diverse preferences</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary> 70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (&gt; 65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another. * Authors contributed equally to this work]]&gt;</itunes:summary>
<description> 70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (&gt; 65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another. * Authors contributed equally to this work]]&gt;</description>
<enclosure length="5809.97225" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/fine-tuning-language-models-to-find-agreement-among-humans-with-diverse-preferences.mp3"/>
<itunes:duration>5809.97225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>235</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/fine-tuning-language-models-to-find-agreement-among-humans-with-diverse-preferences.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Theory of Mind May Have Spontaneously Emerged in Large Language Models</title>
<itunes:title>Theory of Mind May Have Spontaneously Emerged in Large Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1711.4645" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/theory-of-mind-may-have-spontaneously-emerged-in-large-language-models.mp3"/>
<itunes:duration>1711.4645</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>236</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/theory-of-mind-may-have-spontaneously-emerged-in-large-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>DISSOCIATING LANGUAGE AND THOUGHT IN LARGE LANGUAGE MODELS: A COGNITIVE PERSPECTIVE A PREPRINT</title>
<itunes:title>DISSOCIATING LANGUAGE AND THOUGHT IN LARGE LANGUAGE MODELS: A COGNITIVE PERSPECTIVE A PREPRINT</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="7036.4995" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-a-preprint.mp3"/>
<itunes:duration>7036.4995</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>237</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/dissociating-language-and-thought-in-large-language-models-a-cognitive-perspective-a-preprint.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>The Power of Scale for Parameter-Efficient Prompt Tuning</title>
<itunes:title>The Power of Scale for Parameter-Efficient Prompt Tuning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2744.89475" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/the-power-of-scale-for-parameter-efficient-prompt-tuning.mp3"/>
<itunes:duration>2744.89475</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>238</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/the-power-of-scale-for-parameter-efficient-prompt-tuning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Learning Causal Overhypotheses through Exploration in Children and Computational Models</title>
<itunes:title>Learning Causal Overhypotheses through Exploration in Children and Computational Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3155.67025" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/learning-causal-overhypotheses-through-exploration-in-children-and-computational-models.mp3"/>
<itunes:duration>3155.67025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>239</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/learning-causal-overhypotheses-through-exploration-in-children-and-computational-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Artificially Precise Extremism: How Internet-Trained LLMs Exaggerate Our Differences</title>
<itunes:title>Artificially Precise Extremism: How Internet-Trained LLMs Exaggerate Our Differences</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4620.40825" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/artificially-precise-extremism-how-internet-trained-llms-exaggerate-our-differences.mp3"/>
<itunes:duration>4620.40825</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>240</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/artificially-precise-extremism-how-internet-trained-llms-exaggerate-our-differences.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Design and Evaluation of "The Missing CS Class," A Student-led Undergraduate Course to Reduce the Academia-industry Gap</title>
<itunes:title>Design and Evaluation of "The Missing CS Class," A Student-led Undergraduate Course to Reduce the Academia-industry Gap</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2392.50275" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/design-and-evaluation-of-the-missing-cs-class-a-student-led-undergraduate-course-to-reduce-the-academia-industry-gap.mp3"/>
<itunes:duration>2392.50275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>241</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/design-and-evaluation-of-the-missing-cs-class-a-student-led-undergraduate-course-to-reduce-the-academia-industry-gap.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Title: Challenging the appearance of machine intelligence: Cognitive bias in LLMs</title>
<itunes:title>Title: Challenging the appearance of machine intelligence: Cognitive bias in LLMs</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3754.71025" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/title-challenging-the-appearance-of-machine-intelligence-cognitive-bias-in-llms.mp3"/>
<itunes:duration>3754.71025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>242</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/title-challenging-the-appearance-of-machine-intelligence-cognitive-bias-in-llms.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>THREE ERAS OF SURVEY RESEARCH</title>
<itunes:title>THREE ERAS OF SURVEY RESEARCH</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1893.69475" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/three-eras-of-survey-research.mp3"/>
<itunes:duration>1893.69475</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>243</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/three-eras-of-survey-research.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Measuring Public Opinion with Surveys</title>
<itunes:title>Measuring Public Opinion with Surveys</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4211.644" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-public-opinion-with-surveys.mp3"/>
<itunes:duration>4211.644</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>244</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/measuring-public-opinion-with-surveys.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Discovering Language Model Behaviors with Model-Written Evaluations</title>
<itunes:title>Discovering Language Model Behaviors with Model-Written Evaluations</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="8947.9575" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/discovering-language-model-behaviors-with-model-written-evaluations.mp3"/>
<itunes:duration>8947.9575</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>245</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/discovering-language-model-behaviors-with-model-written-evaluations.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Offline Multi-Action Policy Learning: Generalization and Optimization</title>
<itunes:title>Offline Multi-Action Policy Learning: Generalization and Optimization</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="12022.7265" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/offline-multi-action-policy-learning-generalization-and-optimization.mp3"/>
<itunes:duration>12022.7265</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>246</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/offline-multi-action-policy-learning-generalization-and-optimization.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Estimating Individualized Treatment Rules Using Outcome Weighted Learning</title>
<itunes:title>Estimating Individualized Treatment Rules Using Outcome Weighted Learning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2787.78775" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/estimating-individualized-treatment-rules-using-outcome-weighted-learning.mp3"/>
<itunes:duration>2787.78775</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>247</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/estimating-individualized-treatment-rules-using-outcome-weighted-learning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Beyond prediction: Using big data for policy problems</title>
<itunes:title>Beyond prediction: Using big data for policy problems</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1077.0025" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/beyond-prediction-using-big-data-for-policy-problems.mp3"/>
<itunes:duration>1077.0025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>248</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/beyond-prediction-using-big-data-for-policy-problems.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Policy Learning with Observational Data</title>
<itunes:title>Policy Learning with Observational Data</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="8162.4295" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/policy-learning-with-observational-data.mp3"/>
<itunes:duration>8162.4295</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>249</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/policy-learning-with-observational-data.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Ethical and social risks of harm from Language Models</title>
<itunes:title>Ethical and social risks of harm from Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="10196.40175" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ethical-and-social-risks-of-harm-from-language-models.mp3"/>
<itunes:duration>10196.40175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>250</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ethical-and-social-risks-of-harm-from-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>A Theory of Emergent In-Context Learning as Implicit Structure Induction</title>
<itunes:title>A Theory of Emergent In-Context Learning as Implicit Structure Induction</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="9826.2465" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-theory-of-emergent-in-context-learning-as-implicit-structure-induction.mp3"/>
<itunes:duration>9826.2465</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>251</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-theory-of-emergent-in-context-learning-as-implicit-structure-induction.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Generative models for sequential dynamics in active inference</title>
<itunes:title>Generative models for sequential dynamics in active inference</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3262.77225" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/generative-models-for-sequential-dynamics-in-active-inference.mp3"/>
<itunes:duration>3262.77225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>252</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/generative-models-for-sequential-dynamics-in-active-inference.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>MemPrompt: Memory-assisted Prompt Editing with User Feedback</title>
<itunes:title>MemPrompt: Memory-assisted Prompt Editing with User Feedback</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4836.96325" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/memprompt-memory-assisted-prompt-editing-with-user-feedback.mp3"/>
<itunes:duration>4836.96325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>253</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/memprompt-memory-assisted-prompt-editing-with-user-feedback.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Conformal Nucleus Sampling</title>
<itunes:title>Conformal Nucleus Sampling</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1181.884" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/conformal-nucleus-sampling.mp3"/>
<itunes:duration>1181.884</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>254</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/conformal-nucleus-sampling.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance</title>
<itunes:title>FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2199.11825" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance.mp3"/>
<itunes:duration>2199.11825</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>255</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frugalgpt-how-to-use-large-language-models-while-reducing-cost-and-improving-performance.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Evaluating Group Work in (too) Large CS Classes with (too) Few Resources: An Experience Report</title>
<itunes:title>Evaluating Group Work in (too) Large CS Classes with (too) Few Resources: An Experience Report</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1927.706" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/evaluating-group-work-in-too-large-cs-classes-with-too-few-resources-an-experience-report.mp3"/>
<itunes:duration>1927.706</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>256</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/evaluating-group-work-in-too-large-cs-classes-with-too-few-resources-an-experience-report.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Harms from Increasingly Agentic Algorithmic Systems</title>
<itunes:title>Harms from Increasingly Agentic Algorithmic Systems</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4091.37625" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3"/>
<itunes:duration>4091.37625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>257</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/harms-from-increasingly-agentic-algorithmic-systems.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models</title>
<itunes:title>INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2113.28" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ingenious-using-informative-data-subsets-for-efficient-pre-training-of-large-language-models.mp3"/>
<itunes:duration>2113.28</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>258</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ingenious-using-informative-data-subsets-for-efficient-pre-training-of-large-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Connecting levels of analysis in the computational era</title>
<itunes:title>Connecting levels of analysis in the computational era</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="908.22525" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/connecting-levels-of-analysis-in-the-computational-era.mp3"/>
<itunes:duration>908.22525</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>259</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/connecting-levels-of-analysis-in-the-computational-era.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Pretrain on just structure: Understanding linguistic inductive biases using transfer learning</title>
<itunes:title>Pretrain on just structure: Understanding linguistic inductive biases using transfer learning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2385.5805" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pretrain-on-just-structure-understanding-linguistic-inductive-biases-using-transfer-learning.mp3"/>
<itunes:duration>2385.5805</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>260</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pretrain-on-just-structure-understanding-linguistic-inductive-biases-using-transfer-learning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>NUCLEAR ARMS CONTROL VERIFICATION AND LESSONS FOR AI TREATIES</title>
<itunes:title>NUCLEAR ARMS CONTROL VERIFICATION AND LESSONS FOR AI TREATIES</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="9865.822" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nuclear-arms-control-verification-and-lessons-for-ai-treaties.mp3"/>
<itunes:duration>9865.822</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>261</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nuclear-arms-control-verification-and-lessons-for-ai-treaties.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</title>
<itunes:title>Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="4405.2375" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/implicit-bias-of-gradient-descent-for-logistic-regression-at-the-edge-of-stability.mp3"/>
<itunes:duration>4405.2375</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>262</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/implicit-bias-of-gradient-descent-for-logistic-regression-at-the-edge-of-stability.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>COEDIT: Text Editing by Task-Specific Instruction Tuning</title>
<itunes:title>COEDIT: Text Editing by Task-Specific Instruction Tuning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="3578.436" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/coedit-text-editing-by-task-specific-instruction-tuning.mp3"/>
<itunes:duration>3578.436</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>263</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/coedit-text-editing-by-task-specific-instruction-tuning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks</title>
<itunes:title>Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="1905.293" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/stop-uploading-test-data-in-plain-text-practical-strategies-for-mitigating-data-contamination-by-evaluation-benchmarks.mp3"/>
<itunes:duration>1905.293</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>264</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/stop-uploading-test-data-in-plain-text-practical-strategies-for-mitigating-data-contamination-by-evaluation-benchmarks.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability</title>
<itunes:title>Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in PaLM's Translation Capability</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2808.8425" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/searching-for-needles-in-a-haystack-on-the-role-of-incidental-bilingualism-in-palm-s-translation-capability.mp3"/>
<itunes:duration>2808.8425</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>265</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/searching-for-needles-in-a-haystack-on-the-role-of-incidental-bilingualism-in-palm-s-translation-capability.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>UOR: Universal Backdoor Attacks on Pre-trained Language Models</title>
<itunes:title>UOR: Universal Backdoor Attacks on Pre-trained Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2042.5665" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/uor-universal-backdoor-attacks-on-pre-trained-language-models.mp3"/>
<itunes:duration>2042.5665</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>266</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/uor-universal-backdoor-attacks-on-pre-trained-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>A Data Fusion Framework for Multi-Domain Morality Learning</title>
<itunes:title>A Data Fusion Framework for Multi-Domain Morality Learning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2332.63025" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-data-fusion-framework-for-multi-domain-morality-learning.mp3"/>
<itunes:duration>2332.63025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>267</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-data-fusion-framework-for-multi-domain-morality-learning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity</title>
<itunes:title>A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, &amp; Toxicity</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="5245.28325" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-pretrainer-s-guide-to-training-data-measuring-the-effects-of-data-age-domain-coverage-quality-toxicity.mp3"/>
<itunes:duration>5245.28325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>268</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-pretrainer-s-guide-to-training-data-measuring-the-effects-of-data-age-domain-coverage-quality-toxicity.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</title>
<itunes:title>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2872.294" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/megabyte-predicting-million-byte-sequences-with-multiscale-transformers.mp3"/>
<itunes:duration>2872.294</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>269</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/megabyte-predicting-million-byte-sequences-with-multiscale-transformers.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>A survey on datasets for fairness-aware machine learning</title>
<itunes:title>A survey on datasets for fairness-aware machine learning</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="6194.547" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-survey-on-datasets-for-fairness-aware-machine-learning.mp3"/>
<itunes:duration>6194.547</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>270</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/a-survey-on-datasets-for-fairness-aware-machine-learning.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>LM vs LM: Detecting Factual Errors via Cross Examination</title>
<itunes:title>LM vs LM: Detecting Factual Errors via Cross Examination</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2754.74275" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/lm-vs-lm-detecting-factual-errors-via-cross-examination.mp3"/>
<itunes:duration>2754.74275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>271</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/lm-vs-lm-detecting-factual-errors-via-cross-examination.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation</title>
<itunes:title>Connecting the Dots in Trustworthy Artificial Intelligence: From AI Principles, Ethics, and Key Requirements to Responsible AI Systems and Regulation</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="7791.72575" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/connecting-the-dots-in-trustworthy-artificial-intelligence-from-ai-principles-ethics-and-key-requirements-to-responsible-ai-systems-and-regulation.mp3"/>
<itunes:duration>7791.72575</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>272</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/connecting-the-dots-in-trustworthy-artificial-intelligence-from-ai-principles-ethics-and-key-requirements-to-responsible-ai-systems-and-regulation.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Prompt-based methods may underestimate large language models' linguistic generalizations</title>
<itunes:title>Prompt-based methods may underestimate large language models' linguistic generalizations</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2244.51925" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/prompt-based-methods-may-underestimate-large-language-models-linguistic-generalizations.mp3"/>
<itunes:duration>2244.51925</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>273</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/prompt-based-methods-may-underestimate-large-language-models-linguistic-generalizations.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models</title>
<itunes:title>INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2113.1755" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ingenious-using-informative-data-subsets-for-efficient-pre-training-of-large-language-models.mp3"/>
<itunes:duration>2113.1755</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>274</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ingenious-using-informative-data-subsets-for-efficient-pre-training-of-large-language-models.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>NUCLEAR ARMS CONTROL VERIFICATION AND LESSONS FOR AI TREATIES</title>
<itunes:title>NUCLEAR ARMS CONTROL VERIFICATION AND LESSONS FOR AI TREATIES</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="9897.11675" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nuclear-arms-control-verification-and-lessons-for-ai-treaties.mp3"/>
<itunes:duration>9897.11675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>275</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nuclear-arms-control-verification-and-lessons-for-ai-treaties.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</title>
<itunes:title>MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers</itunes:title>
<itunes:author/>
<itunes:subtitle/>
<itunes:summary/>
<description/>
<enclosure length="2530.142" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/megabyte-predicting-million-byte-sequences-with-multiscale-transformers.mp3"/>
<itunes:duration>2530.142</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>276</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/megabyte-predicting-million-byte-sequences-with-multiscale-transformers.mp3</guid>
<pubdate/>
<itunes:explicit>NO</itunes:explicit>
</item><item>
<title>Concealed Data Poisoning Attacks on NLP Models</title>
<itunes:title>Concealed Data Poisoning Attacks on NLP Models</itunes:title>
<itunes:author></itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary></itunes:summary>
<description></description>
<enclosure length="2052.1795" type="audio/mpeg" url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/concealed-data-poisoning-attacks-on-nlp-models.mp3"></enclosure>
<itunes:duration>2052.1795</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode>277</itunes:episode>
<itunes:episodetype>full</itunes:episodetype>
<guid ispermalink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/concealed-data-poisoning-attacks-on-nlp-models.mp3</guid>
<pubdate></pubdate>
<itunes:explicit>NO</itunes:explicit>
</item>
</channel>
</rss>
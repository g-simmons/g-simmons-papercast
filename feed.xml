<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
<channel>
<title>g-simmons-papercast</title>
<link>https://g-simmons.github.io/g-simmons-papercast/</link>
<language>en-us</language>
<atom:link href="https://g-simmons.github.io/g-simmons-papercast/feed.xml" rel="self" type="application/rss+xml"/>
<copyright>Rights to paper content are reserved by the authors for each paper. I make no claim to ownership or copyright of the content of this podcast.</copyright>
<itunes:subtitle></itunes:subtitle>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:summary>Podcast of text-to-speech for arbitrarily chosen NLP papers.</itunes:summary>
<itunes:keywords>Machine Learning, Natural Language Processing, Artificial Intelligence</itunes:keywords>
<description>Podcast of text-to-speech for arbitrarily chosen NLP papers.</description>
<itunes:owner><itunes:name>Gabriel Simmons</itunes:name><itunes:email>gsimmons@ucdavis.edu</itunes:email></itunes:owner>
<itunes:image href="https://g-simmons.github.io/g-simmons-papercast/cover.jpg"/>
<itunes:category text="Mathematics"></itunes:category>
<itunes:category text="Tech News"></itunes:category>
<itunes:category text="Courses"></itunes:category>


<item>
<title>Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference</title>
<itunes:title>Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area]]></itunes:summary>
<description><![CDATA[A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.01007v4.Right_for_the_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3551.6605</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.01007v4.Right_for_the_Wrong_Reasons_Diagnosing_Syntactic_Heuristics_in_Natural_Language_Inference.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
<itunes:title>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.]]></itunes:summary>
<description><![CDATA[Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2004.40175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Generalist Agent</title>
<itunes:title>A Generalist Agent</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.]]></itunes:summary>
<description><![CDATA[Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06175v2.A_Generalist_Agent.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5319.94125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06175v2.A_Generalist_Agent.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>StruBERT: Structure-aware BERT for Table Search and Matching</title>
<itunes:title>StruBERT: Structure-aware BERT for Table Search and Matching</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[A large amount of information is stored in data tables. Users can search for data tables using a keyword-based query. A table is composed primarily of data values that are organized in rows and columns providing implicit structural information. A table is usually accompanied by secondary information such as the caption, page title, etc., that form the textual information. Understanding the connection between the textual and structural information is an important yet neglected aspect in table retrieval as previous methods treat each source of information independently. In addition, users can search for data tables that are similar to an existing table, and this setting can be seen as a content-based table retrieval. In this paper, we propose StruBERT, a structure-aware BERT model that fuses the textual and structural information of a data table to produce context-aware representations for both textual and tabular content of a data table. StruBERT features are integrated in a new end-to-end neural ranking model to solve three table-related downstream tasks: keyword- and content-based table retrieval, and table similarity. We evaluate our approach using three datasets, and we demonstrate substantial improvements in terms of retrieval and classification metrics over state-of-the-art methods.]]></itunes:summary>
<description><![CDATA[A large amount of information is stored in data tables. Users can search for data tables using a keyword-based query. A table is composed primarily of data values that are organized in rows and columns providing implicit structural information. A table is usually accompanied by secondary information such as the caption, page title, etc., that form the textual information. Understanding the connection between the textual and structural information is an important yet neglected aspect in table retrieval as previous methods treat each source of information independently. In addition, users can search for data tables that are similar to an existing table, and this setting can be seen as a content-based table retrieval. In this paper, we propose StruBERT, a structure-aware BERT model that fuses the textual and structural information of a data table to produce context-aware representations for both textual and tabular content of a data table. StruBERT features are integrated in a new end-to-end neural ranking model to solve three table-related downstream tasks: keyword- and content-based table retrieval, and table similarity. We evaluate our approach using three datasets, and we demonstrate substantial improvements in terms of retrieval and classification metrics over state-of-the-art methods.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14278v1.StruBERT_Structure_aware_BERT_for_Table_Search_and_Matching.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2297.835</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14278v1.StruBERT_Structure_aware_BERT_for_Table_Search_and_Matching.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>On Calibration of Modern Neural Networks</title>
<itunes:title>On Calibration of Modern Neural Networks</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.]]></itunes:summary>
<description><![CDATA[Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.04599v2.On_Calibration_of_Modern_Neural_Networks.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2863.6995</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.04599v2.On_Calibration_of_Modern_Neural_Networks.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change</title>
<itunes:title>Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.]]></itunes:summary>
<description><![CDATA[Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1605.09096v6.Diachronic_Word_Embeddings_Reveal_Statistical_Laws_of_Semantic_Change.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2799.96075</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1605.09096v6.Diachronic_Word_Embeddings_Reveal_Statistical_Laws_of_Semantic_Change.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Incivility is Rising among American Politicians on Twitter</title>
<itunes:title>Incivility is Rising among American Politicians on Twitter</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Frimer et al., 2022_Incivility is Rising among American Politicians on Twitter.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6438.2955</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Frimer et al., 2022_Incivility is Rising among American Politicians on Twitter.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations</title>
<itunes:title>CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, humans have the ability to learn new concepts from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. CLUES consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop ExEnt, an entailment-based model that learns classifiers using explanations. ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future. Code and datasets are available at: https://clues-benchmark.github.io.]]></itunes:summary>
<description><![CDATA[Supervised learning has traditionally focused on inductive learning by observing labeled examples of a task. In contrast, humans have the ability to learn new concepts from language. Here, we explore training zero-shot classifiers for structured data purely from language. For this, we introduce CLUES, a benchmark for Classifier Learning Using natural language ExplanationS, consisting of a range of classification tasks over structured data along with natural language supervision in the form of explanations. CLUES consists of 36 real-world and 144 synthetic classification tasks. It contains crowdsourced explanations describing real-world tasks from multiple teachers and programmatically generated explanations for the synthetic tasks. To model the influence of explanations in classifying an example, we develop ExEnt, an entailment-based model that learns classifiers using explanations. ExEnt generalizes up to 18% better (relative) on novel tasks than a baseline that does not use explanations. We delineate key challenges for automated learning from explanations, addressing which can lead to progress on CLUES in the future. Code and datasets are available at: https://clues-benchmark.github.io.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.07142v1.CLUES_A_Benchmark_for_Learning_Classifiers_using_Natural_Language_Explanations.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3474.39025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.07142v1.CLUES_A_Benchmark_for_Learning_Classifiers_using_Natural_Language_Explanations.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</title>
<itunes:title>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.]]></itunes:summary>
<description><![CDATA[Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09712v1.Selection_Inference_Exploiting_Large_Language_Models_for_Interpretable_Logical_Reasoning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4674.4555</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09712v1.Selection_Inference_Exploiting_Large_Language_Models_for_Interpretable_Logical_Reasoning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Understanding Gradient Descent on Edge of Stability in Deep Learning</title>
<itunes:title>Understanding Gradient Descent on Edge of Stability in Deep Learning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Deep learning experiments in Cohen et al. (2021) using deterministic Gradient Descent (GD) revealed an {\em Edge of Stability (EoS)} phase when learning rate (LR) and sharpness (\emph{i.e.}, the largest eigenvalue of Hessian) no longer behave as in traditional optimization. Sharpness stabilizes around $2/$LR and loss goes up and down across iterations, yet still with an overall downward trend. The current paper mathematically analyzes a new mechanism of implicit regularization in the EoS phase, whereby GD updates due to non-smooth loss landscape turn out to evolve along some deterministic flow on the manifold of minimum loss. This is in contrast to many previous results about implicit bias either relying on infinitesimal updates or noise in gradient. Formally, for any smooth function $L$ with certain regularity condition, this effect is demonstrated for (1) {\em Normalized GD}, i.e., GD with a varying LR $ \eta_t =\frac{ \eta }{ || \nabla L(x(t)) || } $ and loss $L$; (2) GD with constant LR and loss $\sqrt{L}$. Both provably enter the Edge of Stability, with the associated flow on the manifold minimizing $\lambda_{\max}(\nabla^2 L)$. The above theoretical results have been corroborated by an experimental study.]]></itunes:summary>
<description><![CDATA[Deep learning experiments in Cohen et al. (2021) using deterministic Gradient Descent (GD) revealed an {\em Edge of Stability (EoS)} phase when learning rate (LR) and sharpness (\emph{i.e.}, the largest eigenvalue of Hessian) no longer behave as in traditional optimization. Sharpness stabilizes around $2/$LR and loss goes up and down across iterations, yet still with an overall downward trend. The current paper mathematically analyzes a new mechanism of implicit regularization in the EoS phase, whereby GD updates due to non-smooth loss landscape turn out to evolve along some deterministic flow on the manifold of minimum loss. This is in contrast to many previous results about implicit bias either relying on infinitesimal updates or noise in gradient. Formally, for any smooth function $L$ with certain regularity condition, this effect is demonstrated for (1) {\em Normalized GD}, i.e., GD with a varying LR $ \eta_t =\frac{ \eta }{ || \nabla L(x(t)) || } $ and loss $L$; (2) GD with constant LR and loss $\sqrt{L}$. Both provably enter the Edge of Stability, with the associated flow on the manifold minimizing $\lambda_{\max}(\nabla^2 L)$. The above theoretical results have been corroborated by an experimental study.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09745v1.Understanding_Gradient_Descent_on_Edge_of_Stability_in_Deep_Learning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>15057.8155</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09745v1.Understanding_Gradient_Descent_on_Edge_of_Stability_in_Deep_Learning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Symbolic Behaviour in Artificial Intelligence</title>
<itunes:title>Symbolic Behaviour in Artificial Intelligence</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.]]></itunes:summary>
<description><![CDATA[The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.03406v2.Symbolic_Behaviour_in_Artificial_Intelligence.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2589.28325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.03406v2.Symbolic_Behaviour_in_Artificial_Intelligence.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Structured Prediction as Translation between Augmented Natural Languages</title>
<itunes:title>Structured Prediction as Translation between Augmented Natural Languages</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.]]></itunes:summary>
<description><![CDATA[We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3945.35175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</title>
<itunes:title>RelationPrompt: Leveraging Prompts to Generate Synthetic Data for Zero-Shot Relation Triplet Extraction</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.]]></itunes:summary>
<description><![CDATA[Despite the importance of relation extraction in building and representing knowledge, less research is focused on generalizing to unseen relations types. We introduce the task setting of Zero-Shot Relation Triplet Extraction (ZeroRTE) to encourage further research in low-resource relation extraction methods. Given an input sentence, each extracted triplet consists of the head entity, relation label, and tail entity where the relation label is not seen at the training stage. To solve ZeroRTE, we propose to synthesize relation examples by prompting language models to generate structured texts. Concretely, we unify language model prompts and structured text approaches to design a structured prompt template for generating synthetic relation samples when conditioning on relation label prompts (RelationPrompt). To overcome the limitation for extracting multiple relation triplets in a sentence, we design a novel Triplet Search Decoding method. Experiments on FewRel and Wiki-ZSL datasets show the efficacy of RelationPrompt for the ZeroRTE task and zero-shot relation classification. Our code and data are available at github.com/declare-lab/RelationPrompt.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09101v1.RelationPrompt_Leveraging_Prompts_to_Generate_Synthetic_Data_for_Zero_Shot_Relation_Triplet_Extraction.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2930.18125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09101v1.RelationPrompt_Leveraging_Prompts_to_Generate_Synthetic_Data_for_Zero_Shot_Relation_Triplet_Extraction.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation</title>
<itunes:title>Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.]]></itunes:summary>
<description><![CDATA[Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.03132v1.Greedy_Search_with_Probabilistic_N_gram_Matching_for_Neural_Machine_Translation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1153.20175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1809.03132v1.Greedy_Search_with_Probabilistic_N_gram_Matching_for_Neural_Machine_Translation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</title>
<itunes:title>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.]]></itunes:summary>
<description><![CDATA[This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1327.72575</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers</title>
<itunes:title>Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that specific distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language -- burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data influenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model -- a skewed, Zipfian distribution over classes -- which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own.]]></itunes:summary>
<description><![CDATA[Large transformer-based language models are able to perform few-shot learning (also known as in-context learning), without having been explicitly trained for it. We hypothesized that specific distributional properties of natural language might drive this emergent phenomenon, as these characteristics might lead to a kind of interpolation between few-shot meta-training (designed to elicit rapid few-shot learning) and standard supervised training (designed to elicit gradual in-weights learning). We also hypothesized that these distributional properties could lead to emergent few-shot learning in domains outside of language. Inspired by this idea, we ran a series of experiments on a standard image-based few-shot dataset. We discovered that a number of data properties did indeed promote the emergence of few-shot learning in transformer models. All of these properties are present in natural language -- burstiness, long-tailedness, and many-to-one or one-to-many label mappings. The data influenced whether models were biased towards either few-shot learning vs. memorizing information in their weights; models could generally perform well at only one or the other. However, we discovered that an additional distributional property could allow the two capabilities to co-exist in the same model -- a skewed, Zipfian distribution over classes -- which occurs in language as well. Notably, training data that could elicit few-shot learning in transformers were unable to elicit few-shot learning in recurrent models. In sum, we find that few-shot learning emerges only from applying the right architecture to the right data distribution; neither component is sufficient on its own.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.05055v2.Data_Distributional_Properties_Drive_Emergent_Few_Shot_Learning_in_Transformers.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2260.00975</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.05055v2.Data_Distributional_Properties_Drive_Emergent_Few_Shot_Learning_in_Transformers.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy</title>
<itunes:title>Does Moral Code Have a Moral Code? Probing Delphi's Moral Philosophy</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -- and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.]]></itunes:summary>
<description><![CDATA[In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets -- and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.12771v1.Does_Moral_Code_Have_a_Moral_Code_Probing_Delphi_s_Moral_Philosophy.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3818.29225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.12771v1.Does_Moral_Code_Have_a_Moral_Code_Probing_Delphi_s_Moral_Philosophy.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>STaR: Bootstrapping Reasoning With Reasoning</title>
<itunes:title>STaR: Bootstrapping Reasoning With Reasoning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.]]></itunes:summary>
<description><![CDATA[Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14465v1.STaR_Bootstrapping_Reasoning_With_Reasoning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3674.40975</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.14465v1.STaR_Bootstrapping_Reasoning_With_Reasoning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The whistleblower's dilemma and the fairness-loyalty tradeoff</title>
<itunes:title>The whistleblower's dilemma and the fairness-loyalty tradeoff</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[• The tradeoff between fairness and loyalty corresponds to whistleblowing decisions. • Experimental and dispositional variation in this tradeoff maps onto whistleblowing. • Five studies demonstrate this previously undocumented relationship. • These results shed light on a novel psychological determinant of whistleblowing. a b s t r a c t a r t i c l e i n f o Whistleblowing -reporting another person's unethical behavior to a third party -often constitutes a conflict between competing moral concerns. Whistleblowing promotes justice and fairness but can also appear disloyal. Five studies demonstrate that a fairness-loyalty tradeoff predicts people's willingness to blow the whistle. Study 1 demonstrates that individual differences in valuing fairness over loyalty predict willingness to report unethical behavior. Studies 2a and 2b demonstrate that experimentally manipulating endorsement of fairness versus loyalty increases willingness to report unethical behavior. Study 3 demonstrates that people recall their decisions to report unethical behavior as driven by valuation of fairness, whereas people recall decisions not to report unethical behavior as driven by valuation of loyalty. Study 4 demonstrates that experimentally manipulating the endorsement of fairness versus loyalty increases whistleblowing in an online marketplace. These findings reveal the psychological determinants of whistleblowing and shed light on factors that encourage or discourage this practice.]]></itunes:summary>
<description><![CDATA[• The tradeoff between fairness and loyalty corresponds to whistleblowing decisions. • Experimental and dispositional variation in this tradeoff maps onto whistleblowing. • Five studies demonstrate this previously undocumented relationship. • These results shed light on a novel psychological determinant of whistleblowing. a b s t r a c t a r t i c l e i n f o Whistleblowing -reporting another person's unethical behavior to a third party -often constitutes a conflict between competing moral concerns. Whistleblowing promotes justice and fairness but can also appear disloyal. Five studies demonstrate that a fairness-loyalty tradeoff predicts people's willingness to blow the whistle. Study 1 demonstrates that individual differences in valuing fairness over loyalty predict willingness to report unethical behavior. Studies 2a and 2b demonstrate that experimentally manipulating endorsement of fairness versus loyalty increases willingness to report unethical behavior. Study 3 demonstrates that people recall their decisions to report unethical behavior as driven by valuation of fairness, whereas people recall decisions not to report unethical behavior as driven by valuation of loyalty. Study 4 demonstrates that experimentally manipulating the endorsement of fairness versus loyalty increases whistleblowing in an online marketplace. These findings reveal the psychological determinants of whistleblowing and shed light on factors that encourage or discourage this practice.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0022103113001352-main.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2721.01875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0022103113001352-main.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Breaking Up is (Relatively) Easy to Do: A Script for the Dissolution of Close Relationships</title>
<itunes:title>Breaking Up is (Relatively) Easy to Do: A Script for the Dissolution of Close Relationships</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/0265407598156007.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6.45225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/0265407598156007.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Probing Pre-Trained Language Models for Cross-Cultural Differences in Values</title>
<itunes:title>Probing Pre-Trained Language Models for Cross-Cultural Differences in Values</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.]]></itunes:summary>
<description><![CDATA[Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13722v1.Probing_Pre_Trained_Language_Models_for_Cross_Cultural_Differences_in_Values.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1589.76</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13722v1.Probing_Pre_Trained_Language_Models_for_Cross_Cultural_Differences_in_Values.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets</title>
<itunes:title>Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.]]></itunes:summary>
<description><![CDATA[Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12942v1.Generating_Data_to_Mitigate_Spurious_Correlations_in_Natural_Language_Inference_Datasets.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2826.39675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12942v1.Generating_Data_to_Mitigate_Spurious_Correlations_in_Natural_Language_Inference_Datasets.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Multiple moral foundations predict responses to sacrificial dilemmas</title>
<itunes:title>Multiple moral foundations predict responses to sacrificial dilemmas</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Moral dilemmas, by definition, demand trade-offs between competing moral goods (e.g., causing one harm to prevent another). Although moral dilemmas have served as a methodological pillar for moral psychology, surprisingly little research has explored how individual differences in moral values influence responses to dilemmatic trade-offs. In a cross-sectional study (N = 307), we tested competing claims regarding the relationship between the endorsement of foundational moral values and responses to sacrificial dilemmas, in which one judges the moral acceptability of causing fatal harm to one person to save multiple others. Inconsistent with Moral Dyad Theory, our results did not support the prediction that Harm concerns would be the unequivocally most important predictor of sacrifice endorsement. Consistent with Moral Foundations Theory, however, multiple moral values are predictive of sacrifice judgments: Harm and Purity negatively predict, and Ingroup positively predicts, endorsement of harmful action in service of saving lives, with Harm and Purity explaining similar amounts of unique variance. The present study demonstrates the utility of pluralistic accounts of morality, even in moral situations in which harm is central. Crown]]></itunes:summary>
<description><![CDATA[Moral dilemmas, by definition, demand trade-offs between competing moral goods (e.g., causing one harm to prevent another). Although moral dilemmas have served as a methodological pillar for moral psychology, surprisingly little research has explored how individual differences in moral values influence responses to dilemmatic trade-offs. In a cross-sectional study (N = 307), we tested competing claims regarding the relationship between the endorsement of foundational moral values and responses to sacrificial dilemmas, in which one judges the moral acceptability of causing fatal harm to one person to save multiple others. Inconsistent with Moral Dyad Theory, our results did not support the prediction that Harm concerns would be the unequivocally most important predictor of sacrifice endorsement. Consistent with Moral Foundations Theory, however, multiple moral values are predictive of sacrifice judgments: Harm and Purity negatively predict, and Ingroup positively predicts, endorsement of harmful action in service of saving lives, with Harm and Purity explaining similar amounts of unique variance. The present study demonstrates the utility of pluralistic accounts of morality, even in moral situations in which harm is central. Crown]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886915003049-main.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2000.90125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886915003049-main.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>B-cos Networks: Alignment is All We Need for Interpretability</title>
<itunes:title>B-cos Networks: Alignment is All We Need for Interpretability</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at https://www.github.com/moboehle/B-cos.]]></itunes:summary>
<description><![CDATA[We present a new direction for increasing the interpretability of deep neural networks (DNNs) by promoting weight-input alignment during training. For this, we propose to replace the linear transforms in DNNs by our B-cos transform. As we show, a sequence (network) of such transforms induces a single linear transform that faithfully summarises the full model computations. Moreover, the B-cos transform introduces alignment pressure on the weights during optimisation. As a result, those induced linear transforms become highly interpretable and align with task-relevant features. Importantly, the B-cos transform is designed to be compatible with existing architectures and we show that it can easily be integrated into common models such as VGGs, ResNets, InceptionNets, and DenseNets, whilst maintaining similar performance on ImageNet. The resulting explanations are of high visual quality and perform well under quantitative metrics for interpretability. Code available at https://www.github.com/moboehle/B-cos.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10268v1.B_cos_Networks_Alignment_is_All_We_Need_for_Interpretability.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3974.191</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10268v1.B_cos_Networks_Alignment_is_All_We_Need_for_Interpretability.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors</title>
<itunes:title>Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.]]></itunes:summary>
<description><![CDATA[Deep learning is increasingly moving towards a transfer learning paradigm whereby large foundation models are fine-tuned on downstream tasks, starting from an initialization learned on the source task. But an initialization contains relatively little information about the source task. Instead, we show that we can learn highly informative posteriors from the source task, through supervised or self-supervised approaches, which then serve as the basis for priors that modify the whole loss surface on the downstream task. This simple modular approach enables significant performance gains and more data-efficient learning on a variety of downstream classification and segmentation tasks, serving as a drop-in replacement for standard pre-training strategies. These highly informative priors also can be saved for future use, similar to pre-trained weights, and stand in contrast to the zero-mean isotropic uninformative priors that are typically used in Bayesian deep learning.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10279v1.Pre_Train_Your_Loss_Easy_Bayesian_Transfer_Learning_with_Informative_Priors.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2752.47025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10279v1.Pre_Train_Your_Loss_Easy_Bayesian_Transfer_Learning_with_Informative_Priors.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization</title>
<itunes:title>Semi-supervised Formality Style Transfer using Language Model Discriminator and Mutual Information Maximization</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.]]></itunes:summary>
<description><![CDATA[Formality style transfer is the task of converting informal sentences to grammatically-correct formal sentences, which can be used to improve performance of many downstream NLP tasks. In this work, we propose a semi-supervised formality style transfer model that utilizes a language model-based discriminator to maximize the likelihood of the output sentence being formal, which allows us to use maximization of token-level conditional probabilities for training. We further propose to maximize mutual information between source and target styles as our training objective instead of maximizing the regular likelihood that often leads to repetitive and trivial generated responses. Experiments showed that our model outperformed previous state-of-the-art baselines significantly in terms of both automated metrics and human judgement. We further generalized our model to unsupervised text style transfer task, and achieved significant improvements on two benchmark sentiment style transfer datasets.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.05090v1.Semi_supervised_Formality_Style_Transfer_using_Language_Model_Discriminator_and_Mutual_Information_Maximization.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2673.11025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.05090v1.Semi_supervised_Formality_Style_Transfer_using_Language_Model_Discriminator_and_Mutual_Information_Maximization.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures</title>
<itunes:title>The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Although numerous models attempted to explain the nature of moral judgment, moral foundations theory (MFT) led to a paradigmatic change in this field by proposing pluralist "moralities" (care, fairness, loyalty, authority, sanctity). The five-factor structure of MFT is thought to be universal and rooted in the evolutionary past but the evidence is scarce regarding the stability of this five-factor structure across diverse cultures. We tested this universality argument in a cross-cultural dataset of 30 diverse societies spanning the WEIRD (Western, educated, industrialized, rich, democratic) and non-WEIRD cultures by testing measurement invariance of the short-form of the moral foundations questionnaire. The results supported the original conceptualization that there are at least five diverse moralities although loadings of items differ across WEIRD and non-WEIRD cultures. In other words, the current research shows for the first time that the five-factor structure of MFT is stable in the WEIRD and non-WEIRD cultures.]]></itunes:summary>
<description><![CDATA[Although numerous models attempted to explain the nature of moral judgment, moral foundations theory (MFT) led to a paradigmatic change in this field by proposing pluralist "moralities" (care, fairness, loyalty, authority, sanctity). The five-factor structure of MFT is thought to be universal and rooted in the evolutionary past but the evidence is scarce regarding the stability of this five-factor structure across diverse cultures. We tested this universality argument in a cross-cultural dataset of 30 diverse societies spanning the WEIRD (Western, educated, industrialized, rich, democratic) and non-WEIRD cultures by testing measurement invariance of the short-form of the moral foundations questionnaire. The results supported the original conceptualization that there are at least five diverse moralities although loadings of items differ across WEIRD and non-WEIRD cultures. In other words, the current research shows for the first time that the five-factor structure of MFT is stable in the WEIRD and non-WEIRD cultures.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886919304799-main.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1921.12325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0191886919304799-main.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Belief-based Generation of Argumentative Claims</title>
<itunes:title>Belief-based Generation of Argumentative Claims</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[When engaging in argumentative discourse, skilled human debaters tailor claims to the beliefs of the audience, to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we additionally evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances, but demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach.]]></itunes:summary>
<description><![CDATA[When engaging in argumentative discourse, skilled human debaters tailor claims to the beliefs of the audience, to construct effective arguments. Recently, the field of computational argumentation witnessed extensive effort to address the automatic generation of arguments. However, existing approaches do not perform any audience-specific adaptation. In this work, we aim to bridge this gap by studying the task of belief-based claim generation: Given a controversial topic and a set of beliefs, generate an argumentative claim tailored to the beliefs. To tackle this task, we model the people's prior beliefs through their stances on controversial topics and extend state-of-the-art text generation models to generate claims conditioned on the beliefs. Our automatic evaluation confirms the ability of our approach to adapt claims to a set of given beliefs. In a manual study, we additionally evaluate the generated claims in terms of informativeness and their likelihood to be uttered by someone with a respective belief. Our results reveal the limitations of modeling users' beliefs based on their stances, but demonstrate the potential of encoding beliefs into argumentative texts, laying the ground for future exploration of audience reach.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.09765v2.Belief_based_Generation_of_Argumentative_Claims.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2260.63675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.09765v2.Belief_based_Generation_of_Argumentative_Claims.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation in RICE-N</title>
<itunes:title>AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation in RICE-N</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Comprehensive global cooperation is essential to limit global temperature increases while continuing economic development, e.g., reducing severe inequality or achieving long-term economic growth. Achieving long-term cooperation on climate change mitigation with n strategic agents poses a complex game-theoretic problem. For example, agents may negotiate and reach climate agreements, but there is no central authority to enforce adherence to those agreements. Hence, it is critical to design negotiation and agreement frameworks that foster cooperation, allow all agents to meet their individual policy objectives, and incentivize long-term adherence. This is an interdisciplinary challenge that calls for collaboration between researchers in machine learning, economics, climate science, law, policy, ethics, and other fields. In particular, we argue that machine learning is a critical tool to address the complexity of this domain. To facilitate this research, here we introduce RICE-N, a multi-region integrated assessment model that simulates the global climate and economy, and which can be used to design and evaluate the strategic outcomes for different negotiation and agreement frameworks. We also describe how to use multi-agent reinforcement learning to train rational agents using RICE-N. This framework underpinsAI for Global Climate Cooperation, a working group collaboration and competition on climate negotiation and agreement design. Here, we invite the scientific community to design and evaluate their solutions using RICE-N, machine learning, economic intuition, and other domain knowledge. More information can be found on www.ai4climatecoop.org.]]></itunes:summary>
<description><![CDATA[Comprehensive global cooperation is essential to limit global temperature increases while continuing economic development, e.g., reducing severe inequality or achieving long-term economic growth. Achieving long-term cooperation on climate change mitigation with n strategic agents poses a complex game-theoretic problem. For example, agents may negotiate and reach climate agreements, but there is no central authority to enforce adherence to those agreements. Hence, it is critical to design negotiation and agreement frameworks that foster cooperation, allow all agents to meet their individual policy objectives, and incentivize long-term adherence. This is an interdisciplinary challenge that calls for collaboration between researchers in machine learning, economics, climate science, law, policy, ethics, and other fields. In particular, we argue that machine learning is a critical tool to address the complexity of this domain. To facilitate this research, here we introduce RICE-N, a multi-region integrated assessment model that simulates the global climate and economy, and which can be used to design and evaluate the strategic outcomes for different negotiation and agreement frameworks. We also describe how to use multi-agent reinforcement learning to train rational agents using RICE-N. This framework underpinsAI for Global Climate Cooperation, a working group collaboration and competition on climate negotiation and agreement design. Here, we invite the scientific community to design and evaluate their solutions using RICE-N, machine learning, economic intuition, and other domain knowledge. More information can be found on www.ai4climatecoop.org.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.07004v1.AI_for_Global_Climate_Cooperation_Modeling_Global_Climate_Negotiations_Agreements_and_Long_Term_Cooperation_in_RICE_N.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5695.84325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.07004v1.AI_for_Global_Climate_Cooperation_Modeling_Global_Climate_Negotiations_Agreements_and_Long_Term_Cooperation_in_RICE_N.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation</title>
<itunes:title>BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.]]></itunes:summary>
<description><![CDATA[Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.11718v1.BOLD_Dataset_and_Metrics_for_Measuring_Biases_in_Open_Ended_Language_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3449.86125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.11718v1.BOLD_Dataset_and_Metrics_for_Measuring_Biases_in_Open_Ended_Language_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Fast and Scalable Structural SVM with Slack Rescaling</title>
<itunes:title>Fast and Scalable Structural SVM with Slack Rescaling</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.]]></itunes:summary>
<description><![CDATA[We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1173.133</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>College and the “Culture War”: Assessing Higher Education’s Influence on Moral Attitudes</title>
<itunes:title>College and the “Culture War”: Assessing Higher Education’s Influence on Moral Attitudes</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Moral differences contribute to social and political conflicts. Against this backdrop, colleges and universities have been criticized for promoting liberal moral attitudes. However, direct evidence for these claims is sparse, and suggestive evidence from studies of political attitudes is inconclusive. Using four waves of data from the National Study of Youth and Religion, we examine the effects of higher education on attitudes related to three dimensions of morality that have been identified as central to conflict: moral relativism, concern for others, and concern for social order. Our results indicate that higher education liberalizes moral concerns for most students, but it also departs from the standard liberal profile by promoting moral absolutism rather than relativism. These effects are strongest for individuals majoring in the humanities, arts, or social sciences, and for students pursuing graduate studies. We conclude with a discussion of the implications of our results for work on political conflict and moral socialization.]]></itunes:summary>
<description><![CDATA[Moral differences contribute to social and political conflicts. Against this backdrop, colleges and universities have been criticized for promoting liberal moral attitudes. However, direct evidence for these claims is sparse, and suggestive evidence from studies of political attitudes is inconclusive. Using four waves of data from the National Study of Youth and Religion, we examine the effects of higher education on attitudes related to three dimensions of morality that have been identified as central to conflict: moral relativism, concern for others, and concern for social order. Our results indicate that higher education liberalizes moral concerns for most students, but it also departs from the standard liberal profile by promoting moral absolutism rather than relativism. These effects are strongest for individuals majoring in the humanities, arts, or social sciences, and for students pursuing graduate studies. We conclude with a discussion of the implications of our results for work on political conflict and moral socialization.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/00031224211041094.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6928.7705</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/00031224211041094.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>MetaFormer is Actually What You Need for Vision</title>
<itunes:title>MetaFormer is Actually What You Need for Vision</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer]]></itunes:summary>
<description><![CDATA[Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2111.11418v2.MetaFormer_is_Actually_What_You_Need_for_Vision.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2160.06525</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2111.11418v2.MetaFormer_is_Actually_What_You_Need_for_Vision.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Five sources of bias in natural language processing</title>
<itunes:title>Five sources of bias in natural language processing</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.]]></itunes:summary>
<description><![CDATA[Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Language and Linguist Compass - 2021 - Hovy - Five sources of bias in natural language processing.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2844.21225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Language and Linguist Compass - 2021 - Hovy - Five sources of bias in natural language processing.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>OPT: Open Pre-trained Transformer Language Models</title>
<itunes:title>OPT: Open Pre-trained Transformer Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.]]></itunes:summary>
<description><![CDATA[Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.01068v3.OPT_Open_Pre_trained_Transformer_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3418.9845</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.01068v3.OPT_Open_Pre_trained_Transformer_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem</title>
<itunes:title>Detection, Disambiguation, Re-ranking: Autoregressive Entity Linking as a Multi-Task Problem</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.]]></itunes:summary>
<description><![CDATA[We propose an autoregressive entity linking model, that is trained with two auxiliary tasks, and learns to re-rank generated samples at inference time. Our proposed novelties address two weaknesses in the literature. First, a recent method proposes to learn mention detection and then entity candidate selection, but relies on predefined sets of candidates. We use encoder-decoder autoregressive entity linking in order to bypass this need, and propose to train mention detection as an auxiliary task instead. Second, previous work suggests that re-ranking could help correct prediction errors. We add a new, auxiliary task, match prediction, to learn re-ranking. Without the use of a knowledge base or candidate sets, our model sets a new state of the art in two benchmark datasets of entity linking: COMETA in the biomedical domain, and AIDA-CoNLL in the news domain. We show through ablation studies that each of the two auxiliary tasks increases performance, and that re-ranking is an important factor to the increase. Finally, our low-resource experimental results suggest that performance on the main task benefits from the knowledge learned by the auxiliary tasks, and not just from the additional training data.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05990v1.Detection_Disambiguation_Re_ranking_Autoregressive_Entity_Linking_as_a_Multi_Task_Problem.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2432.05225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05990v1.Detection_Disambiguation_Re_ranking_Autoregressive_Entity_Linking_as_a_Multi_Task_Problem.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>What Makes Data-to-Text Generation Hard for Pretrained Language Models?</title>
<itunes:title>What Makes Data-to-Text Generation Hard for Pretrained Language Models?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Expressing natural language descriptions of structured facts or relations -- data-to-text generation (D2T) -- increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models(PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T.   In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how these data are incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.]]></itunes:summary>
<description><![CDATA[Expressing natural language descriptions of structured facts or relations -- data-to-text generation (D2T) -- increases the accessibility of structured knowledge repositories. Previous work shows that pre-trained language models(PLMs) perform remarkably well on this task after fine-tuning on a significant amount of task-specific training data. On the other hand, while auto-regressive PLMs can generalize from a few task examples, their efficacy at D2T is largely unexplored. Furthermore, we have an incomplete understanding of the limits of PLMs on D2T.   In this work, we conduct an empirical study of both fine-tuned and auto-regressive PLMs on the DART multi-domain D2T dataset. We consider their performance as a function of the amount of task-specific data and how these data are incorporated into the models: zero and few-shot learning, and fine-tuning of model weights. In addition, we probe the limits of PLMs by measuring performance on subsets of the evaluation data: novel predicates and abstractive test examples. To improve the performance on these subsets, we investigate two techniques: providing predicate descriptions in the context and re-ranking generated candidates by information reflected in the source. Finally, we conduct a human evaluation of model errors and show that D2T generation tasks would benefit from datasets with more careful manual curation.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.11505v1.What_Makes_Data_to_Text_Generation_Hard_for_Pretrained_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2669.871</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.11505v1.What_Makes_Data_to_Text_Generation_Hard_for_Pretrained_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities</title>
<itunes:title>A Comprehensive Survey of Few-shot Learning: Evolution, Applications, Challenges, and Opportunities</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples still remains a serious challenge. In this context, we extensively investigated 200+ latest papers on FSL published in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL along with impartial comparisons of the strengths and weaknesses of the existing works. For the sake of avoiding conceptual confusion, we first elaborate and compare a set of similar concepts including few-shot learning, transfer learning, and meta-learning. Furthermore, we propose a novel taxonomy to classify the existing work according to the level of abstraction of knowledge in accordance with the challenges of FSL. To enrich this survey, in each subsection we provide in-depth analysis and insightful discussion about recent advances on these topics. Moreover, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into the technology evolution trends together with potential future research opportunities in the hope of providing guidance to follow-up research.]]></itunes:summary>
<description><![CDATA[Few-shot learning (FSL) has emerged as an effective learning method and shows great potential. Despite the recent creative works in tackling FSL tasks, learning valid information rapidly from just a few or even zero samples still remains a serious challenge. In this context, we extensively investigated 200+ latest papers on FSL published in the past three years, aiming to present a timely and comprehensive overview of the most recent advances in FSL along with impartial comparisons of the strengths and weaknesses of the existing works. For the sake of avoiding conceptual confusion, we first elaborate and compare a set of similar concepts including few-shot learning, transfer learning, and meta-learning. Furthermore, we propose a novel taxonomy to classify the existing work according to the level of abstraction of knowledge in accordance with the challenges of FSL. To enrich this survey, in each subsection we provide in-depth analysis and insightful discussion about recent advances on these topics. Moreover, taking computer vision as an example, we highlight the important application of FSL, covering various research hotspots. Finally, we conclude the survey with unique insights into the technology evolution trends together with potential future research opportunities in the hope of providing guidance to follow-up research.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06743v1.A_Comprehensive_Survey_of_Few_shot_Learning_Evolution_Applications_Challenges_and_Opportunities.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5407.6865</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.06743v1.A_Comprehensive_Survey_of_Few_shot_Learning_Evolution_Applications_Challenges_and_Opportunities.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Doc2Dict: Information Extraction as Text Generation</title>
<itunes:title>Doc2Dict: Information Extraction as Text Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2235.2195</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims</title>
<itunes:title>Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.]]></itunes:summary>
<description><![CDATA[With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2004.07213v2.Toward_Trustworthy_AI_Development_Mechanisms_for_Supporting_Verifiable_Claims.mp3" length="" type="audio/mpeg"/>
<itunes:duration>10216.986</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2004.07213v2.Toward_Trustworthy_AI_Development_Mechanisms_for_Supporting_Verifiable_Claims.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do</title>
<itunes:title>Large Pre-trained Language Models Contain Human-like Biases of What is Right and Wrong to Do</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended state of the art for many NLP tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show that recent LMs also contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society -- they bring a "moral direction" to surface. That is, we show that these norms can be captured geometrically by a direction, which can be computed, e.g., by a PCA, in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts and providing a path for attenuating or even preventing toxic degeneration in LMs. Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.]]></itunes:summary>
<description><![CDATA[Artificial writing is permeating our lives due to recent advances in large-scale, transformer-based language models (LMs) such as BERT, its variants, GPT-2/3, and others. Using them as pre-trained models and fine-tuning them for specific tasks, researchers have extended state of the art for many NLP tasks and shown that they capture not only linguistic knowledge but also retain general knowledge implicitly present in the data. Unfortunately, LMs trained on unfiltered text corpora suffer from degenerated and biased behaviour. While this is well established, we show that recent LMs also contain human-like biases of what is right and wrong to do, some form of ethical and moral norms of the society -- they bring a "moral direction" to surface. That is, we show that these norms can be captured geometrically by a direction, which can be computed, e.g., by a PCA, in the embedding space, reflecting well the agreement of phrases to social norms implicitly expressed in the training texts and providing a path for attenuating or even preventing toxic degeneration in LMs. Being able to rate the (non-)normativity of arbitrary phrases without explicitly training the LM for this task, we demonstrate the capabilities of the "moral direction" for guiding (even other) LMs towards producing normative text and showcase it on RealToxicityPrompts testbed, preventing the neural toxic degeneration in GPT-2.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11790v3.Large_Pre_trained_Language_Models_Contain_Human_like_Biases_of_What_is_Right_and_Wrong_to_Do.mp3" length="" type="audio/mpeg"/>
<itunes:duration>8467.38275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.11790v3.Large_Pre_trained_Language_Models_Contain_Human_like_Biases_of_What_is_Right_and_Wrong_to_Do.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Article 34 Citation: Card D and Smith NA (2020) On Consequentialism and Fairness. Front</title>
<itunes:title>Article 34 Citation: Card D and Smith NA (2020) On Consequentialism and Fairness. Front</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage "fair" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.]]></itunes:summary>
<description><![CDATA[Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage "fair" outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions (because of the combined problems of uncertainty, subjectivity, and aggregation), it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frai-03-00034.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3851.25875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/frai-03-00034.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Non-Parametric Class Completeness Estimators for Collaborative Knowledge Graphs -- The Case of Wikidata</title>
<itunes:title>Non-Parametric Class Completeness Estimators for Collaborative Knowledge Graphs -- The Case of Wikidata</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Collaborative Knowledge Graph platforms allow humans and automated scripts to collaborate in creating, updating and interlinking entities and facts. To ensure both the completeness of the data as well as a uniform coverage of the different topics, it is crucial to identify underrepresented classes in the Knowledge Graph. In this paper, we tackle this problem by developing statistical techniques for class cardinality estimation in collaborative Knowledge Graph platforms. Our method is able to estimate the completeness of a class - as defined by a schema or ontology - hence can be used to answer questions such as "Does the knowledge base have a complete list of all {Beer Brands|Volcanos|Video Game Consoles}?" As a use-case, we focus on Wikidata, which poses unique challenges in terms of the size of its ontology, the number of users actively populating its graph, and its extremely dynamic nature. Our techniques are derived from species estimation and data-management methodologies, and are applied to the case of graphs and collaborative editing. In our empirical evaluation, we observe that i) the number and frequency of unique class instances drastically influence the performance of an estimator, ii) bursts of inserts cause some estimators to overestimate the true size of the class if they are not properly handled, and iii) one can effectively measure the convergence of a class towards its true size by considering the stability of an estimator against the number of available instances.]]></itunes:summary>
<description><![CDATA[Collaborative Knowledge Graph platforms allow humans and automated scripts to collaborate in creating, updating and interlinking entities and facts. To ensure both the completeness of the data as well as a uniform coverage of the different topics, it is crucial to identify underrepresented classes in the Knowledge Graph. In this paper, we tackle this problem by developing statistical techniques for class cardinality estimation in collaborative Knowledge Graph platforms. Our method is able to estimate the completeness of a class - as defined by a schema or ontology - hence can be used to answer questions such as "Does the knowledge base have a complete list of all {Beer Brands|Volcanos|Video Game Consoles}?" As a use-case, we focus on Wikidata, which poses unique challenges in terms of the size of its ontology, the number of users actively populating its graph, and its extremely dynamic nature. Our techniques are derived from species estimation and data-management methodologies, and are applied to the case of graphs and collaborative editing. In our empirical evaluation, we observe that i) the number and frequency of unique class instances drastically influence the performance of an estimator, ii) bursts of inserts cause some estimators to overestimate the true size of the class if they are not properly handled, and iii) one can effectively measure the convergence of a class towards its true size by considering the stability of an estimator against the number of available instances.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1909.01109v1.Non_Parametric_Class_Completeness_Estimators_for_Collaborative_Knowledge_Graphs_The_Case_of_Wikidata.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2411.46775</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1909.01109v1.Non_Parametric_Class_Completeness_Estimators_for_Collaborative_Knowledge_Graphs_The_Case_of_Wikidata.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Language of Liberty: A preliminary study</title>
<itunes:title>The Language of Liberty: A preliminary study</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Quantifying the moral narratives expressed in the user-generated text, news, or public discourses is fundamental for understanding individuals' concerns and viewpoints and preventing violent protests and social polarisation. The Moral Foundation Theory (MFT) was developed precisely to operationalise morality in a five-dimensional scale system. Recent developments of the theory urged for the introduction of a new foundation, liberty. Being only recently added to the theory, there are no available linguistic resources to assess liberty from text corpora. Given its importance to current social issues such as the vaccination debate, we propose a data-driven approach to derive a liberty lexicon based on aligned documents from online encyclopedias with different worldviews. Despite the preliminary nature of our study, we show proof of the concept that large encyclopedia corpora can point out differences in the way people with contrasting viewpoints express themselves. Such differences can be used to derive a novel lexicon, identifying linguistic markers of the liberty foundation.]]></itunes:summary>
<description><![CDATA[Quantifying the moral narratives expressed in the user-generated text, news, or public discourses is fundamental for understanding individuals' concerns and viewpoints and preventing violent protests and social polarisation. The Moral Foundation Theory (MFT) was developed precisely to operationalise morality in a five-dimensional scale system. Recent developments of the theory urged for the introduction of a new foundation, liberty. Being only recently added to the theory, there are no available linguistic resources to assess liberty from text corpora. Given its importance to current social issues such as the vaccination debate, we propose a data-driven approach to derive a liberty lexicon based on aligned documents from online encyclopedias with different worldviews. Despite the preliminary nature of our study, we show proof of the concept that large encyclopedia corpora can point out differences in the way people with contrasting viewpoints express themselves. Such differences can be used to derive a novel lexicon, identifying linguistic markers of the liberty foundation.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/The_Language_of_Liberty (1).mp3" length="" type="audio/mpeg"/>
<itunes:duration>895.765</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/The_Language_of_Liberty (1).mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Highly Parallel Autoregressive Entity Linking with Discriminative Correction</title>
<itunes:title>Highly Parallel Autoregressive Entity Linking with Discriminative Correction</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is >70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL]]></itunes:summary>
<description><![CDATA[Generative approaches have been recently shown to be effective for both Entity Disambiguation and Entity Linking (i.e., joint mention detection and disambiguation). However, the previously proposed autoregressive formulation for EL suffers from i) high computational cost due to a complex (deep) decoder, ii) non-parallelizable decoding that scales with the source sequence length, and iii) the need for training on a large amount of data. In this work, we propose a very efficient approach that parallelizes autoregressive linking across all potential mentions and relies on a shallow and efficient decoder. Moreover, we augment the generative objective with an extra discriminative component, i.e., a correction term which lets us directly optimize the generator's ranking. When taken together, these techniques tackle all the above issues: our model is >70 times faster and more accurate than the previous generative method, outperforming state-of-the-art approaches on the standard English dataset AIDA-CoNLL. Source code available at https://github.com/nicola-decao/efficient-autoregressive-EL]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.03792v1.Highly_Parallel_Autoregressive_Entity_Linking_with_Discriminative_Correction.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1345.933</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.03792v1.Highly_Parallel_Autoregressive_Entity_Linking_with_Discriminative_Correction.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Towards Understanding Grokking: An Effective Theory of Representation Learning</title>
<itunes:title>Towards Understanding Grokking: An Effective Theory of Representation Learning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.]]></itunes:summary>
<description><![CDATA[We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10343v1.Towards_Understanding_Grokking_An_Effective_Theory_of_Representation_Learning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3217.8155</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.10343v1.Towards_Understanding_Grokking_An_Effective_Theory_of_Representation_Learning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Learning Transferable Visual Models From Natural Language Supervision</title>
<itunes:title>Learning Transferable Visual Models From Natural Language Supervision</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.]]></itunes:summary>
<description><![CDATA[State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00020v1.Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.mp3" length="" type="audio/mpeg"/>
<itunes:duration>9481.24725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2103.00020v1.Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>How to represent part-whole hierarchies in a neural network</title>
<itunes:title>How to represent part-whole hierarchies in a neural network</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language]]></itunes:summary>
<description><![CDATA[This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several different groups to be combined into an imaginary system called GLOM. The advances include transformers, neural fields, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a fixed architecture parse an image into a part-whole hierarchy which has a different structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should significantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.12627v1.How_to_represent_part_whole_hierarchies_in_a_neural_network.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5856.444</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2102.12627v1.How_to_represent_part_whole_hierarchies_in_a_neural_network.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Moral Dilemmas for Moral Machines</title>
<itunes:title>Moral Dilemmas for Moral Machines</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Autonomous systems are being developed and deployed in situations that may require some degree of ethical decision-making ability. As a result, research in machine ethics has proliferated in recent years. This work has included using moral dilemmas as validation mechanisms for implementing decision-making algorithms in ethically-loaded situations. Using trolley-style problems in the context of autonomous vehicles as a case study, I argue (1) that this is a misapplication of philosophical thought experiments because (2) it fails to appreciate the purpose of moral dilemmas, and (3) this has potentially catastrophic consequences; however, (4) there are uses of moral dilemmas in machine ethics that are appropriate and the novel situations that arise in a machine-learning context can shed some light on philosophical work in ethics.]]></itunes:summary>
<description><![CDATA[Autonomous systems are being developed and deployed in situations that may require some degree of ethical decision-making ability. As a result, research in machine ethics has proliferated in recent years. This work has included using moral dilemmas as validation mechanisms for implementing decision-making algorithms in ethically-loaded situations. Using trolley-style problems in the context of autonomous vehicles as a case study, I argue (1) that this is a misapplication of philosophical thought experiments because (2) it fails to appreciate the purpose of moral dilemmas, and (3) this has potentially catastrophic consequences; however, (4) there are uses of moral dilemmas in machine ethics that are appropriate and the novel situations that arise in a machine-learning context can shed some light on philosophical work in ethics.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.06152v1.Moral_Dilemmas_for_Moral_Machines.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2189.244</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.06152v1.Moral_Dilemmas_for_Moral_Machines.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</title>
<itunes:title>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.]]></itunes:summary>
<description><![CDATA[Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2206.11795v1.Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6392.79025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2206.11795v1.Video_PreTraining_VPT_Learning_to_Act_by_Watching_Unlabeled_Online_Videos.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Faithful Reasoning Using Large Language Models</title>
<itunes:title>Faithful Reasoning Using Large Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.]]></itunes:summary>
<description><![CDATA[Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.14271v1.Faithful_Reasoning_Using_Large_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6772.66275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.14271v1.Faithful_Reasoning_Using_Large_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Moral foundations vignettes: a standardized stimulus database of scenarios based on moral foundations theory</title>
<itunes:title>Moral foundations vignettes: a standardized stimulus database of scenarios based on moral foundations theory</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Abstract Research on the emotional, cognitive, and social determinants of moral judgment has surged in recent years. The development of moral foundations theory (MFT) has played an important role, demonstrating the breadth of morality. Moral psychology has responded by investigating how different domains of moral judgment are shaped by a variety of psychological factors. Yet, the discipline lacks a validated set of moral violations that span the moral domain, creating a barrier to investigating influences on judgment and how their neural bases might vary across the moral domain. In this paper, we aim to fill this gap by developing and validating a large set of moral foundations vignettes (MFVs). Each vignette depicts a behavior violating a particular moral foundation and not others. The vignettes are controlled on many dimensions including syntactic structure and complexity making them suitable for neuroimaging research. We demonstrate the validity of our vignettes by examining respondents' classifications of moral violations, conducting exploratory and confirmatory factor analysis, and demonstrating the correspondence between the extracted factors and existing measures of the moral foundations. We expect that the MFVs will be beneficial for a wide variety of behavioral and neuroimaging investigations of moral cognition.]]></itunes:summary>
<description><![CDATA[Abstract Research on the emotional, cognitive, and social determinants of moral judgment has surged in recent years. The development of moral foundations theory (MFT) has played an important role, demonstrating the breadth of morality. Moral psychology has responded by investigating how different domains of moral judgment are shaped by a variety of psychological factors. Yet, the discipline lacks a validated set of moral violations that span the moral domain, creating a barrier to investigating influences on judgment and how their neural bases might vary across the moral domain. In this paper, we aim to fill this gap by developing and validating a large set of moral foundations vignettes (MFVs). Each vignette depicts a behavior violating a particular moral foundation and not others. The vignettes are controlled on many dimensions including syntactic structure and complexity making them suitable for neuroimaging research. We demonstrate the validity of our vignettes by examining respondents' classifications of moral violations, conducting exploratory and confirmatory factor analysis, and demonstrating the correspondence between the extracted factors and existing measures of the moral foundations. We expect that the MFVs will be beneficial for a wide variety of behavioral and neuroimaging investigations of moral cognition.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13428-014-0551-2.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4669.1265</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13428-014-0551-2.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population</title>
<itunes:title>DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.]]></itunes:summary>
<description><![CDATA[We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1652.715</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>LaMDA: Language Models for Dialog Applications</title>
<itunes:title>LaMDA: Language Models for Dialog Applications</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.]]></itunes:summary>
<description><![CDATA[We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.08239v3.LaMDA_Language_Models_for_Dialog_Applications.mp3" length="" type="audio/mpeg"/>
<itunes:duration>7103.765</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.08239v3.LaMDA_Language_Models_for_Dialog_Applications.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>BERT has a Moral Compass: Improvements of ethical and moral values of machines</title>
<itunes:title>BERT has a Moral Compass: Improvements of ethical and moral values of machines</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.]]></itunes:summary>
<description><![CDATA[Allowing machines to choose whether to kill humans would be devastating for world peace and security. But how do we equip machines with the ability to learn ethical or even moral choices? Jentzsch et al.(2019) showed that applying machine learning to human texts can extract deontological ethical reasoning about "right" and "wrong" conduct by calculating a moral bias score on a sentence level using sentence embeddings. The machine learned that it is objectionable to kill living beings, but it is fine to kill time; It is essential to eat, yet one might not eat dirt; it is important to spread information, yet one should not spread misinformation. However, the evaluated moral bias was restricted to simple actions -- one verb -- and a ranking of actions with surrounding context. Recently BERT ---and variants such as RoBERTa and SBERT--- has set a new state-of-the-art performance for a wide range of NLP tasks. But has BERT also a better moral compass? In this paper, we discuss and show that this is indeed the case. Thus, recent improvements of language representations also improve the representation of the underlying ethical and moral values of the machine. We argue that through an advanced semantic representation of text, BERT allows one to get better insights of moral and ethical values implicitly represented in text. This enables the Moral Choice Machine (MCM) to extract more accurate imprints of moral choices and ethical values.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.05238v1.BERT_has_a_Moral_Compass_Improvements_of_ethical_and_moral_values_of_machines.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2212.075</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.05238v1.BERT_has_a_Moral_Compass_Improvements_of_ethical_and_moral_values_of_machines.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Teaching language models to support answers with verified quotes</title>
<itunes:title>Teaching language models to support answers with verified quotes</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.]]></itunes:summary>
<description><![CDATA[Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11147v1.Teaching_language_models_to_support_answers_with_verified_quotes.mp3" length="" type="audio/mpeg"/>
<itunes:duration>6529.85475</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11147v1.Teaching_language_models_to_support_answers_with_verified_quotes.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>UnNatural Language Inference</title>
<itunes:title>UnNatural Language Inference</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.]]></itunes:summary>
<description><![CDATA[Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to know humanlike syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are largely invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle with ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word-order invariant. In the MNLI dataset, for example, we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are sometimes even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists for both Transformers and pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Mandarin Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00010v2.UnNatural_Language_Inference.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3160.47675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00010v2.UnNatural_Language_Inference.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</title>
<itunes:title>GPT-NeoX-20B: An Open-Source Autoregressive Language Model</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.]]></itunes:summary>
<description><![CDATA[We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.06745v1.GPT_NeoX_20B_An_Open_Source_Autoregressive_Language_Model.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4890.09625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.06745v1.GPT_NeoX_20B_An_Open_Source_Autoregressive_Language_Model.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Training Compute-Optimal Large Language Models</title>
<itunes:title>Training Compute-Optimal Large Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4$\times$ more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.]]></itunes:summary>
<description><![CDATA[We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4$\times$ more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.15556v1.Training_Compute_Optimal_Large_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3861.94275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.15556v1.Training_Compute_Optimal_Large_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</title>
<itunes:title>Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.]]></itunes:summary>
<description><![CDATA[Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners without any prompt engineering. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance. Code is available in https://github.com/zjunlp/DART.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.13161v6.Differentiable_Prompt_Makes_Pre_trained_Language_Models_Better_Few_shot_Learners.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2517.15925</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.13161v6.Differentiable_Prompt_Makes_Pre_trained_Language_Models_Better_Few_shot_Learners.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks</title>
<itunes:title>A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.]]></itunes:summary>
<description><![CDATA[Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1611.01587v5.A_Joint_Many_Task_Model_Growing_a_Neural_Network_for_Multiple_NLP_Tasks.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3419.324</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1611.01587v5.A_Joint_Many_Task_Model_Growing_a_Neural_Network_for_Multiple_NLP_Tasks.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Can Machines Learn Morality? The Delphi Experiment</title>
<itunes:title>Can Machines Learn Morality? The Delphi Experiment</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.]]></itunes:summary>
<description><![CDATA[As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., "helping a friend" is generally good, while "helping a friend spread fake news" is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07574v2.Can_Machines_Learn_Morality_The_Delphi_Experiment.mp3" length="" type="audio/mpeg"/>
<itunes:duration>8257.54125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.07574v2.Can_Machines_Learn_Morality_The_Delphi_Experiment.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Understanding Diffusion Models: A Unified Perspective</title>
<itunes:title>Understanding Diffusion Models: A Unified Perspective</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.]]></itunes:summary>
<description><![CDATA[Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models (VDM) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the ELBO. We then prove that optimizing a VDM boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.11970v1.Understanding_Diffusion_Models_A_Unified_Perspective.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5277.93625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.11970v1.Understanding_Diffusion_Models_A_Unified_Perspective.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models</title>
<itunes:title>Outta Control: Laws of Semantic Change and Inherent Biases in Word Representation Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.]]></itunes:summary>
<description><![CDATA[This article evaluates three proposed laws of semantic change. Our claim is that in order to validate a putative law of semantic change, the effect should be observed in the genuine condition but absent or reduced in a suitably matched control condition, in which no change can possibly have taken place. Our analysis shows that the effects reported in recent literature must be substantially revised: (i) the proposed negative correlation between meaning change and word frequency is shown to be largely an artefact of the models of word representation used; (ii) the proposed negative correlation between meaning change and prototypicality is shown to be much weaker than what has been claimed in prior art; and (iii) the proposed positive correlation between meaning change and polysemy is largely an artefact of word frequency. These empirical observations are corroborated by analytical proofs that show that count representations introduce an inherent dependence on word frequency, and thus word frequency cannot be evaluated as an independent factor with these representations.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/D17-1118.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2414.315</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/D17-1118.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Automatic Detection of Generated Text is Easiest when Humans are Fooled</title>
<itunes:title>Automatic Detection of Generated Text is Easiest when Humans are Fooled</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.]]></itunes:summary>
<description><![CDATA[Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies---top-$k$, nucleus sampling, and untruncated random sampling---and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.00650v2.Automatic_Detection_of_Generated_Text_is_Easiest_when_Humans_are_Fooled.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2894.96825</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.00650v2.Automatic_Detection_of_Generated_Text_is_Easiest_when_Humans_are_Fooled.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
<itunes:title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.]]></itunes:summary>
<description><![CDATA[The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3168.67925</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Quest for a Common Model of the Intelligent Decision Maker</title>
<itunes:title>The Quest for a Common Model of the Intelligent Decision Maker</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.]]></itunes:summary>
<description><![CDATA[The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1499.1935</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A General Language Assistant as a Laboratory for Alignment</title>
<itunes:title>A General Language Assistant as a Laboratory for Alignment</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.]]></itunes:summary>
<description><![CDATA[Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.00861v3.A_General_Language_Assistant_as_a_Laboratory_for_Alignment.mp3" length="" type="audio/mpeg"/>
<itunes:duration>8011.102</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.00861v3.A_General_Language_Assistant_as_a_Laboratory_for_Alignment.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</title>
<itunes:title>RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.]]></itunes:summary>
<description><![CDATA[Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.11462v2.RealToxicityPrompts_Evaluating_Neural_Toxic_Degeneration_in_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3925.969</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.11462v2.RealToxicityPrompts_Evaluating_Neural_Toxic_Degeneration_in_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Chain of Thought Prompting Elicits Reasoning in Large Language Models</title>
<itunes:title>Chain of Thought Prompting Elicits Reasoning in Large Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought -- a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.]]></itunes:summary>
<description><![CDATA[Although scaling up language model size has reliably improved performance on a range of NLP tasks, even the largest models currently struggle with certain reasoning tasks such as math word problems, symbolic manipulation, and commonsense reasoning. This paper explores the ability of language models to generate a coherent chain of thought -- a series of short sentences that mimic the reasoning process a person might have when responding to a question. Experiments show that inducing a chain of thought via prompting can enable sufficiently large language models to better perform reasoning tasks that otherwise have flat scaling curves.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.11903v1.Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4299.23275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.11903v1.Chain_of_Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions</title>
<itunes:title>PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.]]></itunes:summary>
<description><![CDATA[Cross-entropy loss and focal loss are the most common choices when training deep neural networks for classification problems. Generally speaking, however, a good loss function can take on much more flexible forms, and should be tailored for different tasks and datasets. Motivated by how functions can be approximated via Taylor expansion, we propose a simple framework, named PolyLoss, to view and design loss functions as a linear combination of polynomial functions. Our PolyLoss allows the importance of different polynomial bases to be easily adjusted depending on the targeting tasks and datasets, while naturally subsuming the aforementioned cross-entropy loss and focal loss as special cases. Extensive experimental results show that the optimal choice within the PolyLoss is indeed dependent on the task and dataset. Simply by introducing one extra hyperparameter and adding one line of code, our Poly-1 formulation outperforms the cross-entropy loss and focal loss on 2D image classification, instance segmentation, object detection, and 3D object detection tasks, sometimes by a large margin.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12511v1.PolyLoss_A_Polynomial_Expansion_Perspective_of_Classification_Loss_Functions.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2992.82275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12511v1.PolyLoss_A_Polynomial_Expansion_Perspective_of_Classification_Loss_Functions.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Distilling a Neural Network Into a Soft Decision Tree</title>
<itunes:title>Distilling a Neural Network Into a Soft Decision Tree</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.]]></itunes:summary>
<description><![CDATA[Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1711.09784v1.Distilling_a_Neural_Network_Into_a_Soft_Decision_Tree.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1108.5845</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1711.09784v1.Distilling_a_Neural_Network_Into_a_Soft_Decision_Tree.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Social Chemistry 101: Learning to Reason about Social and Moral Norms</title>
<itunes:title>Social Chemistry 101: Learning to Reason about Social and Moral Norms</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes."   We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.   Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.]]></itunes:summary>
<description><![CDATA[Social norms -- the unspoken commonsense rules about acceptable social behavior -- are crucial in understanding the underlying causes and intents of people's actions in narratives. For example, underlying an action such as "wanting to call cops on my neighbors" are social norms that inform our conduct, such as "It is expected that you report crimes."   We present Social Chemistry, a new conceptual formalism to study people's everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language. We introduce Social-Chem-101, a large-scale corpus that catalogs 292k rules-of-thumb such as "it is rude to run a blender at 5am" as the basic conceptual units. Each rule-of-thumb is further broken down with 12 different dimensions of people's judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.   Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction. Our model framework, Neural Norm Transformer, learns and generalizes Social-Chem-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2011.00620v3.Social_Chemistry_101_Learning_to_Reason_about_Social_and_Moral_Norms.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3446.04725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2011.00620v3.Social_Chemistry_101_Learning_to_Reason_about_Social_and_Moral_Norms.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement</title>
<itunes:title>Stochastic Beams and Where to Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.]]></itunes:summary>
<description><![CDATA[The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this 'Gumbel-Top-$k$' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1903.06059v2.Stochastic_Beams_and_Where_to_Find_Them_The_Gumbel_Top_k_Trick_for_Sampling_Sequences_Without_Replacement.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3202.325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1903.06059v2.Stochastic_Beams_and_Where_to_Find_Them_The_Gumbel_Top_k_Trick_for_Sampling_Sequences_Without_Replacement.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Do Transformers use variable binding?</title>
<itunes:title>Do Transformers use variable binding?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation. One central symbolic capacity is variable binding: linking an input value to an abstract variable held in system-internal memory. Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding. We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed. We provide the first systematic evaluation of the variable binding capacities of the state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone. The results show a clear discrepancy between classification and sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or reverse strings even when trained on task-specific vocabularies that are switched in the test set; but both models completely fail to generalize across vocabularies in similar sequence classification tasks. These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external "memory" rather than network-internal symbolic operations involving variable binding. Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding.]]></itunes:summary>
<description><![CDATA[Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation. One central symbolic capacity is variable binding: linking an input value to an abstract variable held in system-internal memory. Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding. We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed. We provide the first systematic evaluation of the variable binding capacities of the state-of-the-art Transformer networks BERT and RoBERTa. Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone. The results show a clear discrepancy between classification and sequence-to-sequence tasks: BERT and RoBERTa can easily learn to copy or reverse strings even when trained on task-specific vocabularies that are switched in the test set; but both models completely fail to generalize across vocabularies in similar sequence classification tasks. These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external "memory" rather than network-internal symbolic operations involving variable binding. Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.00162v1.Do_Transformers_use_variable_binding.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3186.2595</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.00162v1.Do_Transformers_use_variable_binding.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning</title>
<itunes:title>Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.]]></itunes:summary>
<description><![CDATA[We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.08200v1.Unsupervised_Natural_Language_Inference_via_Decoupled_Multimodal_Contrastive_Learning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2197.91675</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.08200v1.Unsupervised_Natural_Language_Inference_via_Decoupled_Multimodal_Contrastive_Learning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Masked Autoencoders As Spatiotemporal Learners</title>
<itunes:title>Masked Autoencoders As Spatiotemporal Learners</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.]]></itunes:summary>
<description><![CDATA[This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90% (vs. 75% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., > 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09113v1.Masked_Autoencoders_As_Spatiotemporal_Learners.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2763.8335</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09113v1.Masked_Autoencoders_As_Spatiotemporal_Learners.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Moral reframing: A technique for effective and persuasive communication across political divides</title>
<itunes:title>Moral reframing: A technique for effective and persuasive communication across political divides</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The political landscape in the US and many other countries is characterized by policy impasses and animosity between rival political groups. Research finds that these divisions are Over the last decade, studies of moral reframing have shown its effectiveness across a wide range of polarized topics, including views of economic inequality, environmental protection, same-sex marriage, and major party candidates for the US presidency. In this article, we review the moral reframing literature, examining potential mediators and moderators of the effect, and discuss important questions that remain unanswered about this phenomenon.]]></itunes:summary>
<description><![CDATA[The political landscape in the US and many other countries is characterized by policy impasses and animosity between rival political groups. Research finds that these divisions are Over the last decade, studies of moral reframing have shown its effectiveness across a wide range of polarized topics, including views of economic inequality, environmental protection, same-sex marriage, and major party candidates for the US presidency. In this article, we review the moral reframing literature, examining potential mediators and moderators of the effect, and discuss important questions that remain unanswered about this phenomenon.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2f07d4_50c5fdcaadbf4ec69697fa6a62ac02e3.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2408.6465</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2f07d4_50c5fdcaadbf4ec69697fa6a62ac02e3.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Aligning Generative Language Models with Human Values</title>
<itunes:title>Aligning Generative Language Models with Human Values</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral ). Existing methods learn human values either by directly mimick-ing the behavior of human data, or rigidly con-straining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes S ENSEI , a new reinforcement learning based method that can embed human values judgements into each step of language generation. S ENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, S ENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.]]></itunes:summary>
<description><![CDATA[Although current large-scale generative language models (LMs) can show impressive insights about factual knowledge, they do not exhibit similar success with respect to human values judgements (e.g., whether or not the generations of an LM are moral ). Existing methods learn human values either by directly mimick-ing the behavior of human data, or rigidly con-straining the generation space to human-chosen tokens. These methods are inherently limited in that they do not consider the contextual and abstract nature of human values and as a result often fail when dealing with out-of-domain context or sophisticated and abstract human values. This paper proposes S ENSEI , a new reinforcement learning based method that can embed human values judgements into each step of language generation. S ENSEI deploys an Actor-Critic framework, where the Critic is a reward distributor that simulates the reward assignment procedure of humans, while the Actor guides the generation towards the maximum reward direction. Compared with five existing methods in three human values alignment datasets, S ENSEI not only achieves higher alignment performance in terms of both automatic and human evaluations, but also shows improvements on robustness and transfer learning on unseen human values.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/250562745.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2124.01625</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/250562745.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Single Headed Attention RNN: Stop Thinking With Your Head</title>
<itunes:title>Single Headed Attention RNN: Stop Thinking With Your Head</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.]]></itunes:summary>
<description><![CDATA[The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.11423v2.Single_Headed_Attention_RNN_Stop_Thinking_With_Your_Head.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2614.43925</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1911.11423v2.Single_Headed_Attention_RNN_Stop_Thinking_With_Your_Head.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Risks from Learned Optimization in Advanced Machine Learning Systems</title>
<itunes:title>Risks from Learned Optimization in Advanced Machine Learning Systems</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.]]></itunes:summary>
<description><![CDATA[We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01820v3.Risks_from_Learned_Optimization_in_Advanced_Machine_Learning_Systems.mp3" length="" type="audio/mpeg"/>
<itunes:duration>7254.33475</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01820v3.Risks_from_Learned_Optimization_in_Advanced_Machine_Learning_Systems.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Fully Differentiable Beam Search Decoder</title>
<itunes:title>A Fully Differentiable Beam Search Decoder</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.]]></itunes:summary>
<description><![CDATA[We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2335.08575</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>PERSONALITY PROCESSES AND INDIVIDUAL DIFFERENCES Liberals and Conservatives Rely on Different Sets of Moral Foundations</title>
<itunes:title>PERSONALITY PROCESSES AND INDIVIDUAL DIFFERENCES Liberals and Conservatives Rely on Different Sets of Moral Foundations</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt & J. Graham, 2007;J. Haidt & C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/ respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), "sacredness" reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American "culture war."]]></itunes:summary>
<description><![CDATA[How and why do moral judgments vary across the political spectrum? To test moral foundations theory (J. Haidt & J. Graham, 2007;J. Haidt & C. Joseph, 2004), the authors developed several ways to measure people's use of 5 sets of moral intuitions: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/ respect, and Purity/sanctity. Across 4 studies using multiple methods, liberals consistently showed greater endorsement and use of the Harm/care and Fairness/reciprocity foundations compared to the other 3 foundations, whereas conservatives endorsed and used the 5 foundations more equally. This difference was observed in abstract assessments of the moral relevance of foundation-related concerns such as violence or loyalty (Study 1), moral judgments of statements and scenarios (Study 2), "sacredness" reactions to taboo trade-offs (Study 3), and use of foundation-related words in the moral texts of religious sermons (Study 4). These findings help to illuminate the nature and intractability of moral disagreements in the American "culture war."]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/JPSP-2009-Moral-Foundations.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4912.0915</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/JPSP-2009-Moral-Foundations.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT</title>
<itunes:title>Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[With advances in Machine Learning (ML), neural network-based methods, such as Convolutional/Recurrent Neural Networks, have been proposed to assist terminology curators in the development and maintenance of terminologies. Bidirectional Encoder Representations from Transformers (BERT), a new language representation model, obtains state-of-the-art results on a wide array of general English NLP tasks. We explore BERT’s applicability to medical terminology-related tasks. Utilizing the “next sentence prediction” capability of BERT, we show that the Fine-tuning strategy of Transfer Learning (TL) from the BERTBASE model can address a challenging problem in automatic terminology enrichment – insertion of new concepts. Adding a pre-training strategy enhances the results. We apply our strategies to the two largest hierarchies of SNOMED CT, with one release as training data and the following release as test data. The performance of the combined two proposed TL models achieves an average F1 score of 0.85 and 0.86 for the two hierarchies, respectively.]]></itunes:summary>
<description><![CDATA[With advances in Machine Learning (ML), neural network-based methods, such as Convolutional/Recurrent Neural Networks, have been proposed to assist terminology curators in the development and maintenance of terminologies. Bidirectional Encoder Representations from Transformers (BERT), a new language representation model, obtains state-of-the-art results on a wide array of general English NLP tasks. We explore BERT’s applicability to medical terminology-related tasks. Utilizing the “next sentence prediction” capability of BERT, we show that the Fine-tuning strategy of Transfer Learning (TL) from the BERTBASE model can address a challenging problem in automatic terminology enrichment – insertion of new concepts. Adding a pre-training strategy enhances the results. We apply our strategies to the two largest hierarchies of SNOMED CT, with one release as training data and the following release as test data. The performance of the combined two proposed TL models achieves an average F1 score of 0.85 and 0.86 for the two hierarchies, respectively.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1700.1535</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Transfer Learning from BERT to Support Insertion of New Concepts into SNOMED CT.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</title>
<itunes:title>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.]]></itunes:summary>
<description><![CDATA[Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05832v1.What_Language_Model_Architecture_and_Pretraining_Objective_Work_Best_for_Zero_Shot_Generalization.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4025.86125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05832v1.What_Language_Model_Architecture_and_Pretraining_Objective_Work_Best_for_Zero_Shot_Generalization.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Mapping the Moral Domain NIH Public Access</title>
<itunes:title>Mapping the Moral Domain NIH Public Access</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The moral domain is broader than the empathy and justice concerns assessed by existing measures of moral competence, and it is not just a subset of the values assessed by value inventories. To fill the need for reliable and theoretically-grounded measurement of the full range of moral concerns, we developed the Moral Foundations Questionnaire (MFQ) based on a theoretical model of five universally available (but variably developed) sets of moral intuitions: Harm/care, Fairness/ reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. We present evidence for the internal and external validity of the scale and the model, and in doing so present new findings about morality: 1. Comparative model fitting of confirmatory factor analyses provides empirical justification for a five-factor structure of moral concerns. 2. Convergent/discriminant validity evidence suggests that moral concerns predict personality features and social group attitudes not previously considered morally relevant. 3. We establish pragmatic validity of the measure in providing new knowledge and research opportunities concerning demographic and cultural differences in moral intuitions. These analyses provide evidence for the usefulness of Moral Foundations Theory in simultaneously increasing the scope and sharpening the resolution of psychological views of morality. Keywords morality; scale validation; moral foundations; culture; values Correspondence concerning this article should be sent to jgraham@virginia.edu. Publisher's Disclaimer: The following manuscript is the final accepted manuscript. It has not been subjected to the final copyediting, fact-checking, and proofreading required for formal publication. It is not the definitive, publisher-authenticated version. The American Psychological Association and its Council of Editors disclaim any responsibility or liabilities for errors or omissions of this manuscript version, any version derived from this manuscript by NIH, or other third parties. The published version is available at www.apa.org/pubs/journals/PSP Supplemental data and analyses can be found at www.moralfoundations.org.Author Manuscript J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1. NIH-PA Author ManuscriptHow can we measure moral concerns when people disagree about what "morality" means? To address this problem we created the Moral Foundations Questionnaire (MFQ, presented in the appendix), a measure of the degree to which individuals endorse each of five intuitive systems posited by Moral Foundations Theory: Harm/care, Fairness/reciprocity, Ingroup/ loyalty, Authority/respect, and Purity/sanctity (Haidt & Graham, 2007;Shweder, Much, Mahapatra & Park, 1997). People vary in the extent to which they endorse, value, and use these five foundations, providing an opportunity to better understand moral diversity. In this article, we explain the need for a scale broader than conventional morality scales. We describe the development of the scale, which involved multiple rounds of item analysis using large heterogeneous samples. We present the validation of the scale, organized into evidence confirming internal and external validity. Finally, we describe its pragmatic validity -that is, the practical usefulness of both the theory and the measure in providing new insights about moral psychology. A major goal of Moral Foundations Theory is to expand the range of phenomena studied in moral psychology so that it matches the full range of moral concerns, including those found in non-Western cultures, in religious practices, and among political conservatives. Here we present what we have learned about the content and structure of the moral domain using the Moral Foundations Questionnaire.A great variety of scales are used in moral psychology to measure stages of moral reasoning (e.g., the Defining Issues Test-2; Rest et al., 1999), moral identity (e.g., the Moral Identity Scale; Aquino & Reed, 2002), empathy (e.g., the Interpersonal Reactivity Index; Davis, 1983), and moral deficits such as psychopathy (e.g., Levenson's Self-report Psychopathy Scale; Levenson et al., 1995). Although these scales measure different aspects of morality, they all share the assumption (explicit or implicit) that the moral domain is limited to concerns about individuals harming or unfairly treating other individuals. This is, in part, a reflection of the dominance of Lawrence Kohlberg's (1969Kohlberg's ( , 1971 ideas about morality as justice, and his subsequent debate with Carol Gilligan (1982) about her alternative conception of morality as care. Both sides agreed that morality was about how well or poorly individuals treated other individuals. Turiel (1983, p.3) codified this approach into a widely-cited definition of the moral domain as "prescriptive judgments of justice, rights, and welfare pertaining to how people ought to relate to each other." Any values that were not related to "justice, rights, and welfare" (e.g., patriotism, authority, or chastity) were considered non-moral, and were relegated to the domain of social convention or the domain of personal choice (Turiel, Hildebrandt, & Wainryb, 1991). Definitions of morality in philosophy also frequently stress rules or codes of conduct that reduce harm to others (e.g., Gert, 2005;Singer, 1979). Kohlberg certainly noticed that people sometimes justified moral judgments by referring to group-level moral concerns such as authority, loyalty, and tradition, but he thought that such thinking was immature and conventional, a part of the "law and order" ethos of stage 4. With sufficient opportunities for role-taking, adolescents were said to move beyond stage 4 and to begin using post-conventional reasoning based on an understanding of justice. Kohlberg's stage theory has been criticized on many grounds; one criticism relevant for our purposes was that Kohlberg's post-conventional morality enshrined politically liberal ideals as developmental endpoints (Hogan, Johnson, & Emler, 1978;Shweder, 1982;Sullivan, 1977;  for related critiques see Puka, 1994). This critique was backed up by the demonstration that liberals routinely obtain higher principled reasoning scores on the Defining Issues Test, but that conservative students rose to liberal levels when told to "respond as a left-winger would" (Emler, Renwick, & Malone, 1983). Conservatives could reason at the "higher stage," but were not doing so presumably because they had different priorities in their moral reasoning. Despite these critiques, the notion that "true" moral  Oliveira-Souza, & Grafman, 2006). Whether carried out in a scanner, on a website, or in a business school lab room using real money, morality is still usually operationalized as helping (vs. harming) or as playing fair (vs cheating). Kohlberg and Turiel based their circumscription of the moral domain on a line of enlightenment thinking running from Immanuel Kant to John Stuart Mill to John Rawls in which the autonomy and/or welfare of the individual are the starting point for ethical inquiry. Yet even Kant (1797Kant ( /1996) had intuitions of a broader moral domain. He wrote that masturbation was "in the highest degree opposed to morality," although he granted that "it is not so easy to produce a rational demonstration" of its wrongness. And before the Enlightenment, philosophers routinely considered a much broader moral domain. Much of ancient moral philosophy, from Greece to India to Japan, was virtue-based. These societies valued benevolence and fairness, but they also emphasized group-level concerns about social order, authority, duty, loyalty to one's family or group, and controlling one's carnal desires to cultivate one's soul or gain a more favorable rebirth (Larue, 1991;Shweder et al., 1997 Cross-cultural research on moral judgment has revealed that Turiel's definition of the moral domain works well among educated and politically liberal Westerners, for whom harmless offenses are rarely condemned, even when they are disgusting or disrespectful (Haidt, Koller, & Dias, 1993).1 However, research on people in India (Shweder, Mahapatra, & Miller, 1987) Dias, 1993), and conservatives in the United States (Graham, Haidt, & Nosek, 2009;Haidt & Hersh, 2001;Jensen, 1998) has revealed moral considerations beyond the individualbased concerns of harm and fairness, involving concerns about spiritual purity and degradation (even for acts that involve no harm), concerns about proper hierarchical role fulfillment, and moral expectations of loyalty to the local or national group. Illustrative of this breadth are open-ended responses participants in our studies gave when asked to define morality in their own words. Parallel to Turiel's definition, many made reference to harm and human welfare (e.g., "Avoiding harm to others"), and fairness or justice ("Morality is doing the right things to ensure fair treatment for all"). However, others made reference to moral issues beyond justice, rights, and welfare, and to morally valuable entities that were not individuals (e.g., "Morality is having a system [that] protects the social institutions of family, community, and country."). Others made reference to duty, obedience, respect, and preserving tradition (e.g., "Matters of duty, irrespective of one's own personal desires or ends"). And some made reference to God or religious norms, decency, the soul, and maintaining purity of mind (e.g., "not having dirty thoughts"  (Rokeach, 1973;Schwartz, 1992). Values research has much to offer the empirical study of morality, and is too often ignored by moral psychologists. Clearly, many values are moral values, even if morality is defined only in terms of welfare and fairness concerns (e.g., benevolence and universalism). However, in seeking to identify a list of the most important values, there is a risk that some common moral concerns or intuitions will be missed. For example, reciprocity, loyalty to one's team or tribe, and concerns about bodily and spiritual purity are ubiquitous in anthropological accounts of In order to fill the need for a systematic theory of morality, explaining its origins, development, and cultural variations, we created Moral Foundations Theory (MFT). Haidt and Joseph (2004) began by surveying the literatures in evolutionary psychology and anthropology, looking for matches -for virtues and areas of moral regulation that were common (though not necessarily universal) across cultures, and that had some clear counterpart in evolutionary thinking. For example, virtues related to fairness and practices of reciprocal gift exchange (e.g., Mauss, 1924Mauss, /1990) bore an obvious similarity to the evolutionary literature on reciprocal altruism (Trivers, 1971); virtues of purity and practices regulating food and sex (e.g., Douglas, 1966) bore an obvious relationship to the evolutionary literature on disgust (Rozin, Haidt, & McCauley, 2000). The results of this cross-disciplinary review produced five top candidates for being the psychological "foundations" upon which cultures construct their moralities. These five foundations are consistent with, but expand upon, several existing taxonomies of moral concern, including Fiske's (1992) four models of social relationships, Shweder et al.'s (1997) account of the "three ethics" of autonomy, community, and divinity that are found widely around the world, and Hogan et al. 's (1978)  evolution-based socioanalytic theory of moral development. MFT can therefore be seen as an attempt to specify the "evolved psychological mechanisms" that were part of the definition of moral systems given earlier. Even if all moral systems are social constructions, they are constructed by people whose minds are not at all like blank slates (Marcus, 2004). In this way, MFT allows for intuitive or emotional bases of moral judgments, as well as more deliberate reasoning processes (cf. Greene et al., 2001;Haidt, 2001). Haidt and Graham (2007) expanded the theory and modified the names of the foundations to become: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/ sanctity. Harm and Fairness generally correspond to Shweder at al.'s (1997) ethics of autonomy; Ingroup and Authority to the ethics of community; and Purity to the ethics of divinity. Haidt and Graham also applied the theory to a particular kind of cultural variation within the United States: the "culture war" between political liberals and conservatives. Drawing on Shweder and several political theorists (e.g., Burke, 1790Burke,  /2003 Mill,  1859Mill, /2003Lakoff, 1996;Muller, 1997;Sowell, 2002), liberalism was hypothesized to indicate a morality in which the individual is the locus of moral value. In such a moral world, moral regulation revolves around protecting individuals from harm or unfair treatment by other individuals, or by the social system. In contrast, conservatives-at least, the social conservatives of the religious right-try to create more tightly-ordered communities in which (for example) proper relationships between parent and child, man and woman, and human and God are part of the aim of moral regulation. In such a moral world, the individual is not the primary locus of moral value; the building block of society is thought to be the family, and a much greater emphasis is placed on virtues and institutions that bind people into roles, duties, and mutual obligations. This analysis led to the hypothesis that liberal morality would prioritize Harm and Fairness over the other three foundations (because the "individualizing foundations" of Harm and NIH-PA Author ManuscriptFairness are all that are needed to support the individual-focused contractual approaches to society often used in enlightenment ethics), whereas conservative morality would also incorporate Ingroup, Authority, and Purity to a substantial degree (because these "binding foundations" are about binding people together into larger groups and institutions). The first draft of the Moral Foundations Questionnaire was created in part to test this hypothesis about ideological differences, and was useful (along with other methods, such as content coding of religious sermons) in supporting the hypothesis (Graham, et al., 2009). However, our goal in subsequent theory and measure development -reported in this paperwas much broader. Moral Foundations Theory provides a conceptual organization for measuring and describing differences in moral concerns across individuals, social groups, and cultures. In theory, any pattern of "settings" or endorsement levels for the five foundations is possible. Thus, a reliable and valid scale was needed to measure the degree to which any individual's moral beliefs and concerns rely upon each of the five hypothesized foundations. Further, theory-driven scale validation is a means of testing hypotheses and theoretical claims (cf. Hogan & Nicholson, 1998). We describe the development of the MFQ below, along with what it revealed about the structure and variation of the moral domain.Because we wanted to gauge individual differences in the range of concerns that people consider morally relevant, the first version of the MFQ (reported in Graham et al., 2009, Study 1, Appendix A) explicitly asked participants to evaluate the moral relevance of several foundation-related concerns (e.g., "Whether or not some people were treated differently from others," for Fairness). For the second version (reported in Graham et al., 2009, Study 2, Appendices A and B) we added a new section that assessed levels of agreement with more specific and contextualized moral judgment statements (see below). These first two versions of the MFQ were tested on heterogeneous populations (using ProjectImplicit.org, a popular web-based virtual laboratory), with large sample sizes (total n=3,825). With these data, we compared different confirmatory factor analysis models that corresponded to distinct, theory-guided conceptualizations of the possible factor structure (see supplementary materials at MoralFoundations.org). Using the comparative model fitting techniques described below, we found evidence that a five-factor solution was an improvement (weighing both fit and parsimony) over models representing a single morality factor, two factors (an "individualizing" factor underlying Harm and Fairness items, and a "binding" factor underlying all other items), and three factors (corresponding to Shweder's three ethics of autonomy, community, and divinity). While the overall model fits were reasonably good (see supplements), some of the internal consistencies of these early versions of the scale were low. In addition, some items had weak loadings on the latent factors. Correlation matrices of all items in the second version revealed that some items that were written to represent one foundation actually related more strongly to another foundation, and some items correlated so highly with each other that they appeared redundant. Problematic items were replaced with new items in the third version of the scale. Pilot testing for the third version of the MFQ was extensive, involving data from over 28,000 participants (surveyed at YourMorals.org) and external validations with eight conceptually related scales. Item and factor analyses on this third version showed improvements over versions 1 and 2. No subscale had more than one item that loaded poorly on the latent factor, so we focused our item analyses on determining how reducing from 40 to 30, 20, or even 10 items would impact the validity of the scale. We developed a novel method for empirically selecting item combinations that would maximize both internal and external validity. For all 10 subscales (four relevance items or four judgments items for each of the five foundations) we identified three criterion scales. The first criterion scale for each subscale was the corresponding foundation subscale using the alternative format (for instance, Harm-judgments as internal validity criterion for Harmrelevance). The second criterion subscale was the corresponding foundation subscale from the "taboo-tradeoffs" questionnaire (see Graham, et al., 2009, Study 3, Appendix C). Modeled on work by Tetlock, Kristel, Elson, Green, and Lerner (2000), this questionnaire asks participants to indicate how much money they would require to perform actions that violate the foundations in a variety of ways (for instance, "kick a dog in the head, hard" for Harm), including options of performing the violation for free, as well as refusing to perform it for any amount of money. The third criterion (used for both relevance and judgments subscales) was an external scale we expected to be related to one particular moral foundation. For Harm the external criterion was the empathy subscale of the Interpersonal Reactivity Index (Davis, 1983); for Fairness, the Schwartz Equality value item (Schwartz, 1992); for Ingroup, the Schwartz Loyalty value item (Schwartz, 1992); for Authority, the Right-Wing Authoritarianism scale (Zakrisson, 2005); and for Purity, the Disgust ScaleRevised (Haidt, McCauley, & Rozin, 1994, modified by Olatunji et al., 2008. In order to quantify how the quality of the scale would decline if shortened, and in order to select the combination of items that would retain the greatest internal and external validity, we calculated correlations between every combination of items and the three criterion scales, as well as a combined item-total correlation with all three criterion scales. An example for Harm-Relevance is shown in Figure 1 (detailed reports of the criterion analyses for all ten subscales can be found in the supplements). The top left panel of Figure 1 plots the corrected item-total correlations with the three criterion scales (Harm-judgments, Harm-tabootradeoffs questionnaire, and IRI-empathy, shown in the other three panels) for every single Harm-relevance item, every 2-item combination, every 3-item combination, and the 4-item aggregate. One can see in this figure that a single item, H ("whether or not someone was harmed") was negatively impacting the internal and external validity of the Harm-relevance aggregates that included it, and that a 3-item aggregate excluding it had even better internal and external reliabilities than the 4-item aggregate. All ten of these analyses revealed that using the best three-item combination yielded internal and external validities as good if not better than using all four items; moreover, we found that in most cases the optimal 2-item combinations were nearly as good as the 3-item combinations (although the 3-item combinations were preferable for their broader conceptual coverage). The best three items from each subscale were retained for the fourth and final version of the MFQ, shown in the Appendix. The 20 starred items make up the short-form MFQ.Throughout the several rounds of item generation and selection, we sought to minimize conceptual and empirical redundancy among items. On this point, we differ from some approaches to scale development that prioritize high internal consistency. Internal consistency is important, but so is comprehensive coverage of the various facets of the construct (the scale development work of Harrison Gough [1979, 1984]  is a good example of this balance; see also John & Soto, 2007). From our point of view, it is better to have dissimilar items that are moderately correlated but that each capture a different facet of a foundation than it is to have similar items that are highly correlated and capture only a small amount of the foundation's scope. As such, our aim in item analysis was not to maximize internal consistency via item redundancy. Instead, we sought a balance between achieving (a) sufficient internal consistency to believe that there was a common core, and (b) maximal item heterogeneity to increase confidence that we were representing the foundation in full. For the moral relevance items, we attempted to cover a wide conceptual area for each foundation while avoiding obvious culture-war issues (e.g., one item is about the moral relevance of rights violations in the abstract, without specifying particular content such as gun rights or gay rights). Items were generated to capture different instances of a foundational moral concern, for instance asking about group loyalty in reference to different specific groups (nation, family) as well as to one's group left in the abstract. We had two reasons for adding the judgments subscale to the relevance subscale. First, we wanted multiple response formats to minimize the impact of variance based on response set (for instance, some people may be more likely to indicate that everything is morally relevant). Second, we wanted to supplement the abstract relevance assessments-which, as self-theories about how one makes moral judgments, may be inaccurate with regard to actual moral judgments (Nisbett & Wilson, 1977)-with contextualized items that could more directly gauge actual moral judgments. To fill out the judgments subscale, participants need not directly consider or be aware of the basis for their moral judgments; they just need to decide whether they endorse or reject the action or event. In this way, the "relevance" subscale may better assess explicit theories about what is morally relevant, and the "judgments" subscale may better assess actual use of moral foundations in judgment (see initial evidence for this conclusion in Graham et al., 2009, Study 2). These judgments took the form of normative declarations (e.g., "It can never be right to kill a human being," for Harm), hypotheticals (e.g., "If I were a soldier and disagreed with my commanding officer's orders, I would obey anyway because that is my duty," for Authority), virtue endorsements (e.g., "Chastity is an important and valuable virtue," for Purity), and opinions about government policies (e.g., "When the government makes laws, the number one principle should be ensuring that everyone is treated fairly," for Fairness). With this variety of both item formats and specific item content, the final version of the MFQ gauges sensitivities to basic kinds of moral concerns, not just opinions on specific moral issues. Sample sizes, number of items, and alphas from pilot testing on all four versions of the MFQ are shown in Table 1. Below we describe validity analyses on the fourth and final version of the scale shown in the Appendix.Participants were 34,476 adults (37% female; mean age 36.2) who had previously registered at YourMorals.org and selected to take the Moral Foundations Questionnaire. Participants come to the website via many different routes, identified using a web tracker. In this sample, 40.5% entered the URL directly into their browser. The most common referring sites were edge.org (29.7%), search engines (8.1%), message boards (3.9%), ted.com (2.2%), and alternet.org (1.9%). (For an analysis demonstrating that response patterns on the MFQ are similar for participants from diverse referring sources, see Iyer, 2009). The relevance section preceded the judgments section, and items within each section were given in an order randomized for each participant. A subset of this sample also chose to take one or several other surveys on the site. This is the sample and procedure used for all internal and external validity analyses except the test-retest, pragmatic, and incremental predictive validity analyses, which are detailed below.Means, standard deviations, and alphas for each subscale of the MFQ are presented in Table  2. Means are also given separately for liberals, moderates, conservatives, and libertarians, to Relations between the relevance and judgments subscales are shown in Table 3. The top panel shows zero-order correlations. To ensure that these relationships were not solely driven by common relations to political ideology, the bottom panel shows partial correlations controlling for politics. Both panels show convergent validity for each foundation as measured by the two formats, as well as discriminant validity in that these relations are stronger than relations between different foundations, despite high correlations among many of the foundations (average same-foundation r = .48, average differentfoundation r = .14).2We gave the MFQ to 123 college students (mean age = 20.1, 69.9% female) from the University of Southern California. After an average interval of 37.4 days (range 28 to 43 days), participants completed the MFQ a second time. In both instances, the MFQ was administered via a class website which recorded the date and time of completion automatically. Question order was randomized for each participant each time.  Table 2. This suggests that item responses are quite stable over time and that within-occasion variation is more a function of the broad diversity of measurement rather than instability.Although Moral Foundations Theory predicts a specific factor structure for moral concerns, we began with exploratory factor analyses to see what factors emerged from the items in the absence of conceptual constraints. Factor analysis for all 30 items of the MFQ was performed using direct oblimin rotation with Kaiser normalization (allowing the factors to be correlated) and maximum likelihood estimation (Fabrigar, Wegener, McCallum, &  Strahan, 1999). Six factors with eigenvalues greater than 1 emerged, but scree plot and factor loadings indicated that only the first two factors provided meaningful incremental explanatory power and interpretability (only 2 of the 120 loadings on the last four factors were above .3; see also Costello & Osborne, 2005, on factor retention from scree plot analysis). The first two factors were retained, and are shown in Table 4. As the table indicates, the two factors clearly corresponded to the binding foundations (Ingroup, Authority, and Purity; factor 1) and individualizing foundations (Harm and Fairness; factor 2), and the strongest loading for all 30 items was as predicted. Although this analysis supported our distinction between individual-focused and groupfocused moral concerns, it remains an open question whether we are justified in treating the theoretically-derived moral foundations as five factors, rather than two. To answer this question we turn to comparisons between different confirmatory factor analysis models. 2 Correlations between the latent foundation factors can be found in Figure 2 (five correlated factors model), and zero-order correlations between the factors are available in a supplement at the first three authors' websites.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author ManuscriptThe large sample sizes we obtained for each version of the MFQ allowed us to create structural equation models comparing different theoretically-derived factor structures. Table  5 describes the comparative model fitting with the final version of the MFQ. The first three numerical columns provide fit statistics for the individual models, and the last two columns show the degree to which each model was an improvement over the model in the row above it. Figure 2 shows the different confirmatory factor analysis models for the full scale; as Table 5 reflects, the same models were constructed for the relevance and judgments subscales separately as well. In the first step, we compared nested first-order models. Our hypothesis was that model 4 (five correlated factors: Harm, Fairness, Ingroup, Authority, and Purity) would provide a better overall model fit than a single morality factor model (1), two-factor model (2: individualizing and binding, corresponding to the results of the exploratory factor analysis), and three-factor model (3: corresponding to Shweder et al.'s (1997) ethics of autonomy, community, and divinity). All three tests (relevance subscale, judgments subscale, and full MFQ) confirmed these predictions; the overall best model (weighing fit and parsimony) was the five-factor model in every case.3 In the second step, we tested whether the five factors could be more parsimoniously modeled with two correlated superordinate factors representing our theoretical distinction of "individualizing" and "binding" foundations (see the hierarchical model in Figure 2). As Table 6 shows, however, the model with five intercorrelated factors was a significant improvement (again, weighing both fit and parsimony) over the hierarchical models. In general, confirmatory factor analyses provide robust support for our five-factor conceptualization of the moral foundations.We identified several other scales and scale items also taken by participants at YourMorals.org that we predicted would relate to the MFQ foundation scores. Scales were grouped into five external criteria scale sets, one set for each foundation. Harm criterion scales were the empathic concern subscale of the Interpersonal Reactivity Index (Davis, 1983; n=134), Levenson's (1995) Psychopathy Scale (reverse-scored; n=116), Schwartz's (1992; n=4,228)  Benevolence subscale, and three items from the Adapted Good-Self Assessment (Barriga, Morrison, Liau, & Gibbs, 2001; n=89) on the importance of being kind/caring, sympathetic/compassionate, and generous/giving. Fairness scales were Social Dominance Orientation (reverse-scored, as it measures preference for social inequalities; Pratto, Sidanius, Stallworth, & Malle, 1994; n=1,215), importance of being fair/just on Barriga's Good-Self scale, and endorsement of the social justice item on Schwartz's values scale. Ingroup scales were the importance of being loyal/faithful on Barriga's Good-Self scale, and endorsement of loyalty, national security, and family security items on Schwartz's values scale. Authority scales were Right-Wing Authoritarianism (Zakrisson, 2005;  n=1,093), the Traditionalism subscale of the Progressive and Traditional Justice scale (Haidt, Darley, & Gromet, 2009; n=1,384), and endorsement of the social order, authority, respect for tradition, honoring parents, and obedience values on Schwartz's value scale. Purity scales were the Disgust Scale-Revised (Haidt, McCauley & Rozin, 1994 modified by  Olatunji et al., 2008 n=1,681), self-reported religious attendance (n=32,607), and 3  We also tested whether a six-factor model separating Authority and Tradition (shown in Figure 2) would improve upon the fivefactor model. However, these six-factor models were a worse fit than the five-factor models, 0.60 ≤ ε a ≤ 0.65; in addition, as Figure 2 shows, the Authority and Tradition factors were very highly correlated (r = .96), further supporting a single latent factor for these items.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author Manuscript NIH-PA Author Manuscript endorsement of the self-discipline, clean, and devout items on Schwartz's values scale. Items from the same scale were averaged together, and correlations between the foundations and the scales were averaged together for each criterion group. Correlations between the foundations and the external criterion scales are shown in Table 7. As the table shows, each foundation was the strongest predictor for its own conceptually related group of external scales (average r = .51, vs. average r = .14 for the off-diagonals). This provides evidence of both convergent and discriminant validity, despite relatively substantial relations among the foundations.A subset of the participants who took the MFQ also took a survey in which they reported their gut reactions to various social groups. We constructed this survey by first identifying groups conceptually related to each foundation because they represent either virtues or vices of that foundation. For instance, we predicted that people whose morality rested heavily on the Harm/care foundation would have especially positive reactions to "caring" groups such as nurses, and especially negative reactions to "harming" groups like hunters. The foundation relevance of each group was identified a priori by the authors. Harm-related groups were nurses, environmentalists, pacifists, vegetarians, and hunters (r). Fairnessrelated groups were ACLU members, labor unions, rich people (r), and CEOs (r). Ingrouprelated groups were Americans, U.S. Government, flag burners (r), and illegal immigrants (r). Authority-related groups were soldiers, police officers, U.S. Marines, U.S. Military, people who spank their children, and anarchists (r). Purity-related groups were virgins, highly religious people, spiritual people, atheists (r), prostitutes (r), homosexuals (r), people who have casual sex (r), and people with tattoos or piercings (r). Groups indicated by "(r)" represented vices of a foundation and were reverse-scored, and for U.S.-specific groups only U.S. citizens were included in the analyses. Because attitudes toward social groups and moral foundations scores are both related to political ideology, we used partial correlations controlling for political ideology to see which groups would be uniquely predicted by one or more foundations. Partial correlations between foundations and all social groups were averaged for each set of foundation-related groups; these averages are shown in Table 8. As the table shows, each foundation was the strongest predictor (above and beyond politics) of attitudes toward conceptually related social groups, providing further evidence of predictive and discriminant validity. Beyond validation of the scale, these results also suggest that attitudes about social groups are in part moral judgments about those groups. Moral Foundations Theory and the MFQ may be useful for researchers who want to know which moral concerns are related to prejudice toward any particular group.The preceding sections establish that the theorized model of Moral Foundations as five interrelated factors is a better fit than other plausible models, and that each of the five factors predict foundation-relevant outcomes. An open question is whether the MFQ has predictive validity beyond existing measures. Because it measures domains that are not present in other theoretical conceptions of morality-Ingroup, Authority, and Purity-it surely expands the predictive validity of morality measures. However, even broader measures exist, such as the Schwartz Values Scale, which measures endorsement of ten broad classes of values. This scale contains several values and subscale factors that conceptually overlap with the moral foundations (e.g., Benevolence with Harm/care, Traditionalism with Authority/respect), and many self-interested values that we consider to be outside the moral domain and not covered by the MFQ (e.g., Achievement, Hedonism). Even though Moral Foundations Theory has a different conceptual starting point (an evolutionary account of why humans have the moral intuitions they do, contra Schwartz's factor-analytic approach), it is nonetheless worthwhile to test whether the MFQ has predictive validity beyond Schwartz's scale in predicting a NIH-PA Author Manuscript NIH-PA Author Manuscript variety of scales, opinions, and self-reported behaviors relevant to morality. Because Schwartz's scale is larger both in terms of subscales (10 vs. 5) and items (58 vs. 30), this is a particularly tough test of the predictive validity of the MFQ. Data for these analyses came from 10,652 visitors to YourMorals.org who took both the MFQ and the Schwartz Values Scale (SVS), 92% of whom also took other scales or measures. We used two-step regressions to test whether the five MFQ subscales added incremental predictive validity beyond the ten SVS subscales for the external criteria (scales and attitudes toward foundation-related social groups) described above, as well as for positions on a wide range of political issues. Note that this analysis focuses on the incremental validity of the aggregate MFQ in comparison to the aggregate SVS, rather than investigating which of the particular moral concerns predict each criterion variable (for such work, see Koleva, Graham, Iyer, Ditto, & Haidt, 2010). In every analysis, the MFQ made a significant improvement to prediction when added to the SVS (average ΔR² = 8%, all ΔR² significant at p < .001). To provide a point of comparison, we repeated this analysis by adding the 44-item Big Five personality inventory to the SVS, which yielded an average ΔR² of only 2%. R² and ΔR² for the scales and foundation-related social group averages can be found in Table 9. We also investigated R² for the MFQ alone, to further compare its predictive validity with that of the SVS. As the bolded values in Table  9 show, the MFQ was actually a more powerful predictor than the SVS for most of the scales and political issue positions, and all of the social group attitudes. Given that the SVS is a comprehensive, large, and well-validated measure of values, the MFQ is clearing a high bar in providing unique predictive validity for outcomes relevant for moral and political psychology, and for the psychology of prejudice.Because every step of the scale development used large heterogeneous samples, we can be more confident about the MFQ's generalizability than if we had used college students only (Sears, 1986). However, the samples obtained at ProjectImplicit.org and at YourMorals.org are not representative of any national or international population -the current sample is disproportionately from the U.S. (80%), white (87%), male (63%), and educated (mean education between "completed college" and "some graduate school") compared to international or U.S. national averages. Thanks to the large sample sizes, however, we were able to test whether the five-factor model of moral concerns was consistent across national and geographic groups. All participants self-reported their current country of residence, the country in which they grew up (if different), and the age at which they moved (if they grew up in a different country). For participants who reported moving to their current country at age 14 or older, the country in which they reported growing up was used for the cross-cultural analysis. We created 12 location codes, four of which indicated the four nations from which the largest number of participants had come (U.S., Canada, U.K., Australia); the other eight location codes indicated multi-nation regions of the world (i.e. East Asia, Middle East). Model fit information for each location code is shown in Table 10. As the table shows, the five-factor model of the MFQ is a reasonable or good fit (all ε a < .06, all CFI >.7) for all 11 world regions for which we were able to run the fit analyses, providing evidence that the measurement and theory of five foundational moral concerns is not specific to U.S. or Western participants. Notably, although the five-foundation model is a good fit in all these areas, the data shows much cross-cultural variation in the patterns of moral foundation endorsement. Even controlling for politics, age, gender, religious attendance, and education, world region is a significant (ps < .001) predictor of all five foundation scores, indicating  (Rozin, 2006). For instance, the above validation exercises with external scales and social group attitudes showed that many traits and attitudes that don't seem to be about morality on the surface nevertheless show a systematic and theoretically meaningful relationship to moral concerns measured by the MFQ. We present here three additional findings made possible by MFT's broadened definition of morality, and its finer conceptual resolution of morality's components.Using a variety of measures and methods, Graham et al. (2009) showed that liberals value Harm and Fairness concerns more than conservatives, while conservatives value Ingroup, Authority, and Purity concerns more than liberals. The vast majority of these participants, however, came from the United States, leaving open the question of whether these patterns were limited to the particular ideological conflicts of the United States. Table 11 shows correlations between political ideology4 and the five foundations for the different world areas described in the Generalizability section. The correlations indicate that the liberal-conservative patterns found in the U.S. are robust across national and cultural contexts, both in terms of direction (negative correlations [liberals higher] for Harm and Fairness, positive correlations [conservatives higher] for Ingroup, Authority, and Purity) and in terms of magnitude: correlations are consistently strongest for Authority and Purity, and weakest for Harm. This suggests that across cultures, the most intractable political debates are likely to involve concerns related to respect for traditions/authorities and physical/spiritual purity, while the greatest degree of moral commonality may be found in issues related to harm and care. It also reinforces the claim that political ideology can be self-assessed and that the unidimensional left-right construct has some degree of common meaning across societies, despite differences in political party structures and particular national issues (Jost, 2006). 4 Because the terms "liberal" and "conservative" can mean different things in different nations (i.e., the Liberal Party in Australia is actually center-right ideologically), the political identification item on YourMorals.org clarifies to participants that the items from "very liberal" to "very conservative" should be read as "very left-wing" to "very right-wing." Graham et al. 06) compared to Western participants, and were only very slightly more concerned about Harm, Fairness, and Authority (mean differences < .1, ts < 7, ds < .04). The fact that differences are concentrated in Ingroup and Purity makes some sense in light of established cultural differences in collectivism (Triandis, 1995) and the role of purity concerns in daily life and religious practice, particularly in South Asia (Shweder et al., 1997). But it is noteworthy that there are not large differences in Authority, given greater sensitivity to social hierarchy (Power Distance scores) in eastern nations (Hofstede, 2001). The small effect sizes for all the East-West differences suggest that variation within cultures (e.g., by gender or political ideology) will exceed the east-West variations given so much attention in crosscultural research. Here we see that the increased resolution afforded by MFT allows us to find moral differences we would not have been able to find otherwisedifferences that open up intriguing questions for further research. As the effect sizes show, these gender differences were much stronger than the differences between Eastern and Western cultures. The gender patterns make sense in light of previous research on empathy (Davis, 1983), egalitarianism (Arts & Gelissen, 2001), and disgust sensitivity (Druschel & Sherman, 1999), but they also show an important divergence from the political patterns in that Purity is here grouped with Harm and Fairness, rather than Ingroup and Authority. Here too the finer resolution and broadened scope of MFT allowed us to find and describe differences in moral personality not possible before.Moral Foundations Theory (Haidt & Joseph, 2004;Haidt & Graham, 2007) was created by selecting the closest links between evolutionary accounts of human sociality and anthropological accounts of the breadth and variability of the moral domain (see especially Fiske, 1992, andShweder et al., 1997). The findings reported in this article suggest that those anthropologists were right. From a purely descriptive perspective, the domain of morality consists of more than just "prescriptive judgments of justice, rights, and welfare" (Turiel, 1983, p. 3). Furthermore, we found that one does not need to travel to non-Western nations to find this broader conception of morality. In every country and world region we examined, people on the political right placed greater emphasis on concerns about ingroup loyalty, respect for authorities and traditions, and physical/spiritual purity than did people on the political left. The Moral Foundations Questionnaire fills the need for a theoretically-grounded scale covering the full range of human moral concerns. We found substantial evidence that the scale is reliable and valid. The scale is internally consistent (both within and between two question formats) while maintaining conceptual coverage of diverse manifestations of foundation-related concerns. Test-retest analyses showed stability of foundation subscale scores over time. External validations of the MFQ using widely-used scales, as well as attitudes toward conceptually related social groups, showed convergent, discriminant, and predictive validity. Factor analyses confirmed our theoretical parsing of the moral domain into five sets of concerns: the five-factor model fit the data better (weighing both fit and parsimony) than competing models, and this five-factor representation provided a good fit for participants in 11 different world areas. In addition to the scale itself, we expect that the method introduced in this paper (see Figure 1) for empirically selecting items to maximize both internal and external validity will also be of use to researchers. The best existing instrument for assessing moral concerns beyond harm and fairness is the Schwartz Values Scale (SVS; Schwartz, 1992), which includes group-level values such as "tradition" and "conformity." However, the SVS was created to measure a broad spectrum of values; it was not designed to cover the moral domain specifically, and it does not cover concerns about group-loyalty and spiritual purity. In a head-to-head comparison, the MFQ showed incremental predictive validity beyond the SVS for a diverse array of external scales related to moral personality, attitudes toward social groups, and opinions about moral and political issues (see Table 9). Further, in most cases, the MFQ performed even better than the SVS in overall variance explained of criterion variables, despite its shorter length and narrower conceptual coverage. This further illustrates the MFQ's effective measurement properties, balancing relatively short length and wide coverage of the moral domain.The research reported here indicates that the MFQ is a reliable and valid instrument for measuring a broad range of moral concerns. But in the process of developing and validating the MFQ, we also generated a number of substantive discoveries about moral psychology, such as:A map of the moral domain. Because it distinguishes five kinds of moral concerns, and gives separate evolutionary accounts to explain each of their origins (Haidt & Joseph, 2007), MFT is not as parsimonious as theories of morality that try to derive the entire moral domain from one or two principles or processes (usually kin selection plus reciprocal altruism -see Dawkins, 1976;Hauser, 2006;Joyce, 2006). However, comparisons of different structural models revealed that the five-factor solution is an improvement over other theoretically-derived models, even taking into account the relative loss of parsimony. Analyses of international data showed that this five-factor model was a good fit in every area of the world we were able to examine. This provides empirical evidence for MFT's central claim about the structure of human morality, and points toward the usefulness of this added A guide to where the action is. We found some small and easily interpretable crosscultural differences in moral foundation scores: people in Eastern cultures were slightly more likely to value Ingroup and Purity than people in Western cultures. As with all research that relies upon educated participants, our cross-national differences would probably have been much larger if we had found a way to survey rural villagers and the urban poor in Asia. Nonetheless, the cross-national differences we did find were dwarfed by the within-nation (or within-region) differences we examined, including both ideological differences and sex differences (women valued Harm, Fairness, and Purity more than men, even controlling for political ideology). With reliable measures of these different kinds of moral concerns, social and personality psychologists can now begin to examine many such patterns of similarities and dissimilarities, as well as the processes behind them.A method for discovering moral prejudices. The finer resolution offered by the MFQ also revealed the potential role of moral judgment in prejudice. When we examined attitudes toward various social groups, we found that MFQ subscales indicated varying patterns of moral concerns that might lead some people to dislike some groups. This suggests that attitudes toward social groups may often be expressions of moral judgments about those groups -vague intuitions or explicit convictions that a particular social group upholds or violates one or more foundational concerns. The moral foundations can thus be used as a kind of decoder ring, allowing us to see multiple and sometimes unexpected moral threads connecting seemingly unrelated attitudes and opinions (cf. Koleva et al, 2010). This possibility emphasizes our functional definition of morality as a description of what motivates people to suppress selfishness, rather than a prescriptive definition of how one ought to behave. By describing and quantifying a broadened range of human moral concerns, MFT and the MFQ can aid in our understanding of the dangers of morality, as well as the benefits.The map of the moral domain that we offer is provisional. We hope that other researchers will help us improve it. Here are four next steps:]]></itunes:summary>
<description><![CDATA[The moral domain is broader than the empathy and justice concerns assessed by existing measures of moral competence, and it is not just a subset of the values assessed by value inventories. To fill the need for reliable and theoretically-grounded measurement of the full range of moral concerns, we developed the Moral Foundations Questionnaire (MFQ) based on a theoretical model of five universally available (but variably developed) sets of moral intuitions: Harm/care, Fairness/ reciprocity, Ingroup/loyalty, Authority/respect, and Purity/sanctity. We present evidence for the internal and external validity of the scale and the model, and in doing so present new findings about morality: 1. Comparative model fitting of confirmatory factor analyses provides empirical justification for a five-factor structure of moral concerns. 2. Convergent/discriminant validity evidence suggests that moral concerns predict personality features and social group attitudes not previously considered morally relevant. 3. We establish pragmatic validity of the measure in providing new knowledge and research opportunities concerning demographic and cultural differences in moral intuitions. These analyses provide evidence for the usefulness of Moral Foundations Theory in simultaneously increasing the scope and sharpening the resolution of psychological views of morality. Keywords morality; scale validation; moral foundations; culture; values Correspondence concerning this article should be sent to jgraham@virginia.edu. Publisher's Disclaimer: The following manuscript is the final accepted manuscript. It has not been subjected to the final copyediting, fact-checking, and proofreading required for formal publication. It is not the definitive, publisher-authenticated version. The American Psychological Association and its Council of Editors disclaim any responsibility or liabilities for errors or omissions of this manuscript version, any version derived from this manuscript by NIH, or other third parties. The published version is available at www.apa.org/pubs/journals/PSP Supplemental data and analyses can be found at www.moralfoundations.org.Author Manuscript J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1. NIH-PA Author ManuscriptHow can we measure moral concerns when people disagree about what "morality" means? To address this problem we created the Moral Foundations Questionnaire (MFQ, presented in the appendix), a measure of the degree to which individuals endorse each of five intuitive systems posited by Moral Foundations Theory: Harm/care, Fairness/reciprocity, Ingroup/ loyalty, Authority/respect, and Purity/sanctity (Haidt & Graham, 2007;Shweder, Much, Mahapatra & Park, 1997). People vary in the extent to which they endorse, value, and use these five foundations, providing an opportunity to better understand moral diversity. In this article, we explain the need for a scale broader than conventional morality scales. We describe the development of the scale, which involved multiple rounds of item analysis using large heterogeneous samples. We present the validation of the scale, organized into evidence confirming internal and external validity. Finally, we describe its pragmatic validity -that is, the practical usefulness of both the theory and the measure in providing new insights about moral psychology. A major goal of Moral Foundations Theory is to expand the range of phenomena studied in moral psychology so that it matches the full range of moral concerns, including those found in non-Western cultures, in religious practices, and among political conservatives. Here we present what we have learned about the content and structure of the moral domain using the Moral Foundations Questionnaire.A great variety of scales are used in moral psychology to measure stages of moral reasoning (e.g., the Defining Issues Test-2; Rest et al., 1999), moral identity (e.g., the Moral Identity Scale; Aquino & Reed, 2002), empathy (e.g., the Interpersonal Reactivity Index; Davis, 1983), and moral deficits such as psychopathy (e.g., Levenson's Self-report Psychopathy Scale; Levenson et al., 1995). Although these scales measure different aspects of morality, they all share the assumption (explicit or implicit) that the moral domain is limited to concerns about individuals harming or unfairly treating other individuals. This is, in part, a reflection of the dominance of Lawrence Kohlberg's (1969Kohlberg's ( , 1971 ideas about morality as justice, and his subsequent debate with Carol Gilligan (1982) about her alternative conception of morality as care. Both sides agreed that morality was about how well or poorly individuals treated other individuals. Turiel (1983, p.3) codified this approach into a widely-cited definition of the moral domain as "prescriptive judgments of justice, rights, and welfare pertaining to how people ought to relate to each other." Any values that were not related to "justice, rights, and welfare" (e.g., patriotism, authority, or chastity) were considered non-moral, and were relegated to the domain of social convention or the domain of personal choice (Turiel, Hildebrandt, & Wainryb, 1991). Definitions of morality in philosophy also frequently stress rules or codes of conduct that reduce harm to others (e.g., Gert, 2005;Singer, 1979). Kohlberg certainly noticed that people sometimes justified moral judgments by referring to group-level moral concerns such as authority, loyalty, and tradition, but he thought that such thinking was immature and conventional, a part of the "law and order" ethos of stage 4. With sufficient opportunities for role-taking, adolescents were said to move beyond stage 4 and to begin using post-conventional reasoning based on an understanding of justice. Kohlberg's stage theory has been criticized on many grounds; one criticism relevant for our purposes was that Kohlberg's post-conventional morality enshrined politically liberal ideals as developmental endpoints (Hogan, Johnson, & Emler, 1978;Shweder, 1982;Sullivan, 1977;  for related critiques see Puka, 1994). This critique was backed up by the demonstration that liberals routinely obtain higher principled reasoning scores on the Defining Issues Test, but that conservative students rose to liberal levels when told to "respond as a left-winger would" (Emler, Renwick, & Malone, 1983). Conservatives could reason at the "higher stage," but were not doing so presumably because they had different priorities in their moral reasoning. Despite these critiques, the notion that "true" moral  Oliveira-Souza, & Grafman, 2006). Whether carried out in a scanner, on a website, or in a business school lab room using real money, morality is still usually operationalized as helping (vs. harming) or as playing fair (vs cheating). Kohlberg and Turiel based their circumscription of the moral domain on a line of enlightenment thinking running from Immanuel Kant to John Stuart Mill to John Rawls in which the autonomy and/or welfare of the individual are the starting point for ethical inquiry. Yet even Kant (1797Kant ( /1996) had intuitions of a broader moral domain. He wrote that masturbation was "in the highest degree opposed to morality," although he granted that "it is not so easy to produce a rational demonstration" of its wrongness. And before the Enlightenment, philosophers routinely considered a much broader moral domain. Much of ancient moral philosophy, from Greece to India to Japan, was virtue-based. These societies valued benevolence and fairness, but they also emphasized group-level concerns about social order, authority, duty, loyalty to one's family or group, and controlling one's carnal desires to cultivate one's soul or gain a more favorable rebirth (Larue, 1991;Shweder et al., 1997 Cross-cultural research on moral judgment has revealed that Turiel's definition of the moral domain works well among educated and politically liberal Westerners, for whom harmless offenses are rarely condemned, even when they are disgusting or disrespectful (Haidt, Koller, & Dias, 1993).1 However, research on people in India (Shweder, Mahapatra, & Miller, 1987) Dias, 1993), and conservatives in the United States (Graham, Haidt, & Nosek, 2009;Haidt & Hersh, 2001;Jensen, 1998) has revealed moral considerations beyond the individualbased concerns of harm and fairness, involving concerns about spiritual purity and degradation (even for acts that involve no harm), concerns about proper hierarchical role fulfillment, and moral expectations of loyalty to the local or national group. Illustrative of this breadth are open-ended responses participants in our studies gave when asked to define morality in their own words. Parallel to Turiel's definition, many made reference to harm and human welfare (e.g., "Avoiding harm to others"), and fairness or justice ("Morality is doing the right things to ensure fair treatment for all"). However, others made reference to moral issues beyond justice, rights, and welfare, and to morally valuable entities that were not individuals (e.g., "Morality is having a system [that] protects the social institutions of family, community, and country."). Others made reference to duty, obedience, respect, and preserving tradition (e.g., "Matters of duty, irrespective of one's own personal desires or ends"). And some made reference to God or religious norms, decency, the soul, and maintaining purity of mind (e.g., "not having dirty thoughts"  (Rokeach, 1973;Schwartz, 1992). Values research has much to offer the empirical study of morality, and is too often ignored by moral psychologists. Clearly, many values are moral values, even if morality is defined only in terms of welfare and fairness concerns (e.g., benevolence and universalism). However, in seeking to identify a list of the most important values, there is a risk that some common moral concerns or intuitions will be missed. For example, reciprocity, loyalty to one's team or tribe, and concerns about bodily and spiritual purity are ubiquitous in anthropological accounts of In order to fill the need for a systematic theory of morality, explaining its origins, development, and cultural variations, we created Moral Foundations Theory (MFT). Haidt and Joseph (2004) began by surveying the literatures in evolutionary psychology and anthropology, looking for matches -for virtues and areas of moral regulation that were common (though not necessarily universal) across cultures, and that had some clear counterpart in evolutionary thinking. For example, virtues related to fairness and practices of reciprocal gift exchange (e.g., Mauss, 1924Mauss, /1990) bore an obvious similarity to the evolutionary literature on reciprocal altruism (Trivers, 1971); virtues of purity and practices regulating food and sex (e.g., Douglas, 1966) bore an obvious relationship to the evolutionary literature on disgust (Rozin, Haidt, & McCauley, 2000). The results of this cross-disciplinary review produced five top candidates for being the psychological "foundations" upon which cultures construct their moralities. These five foundations are consistent with, but expand upon, several existing taxonomies of moral concern, including Fiske's (1992) four models of social relationships, Shweder et al.'s (1997) account of the "three ethics" of autonomy, community, and divinity that are found widely around the world, and Hogan et al. 's (1978)  evolution-based socioanalytic theory of moral development. MFT can therefore be seen as an attempt to specify the "evolved psychological mechanisms" that were part of the definition of moral systems given earlier. Even if all moral systems are social constructions, they are constructed by people whose minds are not at all like blank slates (Marcus, 2004). In this way, MFT allows for intuitive or emotional bases of moral judgments, as well as more deliberate reasoning processes (cf. Greene et al., 2001;Haidt, 2001). Haidt and Graham (2007) expanded the theory and modified the names of the foundations to become: Harm/care, Fairness/reciprocity, Ingroup/loyalty, Authority/respect, and Purity/ sanctity. Harm and Fairness generally correspond to Shweder at al.'s (1997) ethics of autonomy; Ingroup and Authority to the ethics of community; and Purity to the ethics of divinity. Haidt and Graham also applied the theory to a particular kind of cultural variation within the United States: the "culture war" between political liberals and conservatives. Drawing on Shweder and several political theorists (e.g., Burke, 1790Burke,  /2003 Mill,  1859Mill, /2003Lakoff, 1996;Muller, 1997;Sowell, 2002), liberalism was hypothesized to indicate a morality in which the individual is the locus of moral value. In such a moral world, moral regulation revolves around protecting individuals from harm or unfair treatment by other individuals, or by the social system. In contrast, conservatives-at least, the social conservatives of the religious right-try to create more tightly-ordered communities in which (for example) proper relationships between parent and child, man and woman, and human and God are part of the aim of moral regulation. In such a moral world, the individual is not the primary locus of moral value; the building block of society is thought to be the family, and a much greater emphasis is placed on virtues and institutions that bind people into roles, duties, and mutual obligations. This analysis led to the hypothesis that liberal morality would prioritize Harm and Fairness over the other three foundations (because the "individualizing foundations" of Harm and NIH-PA Author ManuscriptFairness are all that are needed to support the individual-focused contractual approaches to society often used in enlightenment ethics), whereas conservative morality would also incorporate Ingroup, Authority, and Purity to a substantial degree (because these "binding foundations" are about binding people together into larger groups and institutions). The first draft of the Moral Foundations Questionnaire was created in part to test this hypothesis about ideological differences, and was useful (along with other methods, such as content coding of religious sermons) in supporting the hypothesis (Graham, et al., 2009). However, our goal in subsequent theory and measure development -reported in this paperwas much broader. Moral Foundations Theory provides a conceptual organization for measuring and describing differences in moral concerns across individuals, social groups, and cultures. In theory, any pattern of "settings" or endorsement levels for the five foundations is possible. Thus, a reliable and valid scale was needed to measure the degree to which any individual's moral beliefs and concerns rely upon each of the five hypothesized foundations. Further, theory-driven scale validation is a means of testing hypotheses and theoretical claims (cf. Hogan & Nicholson, 1998). We describe the development of the MFQ below, along with what it revealed about the structure and variation of the moral domain.Because we wanted to gauge individual differences in the range of concerns that people consider morally relevant, the first version of the MFQ (reported in Graham et al., 2009, Study 1, Appendix A) explicitly asked participants to evaluate the moral relevance of several foundation-related concerns (e.g., "Whether or not some people were treated differently from others," for Fairness). For the second version (reported in Graham et al., 2009, Study 2, Appendices A and B) we added a new section that assessed levels of agreement with more specific and contextualized moral judgment statements (see below). These first two versions of the MFQ were tested on heterogeneous populations (using ProjectImplicit.org, a popular web-based virtual laboratory), with large sample sizes (total n=3,825). With these data, we compared different confirmatory factor analysis models that corresponded to distinct, theory-guided conceptualizations of the possible factor structure (see supplementary materials at MoralFoundations.org). Using the comparative model fitting techniques described below, we found evidence that a five-factor solution was an improvement (weighing both fit and parsimony) over models representing a single morality factor, two factors (an "individualizing" factor underlying Harm and Fairness items, and a "binding" factor underlying all other items), and three factors (corresponding to Shweder's three ethics of autonomy, community, and divinity). While the overall model fits were reasonably good (see supplements), some of the internal consistencies of these early versions of the scale were low. In addition, some items had weak loadings on the latent factors. Correlation matrices of all items in the second version revealed that some items that were written to represent one foundation actually related more strongly to another foundation, and some items correlated so highly with each other that they appeared redundant. Problematic items were replaced with new items in the third version of the scale. Pilot testing for the third version of the MFQ was extensive, involving data from over 28,000 participants (surveyed at YourMorals.org) and external validations with eight conceptually related scales. Item and factor analyses on this third version showed improvements over versions 1 and 2. No subscale had more than one item that loaded poorly on the latent factor, so we focused our item analyses on determining how reducing from 40 to 30, 20, or even 10 items would impact the validity of the scale. We developed a novel method for empirically selecting item combinations that would maximize both internal and external validity. For all 10 subscales (four relevance items or four judgments items for each of the five foundations) we identified three criterion scales. The first criterion scale for each subscale was the corresponding foundation subscale using the alternative format (for instance, Harm-judgments as internal validity criterion for Harmrelevance). The second criterion subscale was the corresponding foundation subscale from the "taboo-tradeoffs" questionnaire (see Graham, et al., 2009, Study 3, Appendix C). Modeled on work by Tetlock, Kristel, Elson, Green, and Lerner (2000), this questionnaire asks participants to indicate how much money they would require to perform actions that violate the foundations in a variety of ways (for instance, "kick a dog in the head, hard" for Harm), including options of performing the violation for free, as well as refusing to perform it for any amount of money. The third criterion (used for both relevance and judgments subscales) was an external scale we expected to be related to one particular moral foundation. For Harm the external criterion was the empathy subscale of the Interpersonal Reactivity Index (Davis, 1983); for Fairness, the Schwartz Equality value item (Schwartz, 1992); for Ingroup, the Schwartz Loyalty value item (Schwartz, 1992); for Authority, the Right-Wing Authoritarianism scale (Zakrisson, 2005); and for Purity, the Disgust ScaleRevised (Haidt, McCauley, & Rozin, 1994, modified by Olatunji et al., 2008. In order to quantify how the quality of the scale would decline if shortened, and in order to select the combination of items that would retain the greatest internal and external validity, we calculated correlations between every combination of items and the three criterion scales, as well as a combined item-total correlation with all three criterion scales. An example for Harm-Relevance is shown in Figure 1 (detailed reports of the criterion analyses for all ten subscales can be found in the supplements). The top left panel of Figure 1 plots the corrected item-total correlations with the three criterion scales (Harm-judgments, Harm-tabootradeoffs questionnaire, and IRI-empathy, shown in the other three panels) for every single Harm-relevance item, every 2-item combination, every 3-item combination, and the 4-item aggregate. One can see in this figure that a single item, H ("whether or not someone was harmed") was negatively impacting the internal and external validity of the Harm-relevance aggregates that included it, and that a 3-item aggregate excluding it had even better internal and external reliabilities than the 4-item aggregate. All ten of these analyses revealed that using the best three-item combination yielded internal and external validities as good if not better than using all four items; moreover, we found that in most cases the optimal 2-item combinations were nearly as good as the 3-item combinations (although the 3-item combinations were preferable for their broader conceptual coverage). The best three items from each subscale were retained for the fourth and final version of the MFQ, shown in the Appendix. The 20 starred items make up the short-form MFQ.Throughout the several rounds of item generation and selection, we sought to minimize conceptual and empirical redundancy among items. On this point, we differ from some approaches to scale development that prioritize high internal consistency. Internal consistency is important, but so is comprehensive coverage of the various facets of the construct (the scale development work of Harrison Gough [1979, 1984]  is a good example of this balance; see also John & Soto, 2007). From our point of view, it is better to have dissimilar items that are moderately correlated but that each capture a different facet of a foundation than it is to have similar items that are highly correlated and capture only a small amount of the foundation's scope. As such, our aim in item analysis was not to maximize internal consistency via item redundancy. Instead, we sought a balance between achieving (a) sufficient internal consistency to believe that there was a common core, and (b) maximal item heterogeneity to increase confidence that we were representing the foundation in full. For the moral relevance items, we attempted to cover a wide conceptual area for each foundation while avoiding obvious culture-war issues (e.g., one item is about the moral relevance of rights violations in the abstract, without specifying particular content such as gun rights or gay rights). Items were generated to capture different instances of a foundational moral concern, for instance asking about group loyalty in reference to different specific groups (nation, family) as well as to one's group left in the abstract. We had two reasons for adding the judgments subscale to the relevance subscale. First, we wanted multiple response formats to minimize the impact of variance based on response set (for instance, some people may be more likely to indicate that everything is morally relevant). Second, we wanted to supplement the abstract relevance assessments-which, as self-theories about how one makes moral judgments, may be inaccurate with regard to actual moral judgments (Nisbett & Wilson, 1977)-with contextualized items that could more directly gauge actual moral judgments. To fill out the judgments subscale, participants need not directly consider or be aware of the basis for their moral judgments; they just need to decide whether they endorse or reject the action or event. In this way, the "relevance" subscale may better assess explicit theories about what is morally relevant, and the "judgments" subscale may better assess actual use of moral foundations in judgment (see initial evidence for this conclusion in Graham et al., 2009, Study 2). These judgments took the form of normative declarations (e.g., "It can never be right to kill a human being," for Harm), hypotheticals (e.g., "If I were a soldier and disagreed with my commanding officer's orders, I would obey anyway because that is my duty," for Authority), virtue endorsements (e.g., "Chastity is an important and valuable virtue," for Purity), and opinions about government policies (e.g., "When the government makes laws, the number one principle should be ensuring that everyone is treated fairly," for Fairness). With this variety of both item formats and specific item content, the final version of the MFQ gauges sensitivities to basic kinds of moral concerns, not just opinions on specific moral issues. Sample sizes, number of items, and alphas from pilot testing on all four versions of the MFQ are shown in Table 1. Below we describe validity analyses on the fourth and final version of the scale shown in the Appendix.Participants were 34,476 adults (37% female; mean age 36.2) who had previously registered at YourMorals.org and selected to take the Moral Foundations Questionnaire. Participants come to the website via many different routes, identified using a web tracker. In this sample, 40.5% entered the URL directly into their browser. The most common referring sites were edge.org (29.7%), search engines (8.1%), message boards (3.9%), ted.com (2.2%), and alternet.org (1.9%). (For an analysis demonstrating that response patterns on the MFQ are similar for participants from diverse referring sources, see Iyer, 2009). The relevance section preceded the judgments section, and items within each section were given in an order randomized for each participant. A subset of this sample also chose to take one or several other surveys on the site. This is the sample and procedure used for all internal and external validity analyses except the test-retest, pragmatic, and incremental predictive validity analyses, which are detailed below.Means, standard deviations, and alphas for each subscale of the MFQ are presented in Table  2. Means are also given separately for liberals, moderates, conservatives, and libertarians, to Relations between the relevance and judgments subscales are shown in Table 3. The top panel shows zero-order correlations. To ensure that these relationships were not solely driven by common relations to political ideology, the bottom panel shows partial correlations controlling for politics. Both panels show convergent validity for each foundation as measured by the two formats, as well as discriminant validity in that these relations are stronger than relations between different foundations, despite high correlations among many of the foundations (average same-foundation r = .48, average differentfoundation r = .14).2We gave the MFQ to 123 college students (mean age = 20.1, 69.9% female) from the University of Southern California. After an average interval of 37.4 days (range 28 to 43 days), participants completed the MFQ a second time. In both instances, the MFQ was administered via a class website which recorded the date and time of completion automatically. Question order was randomized for each participant each time.  Table 2. This suggests that item responses are quite stable over time and that within-occasion variation is more a function of the broad diversity of measurement rather than instability.Although Moral Foundations Theory predicts a specific factor structure for moral concerns, we began with exploratory factor analyses to see what factors emerged from the items in the absence of conceptual constraints. Factor analysis for all 30 items of the MFQ was performed using direct oblimin rotation with Kaiser normalization (allowing the factors to be correlated) and maximum likelihood estimation (Fabrigar, Wegener, McCallum, &  Strahan, 1999). Six factors with eigenvalues greater than 1 emerged, but scree plot and factor loadings indicated that only the first two factors provided meaningful incremental explanatory power and interpretability (only 2 of the 120 loadings on the last four factors were above .3; see also Costello & Osborne, 2005, on factor retention from scree plot analysis). The first two factors were retained, and are shown in Table 4. As the table indicates, the two factors clearly corresponded to the binding foundations (Ingroup, Authority, and Purity; factor 1) and individualizing foundations (Harm and Fairness; factor 2), and the strongest loading for all 30 items was as predicted. Although this analysis supported our distinction between individual-focused and groupfocused moral concerns, it remains an open question whether we are justified in treating the theoretically-derived moral foundations as five factors, rather than two. To answer this question we turn to comparisons between different confirmatory factor analysis models. 2 Correlations between the latent foundation factors can be found in Figure 2 (five correlated factors model), and zero-order correlations between the factors are available in a supplement at the first three authors' websites.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author ManuscriptThe large sample sizes we obtained for each version of the MFQ allowed us to create structural equation models comparing different theoretically-derived factor structures. Table  5 describes the comparative model fitting with the final version of the MFQ. The first three numerical columns provide fit statistics for the individual models, and the last two columns show the degree to which each model was an improvement over the model in the row above it. Figure 2 shows the different confirmatory factor analysis models for the full scale; as Table 5 reflects, the same models were constructed for the relevance and judgments subscales separately as well. In the first step, we compared nested first-order models. Our hypothesis was that model 4 (five correlated factors: Harm, Fairness, Ingroup, Authority, and Purity) would provide a better overall model fit than a single morality factor model (1), two-factor model (2: individualizing and binding, corresponding to the results of the exploratory factor analysis), and three-factor model (3: corresponding to Shweder et al.'s (1997) ethics of autonomy, community, and divinity). All three tests (relevance subscale, judgments subscale, and full MFQ) confirmed these predictions; the overall best model (weighing fit and parsimony) was the five-factor model in every case.3 In the second step, we tested whether the five factors could be more parsimoniously modeled with two correlated superordinate factors representing our theoretical distinction of "individualizing" and "binding" foundations (see the hierarchical model in Figure 2). As Table 6 shows, however, the model with five intercorrelated factors was a significant improvement (again, weighing both fit and parsimony) over the hierarchical models. In general, confirmatory factor analyses provide robust support for our five-factor conceptualization of the moral foundations.We identified several other scales and scale items also taken by participants at YourMorals.org that we predicted would relate to the MFQ foundation scores. Scales were grouped into five external criteria scale sets, one set for each foundation. Harm criterion scales were the empathic concern subscale of the Interpersonal Reactivity Index (Davis, 1983; n=134), Levenson's (1995) Psychopathy Scale (reverse-scored; n=116), Schwartz's (1992; n=4,228)  Benevolence subscale, and three items from the Adapted Good-Self Assessment (Barriga, Morrison, Liau, & Gibbs, 2001; n=89) on the importance of being kind/caring, sympathetic/compassionate, and generous/giving. Fairness scales were Social Dominance Orientation (reverse-scored, as it measures preference for social inequalities; Pratto, Sidanius, Stallworth, & Malle, 1994; n=1,215), importance of being fair/just on Barriga's Good-Self scale, and endorsement of the social justice item on Schwartz's values scale. Ingroup scales were the importance of being loyal/faithful on Barriga's Good-Self scale, and endorsement of loyalty, national security, and family security items on Schwartz's values scale. Authority scales were Right-Wing Authoritarianism (Zakrisson, 2005;  n=1,093), the Traditionalism subscale of the Progressive and Traditional Justice scale (Haidt, Darley, & Gromet, 2009; n=1,384), and endorsement of the social order, authority, respect for tradition, honoring parents, and obedience values on Schwartz's value scale. Purity scales were the Disgust Scale-Revised (Haidt, McCauley & Rozin, 1994 modified by  Olatunji et al., 2008 n=1,681), self-reported religious attendance (n=32,607), and 3  We also tested whether a six-factor model separating Authority and Tradition (shown in Figure 2) would improve upon the fivefactor model. However, these six-factor models were a worse fit than the five-factor models, 0.60 ≤ ε a ≤ 0.65; in addition, as Figure 2 shows, the Authority and Tradition factors were very highly correlated (r = .96), further supporting a single latent factor for these items.J Pers Soc Psychol. Author manuscript; available in PMC 2012 August 1.NIH-PA Author Manuscript NIH-PA Author Manuscript endorsement of the self-discipline, clean, and devout items on Schwartz's values scale. Items from the same scale were averaged together, and correlations between the foundations and the scales were averaged together for each criterion group. Correlations between the foundations and the external criterion scales are shown in Table 7. As the table shows, each foundation was the strongest predictor for its own conceptually related group of external scales (average r = .51, vs. average r = .14 for the off-diagonals). This provides evidence of both convergent and discriminant validity, despite relatively substantial relations among the foundations.A subset of the participants who took the MFQ also took a survey in which they reported their gut reactions to various social groups. We constructed this survey by first identifying groups conceptually related to each foundation because they represent either virtues or vices of that foundation. For instance, we predicted that people whose morality rested heavily on the Harm/care foundation would have especially positive reactions to "caring" groups such as nurses, and especially negative reactions to "harming" groups like hunters. The foundation relevance of each group was identified a priori by the authors. Harm-related groups were nurses, environmentalists, pacifists, vegetarians, and hunters (r). Fairnessrelated groups were ACLU members, labor unions, rich people (r), and CEOs (r). Ingrouprelated groups were Americans, U.S. Government, flag burners (r), and illegal immigrants (r). Authority-related groups were soldiers, police officers, U.S. Marines, U.S. Military, people who spank their children, and anarchists (r). Purity-related groups were virgins, highly religious people, spiritual people, atheists (r), prostitutes (r), homosexuals (r), people who have casual sex (r), and people with tattoos or piercings (r). Groups indicated by "(r)" represented vices of a foundation and were reverse-scored, and for U.S.-specific groups only U.S. citizens were included in the analyses. Because attitudes toward social groups and moral foundations scores are both related to political ideology, we used partial correlations controlling for political ideology to see which groups would be uniquely predicted by one or more foundations. Partial correlations between foundations and all social groups were averaged for each set of foundation-related groups; these averages are shown in Table 8. As the table shows, each foundation was the strongest predictor (above and beyond politics) of attitudes toward conceptually related social groups, providing further evidence of predictive and discriminant validity. Beyond validation of the scale, these results also suggest that attitudes about social groups are in part moral judgments about those groups. Moral Foundations Theory and the MFQ may be useful for researchers who want to know which moral concerns are related to prejudice toward any particular group.The preceding sections establish that the theorized model of Moral Foundations as five interrelated factors is a better fit than other plausible models, and that each of the five factors predict foundation-relevant outcomes. An open question is whether the MFQ has predictive validity beyond existing measures. Because it measures domains that are not present in other theoretical conceptions of morality-Ingroup, Authority, and Purity-it surely expands the predictive validity of morality measures. However, even broader measures exist, such as the Schwartz Values Scale, which measures endorsement of ten broad classes of values. This scale contains several values and subscale factors that conceptually overlap with the moral foundations (e.g., Benevolence with Harm/care, Traditionalism with Authority/respect), and many self-interested values that we consider to be outside the moral domain and not covered by the MFQ (e.g., Achievement, Hedonism). Even though Moral Foundations Theory has a different conceptual starting point (an evolutionary account of why humans have the moral intuitions they do, contra Schwartz's factor-analytic approach), it is nonetheless worthwhile to test whether the MFQ has predictive validity beyond Schwartz's scale in predicting a NIH-PA Author Manuscript NIH-PA Author Manuscript variety of scales, opinions, and self-reported behaviors relevant to morality. Because Schwartz's scale is larger both in terms of subscales (10 vs. 5) and items (58 vs. 30), this is a particularly tough test of the predictive validity of the MFQ. Data for these analyses came from 10,652 visitors to YourMorals.org who took both the MFQ and the Schwartz Values Scale (SVS), 92% of whom also took other scales or measures. We used two-step regressions to test whether the five MFQ subscales added incremental predictive validity beyond the ten SVS subscales for the external criteria (scales and attitudes toward foundation-related social groups) described above, as well as for positions on a wide range of political issues. Note that this analysis focuses on the incremental validity of the aggregate MFQ in comparison to the aggregate SVS, rather than investigating which of the particular moral concerns predict each criterion variable (for such work, see Koleva, Graham, Iyer, Ditto, & Haidt, 2010). In every analysis, the MFQ made a significant improvement to prediction when added to the SVS (average ΔR² = 8%, all ΔR² significant at p < .001). To provide a point of comparison, we repeated this analysis by adding the 44-item Big Five personality inventory to the SVS, which yielded an average ΔR² of only 2%. R² and ΔR² for the scales and foundation-related social group averages can be found in Table 9. We also investigated R² for the MFQ alone, to further compare its predictive validity with that of the SVS. As the bolded values in Table  9 show, the MFQ was actually a more powerful predictor than the SVS for most of the scales and political issue positions, and all of the social group attitudes. Given that the SVS is a comprehensive, large, and well-validated measure of values, the MFQ is clearing a high bar in providing unique predictive validity for outcomes relevant for moral and political psychology, and for the psychology of prejudice.Because every step of the scale development used large heterogeneous samples, we can be more confident about the MFQ's generalizability than if we had used college students only (Sears, 1986). However, the samples obtained at ProjectImplicit.org and at YourMorals.org are not representative of any national or international population -the current sample is disproportionately from the U.S. (80%), white (87%), male (63%), and educated (mean education between "completed college" and "some graduate school") compared to international or U.S. national averages. Thanks to the large sample sizes, however, we were able to test whether the five-factor model of moral concerns was consistent across national and geographic groups. All participants self-reported their current country of residence, the country in which they grew up (if different), and the age at which they moved (if they grew up in a different country). For participants who reported moving to their current country at age 14 or older, the country in which they reported growing up was used for the cross-cultural analysis. We created 12 location codes, four of which indicated the four nations from which the largest number of participants had come (U.S., Canada, U.K., Australia); the other eight location codes indicated multi-nation regions of the world (i.e. East Asia, Middle East). Model fit information for each location code is shown in Table 10. As the table shows, the five-factor model of the MFQ is a reasonable or good fit (all ε a < .06, all CFI >.7) for all 11 world regions for which we were able to run the fit analyses, providing evidence that the measurement and theory of five foundational moral concerns is not specific to U.S. or Western participants. Notably, although the five-foundation model is a good fit in all these areas, the data shows much cross-cultural variation in the patterns of moral foundation endorsement. Even controlling for politics, age, gender, religious attendance, and education, world region is a significant (ps < .001) predictor of all five foundation scores, indicating  (Rozin, 2006). For instance, the above validation exercises with external scales and social group attitudes showed that many traits and attitudes that don't seem to be about morality on the surface nevertheless show a systematic and theoretically meaningful relationship to moral concerns measured by the MFQ. We present here three additional findings made possible by MFT's broadened definition of morality, and its finer conceptual resolution of morality's components.Using a variety of measures and methods, Graham et al. (2009) showed that liberals value Harm and Fairness concerns more than conservatives, while conservatives value Ingroup, Authority, and Purity concerns more than liberals. The vast majority of these participants, however, came from the United States, leaving open the question of whether these patterns were limited to the particular ideological conflicts of the United States. Table 11 shows correlations between political ideology4 and the five foundations for the different world areas described in the Generalizability section. The correlations indicate that the liberal-conservative patterns found in the U.S. are robust across national and cultural contexts, both in terms of direction (negative correlations [liberals higher] for Harm and Fairness, positive correlations [conservatives higher] for Ingroup, Authority, and Purity) and in terms of magnitude: correlations are consistently strongest for Authority and Purity, and weakest for Harm. This suggests that across cultures, the most intractable political debates are likely to involve concerns related to respect for traditions/authorities and physical/spiritual purity, while the greatest degree of moral commonality may be found in issues related to harm and care. It also reinforces the claim that political ideology can be self-assessed and that the unidimensional left-right construct has some degree of common meaning across societies, despite differences in political party structures and particular national issues (Jost, 2006). 4 Because the terms "liberal" and "conservative" can mean different things in different nations (i.e., the Liberal Party in Australia is actually center-right ideologically), the political identification item on YourMorals.org clarifies to participants that the items from "very liberal" to "very conservative" should be read as "very left-wing" to "very right-wing." Graham et al. 06) compared to Western participants, and were only very slightly more concerned about Harm, Fairness, and Authority (mean differences < .1, ts < 7, ds < .04). The fact that differences are concentrated in Ingroup and Purity makes some sense in light of established cultural differences in collectivism (Triandis, 1995) and the role of purity concerns in daily life and religious practice, particularly in South Asia (Shweder et al., 1997). But it is noteworthy that there are not large differences in Authority, given greater sensitivity to social hierarchy (Power Distance scores) in eastern nations (Hofstede, 2001). The small effect sizes for all the East-West differences suggest that variation within cultures (e.g., by gender or political ideology) will exceed the east-West variations given so much attention in crosscultural research. Here we see that the increased resolution afforded by MFT allows us to find moral differences we would not have been able to find otherwisedifferences that open up intriguing questions for further research. As the effect sizes show, these gender differences were much stronger than the differences between Eastern and Western cultures. The gender patterns make sense in light of previous research on empathy (Davis, 1983), egalitarianism (Arts & Gelissen, 2001), and disgust sensitivity (Druschel & Sherman, 1999), but they also show an important divergence from the political patterns in that Purity is here grouped with Harm and Fairness, rather than Ingroup and Authority. Here too the finer resolution and broadened scope of MFT allowed us to find and describe differences in moral personality not possible before.Moral Foundations Theory (Haidt & Joseph, 2004;Haidt & Graham, 2007) was created by selecting the closest links between evolutionary accounts of human sociality and anthropological accounts of the breadth and variability of the moral domain (see especially Fiske, 1992, andShweder et al., 1997). The findings reported in this article suggest that those anthropologists were right. From a purely descriptive perspective, the domain of morality consists of more than just "prescriptive judgments of justice, rights, and welfare" (Turiel, 1983, p. 3). Furthermore, we found that one does not need to travel to non-Western nations to find this broader conception of morality. In every country and world region we examined, people on the political right placed greater emphasis on concerns about ingroup loyalty, respect for authorities and traditions, and physical/spiritual purity than did people on the political left. The Moral Foundations Questionnaire fills the need for a theoretically-grounded scale covering the full range of human moral concerns. We found substantial evidence that the scale is reliable and valid. The scale is internally consistent (both within and between two question formats) while maintaining conceptual coverage of diverse manifestations of foundation-related concerns. Test-retest analyses showed stability of foundation subscale scores over time. External validations of the MFQ using widely-used scales, as well as attitudes toward conceptually related social groups, showed convergent, discriminant, and predictive validity. Factor analyses confirmed our theoretical parsing of the moral domain into five sets of concerns: the five-factor model fit the data better (weighing both fit and parsimony) than competing models, and this five-factor representation provided a good fit for participants in 11 different world areas. In addition to the scale itself, we expect that the method introduced in this paper (see Figure 1) for empirically selecting items to maximize both internal and external validity will also be of use to researchers. The best existing instrument for assessing moral concerns beyond harm and fairness is the Schwartz Values Scale (SVS; Schwartz, 1992), which includes group-level values such as "tradition" and "conformity." However, the SVS was created to measure a broad spectrum of values; it was not designed to cover the moral domain specifically, and it does not cover concerns about group-loyalty and spiritual purity. In a head-to-head comparison, the MFQ showed incremental predictive validity beyond the SVS for a diverse array of external scales related to moral personality, attitudes toward social groups, and opinions about moral and political issues (see Table 9). Further, in most cases, the MFQ performed even better than the SVS in overall variance explained of criterion variables, despite its shorter length and narrower conceptual coverage. This further illustrates the MFQ's effective measurement properties, balancing relatively short length and wide coverage of the moral domain.The research reported here indicates that the MFQ is a reliable and valid instrument for measuring a broad range of moral concerns. But in the process of developing and validating the MFQ, we also generated a number of substantive discoveries about moral psychology, such as:A map of the moral domain. Because it distinguishes five kinds of moral concerns, and gives separate evolutionary accounts to explain each of their origins (Haidt & Joseph, 2007), MFT is not as parsimonious as theories of morality that try to derive the entire moral domain from one or two principles or processes (usually kin selection plus reciprocal altruism -see Dawkins, 1976;Hauser, 2006;Joyce, 2006). However, comparisons of different structural models revealed that the five-factor solution is an improvement over other theoretically-derived models, even taking into account the relative loss of parsimony. Analyses of international data showed that this five-factor model was a good fit in every area of the world we were able to examine. This provides empirical evidence for MFT's central claim about the structure of human morality, and points toward the usefulness of this added A guide to where the action is. We found some small and easily interpretable crosscultural differences in moral foundation scores: people in Eastern cultures were slightly more likely to value Ingroup and Purity than people in Western cultures. As with all research that relies upon educated participants, our cross-national differences would probably have been much larger if we had found a way to survey rural villagers and the urban poor in Asia. Nonetheless, the cross-national differences we did find were dwarfed by the within-nation (or within-region) differences we examined, including both ideological differences and sex differences (women valued Harm, Fairness, and Purity more than men, even controlling for political ideology). With reliable measures of these different kinds of moral concerns, social and personality psychologists can now begin to examine many such patterns of similarities and dissimilarities, as well as the processes behind them.A method for discovering moral prejudices. The finer resolution offered by the MFQ also revealed the potential role of moral judgment in prejudice. When we examined attitudes toward various social groups, we found that MFQ subscales indicated varying patterns of moral concerns that might lead some people to dislike some groups. This suggests that attitudes toward social groups may often be expressions of moral judgments about those groups -vague intuitions or explicit convictions that a particular social group upholds or violates one or more foundational concerns. The moral foundations can thus be used as a kind of decoder ring, allowing us to see multiple and sometimes unexpected moral threads connecting seemingly unrelated attitudes and opinions (cf. Koleva et al, 2010). This possibility emphasizes our functional definition of morality as a description of what motivates people to suppress selfishness, rather than a prescriptive definition of how one ought to behave. By describing and quantifying a broadened range of human moral concerns, MFT and the MFQ can aid in our understanding of the dangers of morality, as well as the benefits.The map of the moral domain that we offer is provisional. We hope that other researchers will help us improve it. Here are four next steps:]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nihms246870.mp3" length="" type="audio/mpeg"/>
<itunes:duration>8160.47025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/nihms246870.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics</title>
<itunes:title>The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Abstract Notable progress has been made recently on computational models of semantics using vector representations for word meaning Mikolov, Sutskever, Chen, Corrado, & Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model's semantic space. Consistent with human performance (Osgood, Suci, & Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.]]></itunes:summary>
<description><![CDATA[Abstract Notable progress has been made recently on computational models of semantics using vector representations for word meaning Mikolov, Sutskever, Chen, Corrado, & Dean, 2013). As representations of meaning, recent models presumably hone in on plausible organizational principles for meaning. We performed an analysis on the organization of the skip-gram model's semantic space. Consistent with human performance (Osgood, Suci, & Tannenbaum, 1957), the skip-gram model primarily relies on affective distinctions to organize meaning. We showed that the skip-gram model accounts for unique variance in behavioral measures of lexical access above and beyond that accounted for by affective and lexical measures. We also raised the possibility that word frequency predicts behavioral measures of lexical access due to the fact that word use is organized by semantics. Deconstruction of the semantic representations in semantic models has the potential to reveal organizing principles of human semantics.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13423-016-1053-2.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3976.307</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s13423-016-1053-2.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation</title>
<itunes:title>TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called "Turing Test" problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https://turingbench.ist.psu.edu/]]></itunes:summary>
<description><![CDATA[Recent progress in generative language models has enabled machines to generate astonishingly realistic texts. While there are many legitimate applications of such models, there is also a rising need to distinguish machine-generated texts from human-written ones (e.g., fake news detection). However, to our best knowledge, there is currently no benchmark environment with datasets and tasks to systematically study the so-called "Turing Test" problem for neural text generation methods. In this work, we present the TuringBench benchmark environment, which is comprised of (1) a dataset with 200K human- or machine-generated samples across 20 labels {Human, GPT-1, GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3, GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large, FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and (3) a website with leaderboards. Our preliminary experimental results using TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all language models tested, in generating the most human-like indistinguishable texts with the lowest F1 score by five state-of-the-art TT detection models. The TuringBench is available at: https://turingbench.ist.psu.edu/]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.13296v1.TURINGBENCH_A_Benchmark_Environment_for_Turing_Test_in_the_Age_of_Neural_Text_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3000.13725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.13296v1.TURINGBENCH_A_Benchmark_Environment_for_Turing_Test_in_the_Age_of_Neural_Text_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Case Study: Deontological Ethics in NLP</title>
<itunes:title>Case Study: Deontological Ethics in NLP</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.]]></itunes:summary>
<description><![CDATA[Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.04658v2.Case_Study_Deontological_Ethics_in_NLP.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2717.1005</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2010.04658v2.Case_Study_Deontological_Ethics_in_NLP.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation</title>
<itunes:title>WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.]]></itunes:summary>
<description><![CDATA[A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel paradigm for dataset creation based on human and machine collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI, our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers to ensure quality. The resulting dataset, WANLI, consists of 108,357 natural language inference (NLI) examples that present unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI instead of MNLI (which is 4 times larger) improves performance on seven out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI. Moreover, combining MNLI with WANLI is more effective than combining with other augmentation sets that have been introduced. Our results demonstrate the potential of natural language generation techniques to curate NLP datasets of enhanced quality and diversity.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.05955v1.WANLI_Worker_and_AI_Collaboration_for_Natural_Language_Inference_Dataset_Creation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3234.769</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.05955v1.WANLI_Worker_and_AI_Collaboration_for_Natural_Language_Inference_Dataset_Creation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Corpus for Understanding and Generating Moral Stories</title>
<itunes:title>A Corpus for Understanding and Generating Moral Stories</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.]]></itunes:summary>
<description><![CDATA[Teaching morals is one of the most important purposes of storytelling. An essential ability for understanding and writing moral stories is bridging story plots and implied morals. Its challenges mainly lie in: (1) grasping knowledge about abstract concepts in morals, (2) capturing inter-event discourse relations in stories, and (3) aligning value preferences of stories and morals concerning good or bad behavior. In this paper, we propose two understanding tasks and two generation tasks to assess these abilities of machines. We present STORAL, a new dataset of Chinese and English human-written moral stories. We show the difficulty of the proposed tasks by testing various models with automatic and manual evaluation on STORAL. Furthermore, we present a retrieval-augmented algorithm that effectively exploits related concepts or events in training sets as additional guidance to improve performance on these tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.09438v1.A_Corpus_for_Understanding_and_Generating_Moral_Stories.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3328.18275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.09438v1.A_Corpus_for_Understanding_and_Generating_Moral_Stories.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>RankGen: Improving Text Generation with Large Ranking Models</title>
<itunes:title>RankGen: Improving Text Generation with Large Ranking Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.]]></itunes:summary>
<description><![CDATA[Given an input sequence (or prefix), modern language models often assign high probabilities to output sequences that are repetitive, incoherent, or irrelevant to the prefix; as such, model-generated text also contains such artifacts. To address these issues, we present RankGen, an encoder model (1.2B parameters) that scores model generations given a prefix. RankGen can be flexibly incorporated as a scoring function in beam search and used to decode from any pretrained language model. We train RankGen using large-scale contrastive learning to map a prefix close to the ground-truth sequence that follows it and far away from two types of negatives: (1) random sequences from the same document as the prefix, and, which discourage topically-similar but irrelevant generations; (2) sequences generated from a large language model conditioned on the prefix, which discourage repetition and hallucination. Experiments across four different language models (345M-11B parameters) and two domains show that RankGen significantly outperforms decoding algorithms like nucleus, top-k, and typical sampling on both automatic metrics (85.0 vs 77.3 MAUVE) as well as human evaluations with English writers (74.5% human preference over nucleus sampling). Analysis reveals that RankGen outputs are more relevant to the prefix and improve continuity and coherence compared to baselines. We open source our model checkpoints, code, and human preferences with detailed explanations for future research.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09726v1.RankGen_Improving_Text_Generation_with_Large_Ranking_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4381.91025</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09726v1.RankGen_Improving_Text_Generation_with_Large_Ranking_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition</title>
<itunes:title>Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than token-level representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.]]></itunes:summary>
<description><![CDATA[Nested named entity recognition (nested NER) is a fundamental task in natural language processing. Various span-based methods have been proposed to detect nested entities with span representations. However, span-based methods do not consider the relationship between a span and other entities or phrases, which is helpful in the NER task. Besides, span-based methods have trouble predicting long entities due to limited span enumeration length. To mitigate these issues, we present the Propose-and-Refine Network (PnRNet), a two-stage set prediction network for nested NER. In the propose stage, we use a span-based predictor to generate some coarse entity predictions as entity proposals. In the refine stage, proposals interact with each other, and richer contextual information is incorporated into the proposal representations. The refined proposal representations are used to re-predict entity boundaries and classes. In this way, errors in coarse proposals can be eliminated, and the boundary prediction is no longer constrained by the span enumeration length limitation. Additionally, we build multi-scale sentence representations, which better model the hierarchical structure of sentences and provide richer contextual information than token-level representations. Experiments show that PnRNet achieves state-of-the-art performance on four nested NER datasets and one flat NER dataset.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12732v1.Propose_and_Refine_A_Two_Stage_Set_Prediction_Network_for_Nested_Named_Entity_Recognition.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1766.24325</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.12732v1.Propose_and_Refine_A_Two_Stage_Set_Prediction_Network_for_Nested_Named_Entity_Recognition.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language</title>
<itunes:title>ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.]]></itunes:summary>
<description><![CDATA[Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.13048v2.ProofWriter_Generating_Implications_Proofs_and_Abductive_Statements_over_Natural_Language.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3255.84975</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.13048v2.ProofWriter_Generating_Implications_Proofs_and_Abductive_Statements_over_Natural_Language.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Quantifying Blockchain Extractable Value: How dark is the forest?</title>
<itunes:title>Quantifying Blockchain Extractable Value: How dark is the forest?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.   However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.   Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.   Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.]]></itunes:summary>
<description><![CDATA[Permissionless blockchains such as Bitcoin have excelled at financial services. Yet, opportunistic traders extract monetary value from the mesh of decentralized finance (DeFi) smart contracts through so-called blockchain extractable value (BEV). The recent emergence of centralized BEV relayer portrays BEV as a positive additional revenue source. Because BEV was quantitatively shown to deteriorate the blockchain's consensus security, BEV relayers endanger the ledger security by incentivizing rational miners to fork the chain. For example, a rational miner with a 10% hashrate will fork Ethereum if a BEV opportunity exceeds 4x the block reward.   However, related work is currently missing quantitative insights on past BEV extraction to assess the practical risks of BEV objectively. In this work, we allow to quantify the BEV danger by deriving the USD extracted from sandwich attacks, liquidations, and decentralized exchange arbitrage. We estimate that over 32 months, BEV yielded 540.54M USD in profit, divided among 11,289 addresses when capturing 49,691 cryptocurrencies and 60,830 on-chain markets. The highest BEV instance we find amounts to 4.1M USD, 616.6x the Ethereum block reward.   Moreover, while the practitioner's community has discussed the existence of generalized trading bots, we are, to our knowledge, the first to provide a concrete algorithm. Our algorithm can replace unconfirmed transactions without the need to understand the victim transactions' underlying logic, which we estimate to have yielded a profit of 57,037.32 ETH (35.37M USD) over 32 months of past blockchain data.   Finally, we formalize and analyze emerging BEV relay systems, where miners accept BEV transactions from a centralized relay server instead of the peer-to-peer (P2P) network. We find that such relay systems aggravate the consensus layer attacks and therefore further endanger blockchain security.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05511v5.Quantifying_Blockchain_Extractable_Value_How_dark_is_the_forest.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5625.2605</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05511v5.Quantifying_Blockchain_Extractable_Value_How_dark_is_the_forest.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Do Multilingual Language Models Capture Differing Moral Norms?</title>
<itunes:title>Do Multilingual Language Models Capture Differing Moral Norms?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages.]]></itunes:summary>
<description><![CDATA[Massively multilingual sentence representations are trained on large corpora of uncurated data, with a very imbalanced proportion of languages included in the training. This may cause the models to grasp cultural values including moral judgments from the high-resource languages and impose them on the low-resource languages. The lack of data in certain languages can also lead to developing random and thus potentially harmful beliefs. Both these issues can negatively influence zero-shot cross-lingual model transfer and potentially lead to harmful outcomes. Therefore, we aim to (1) detect and quantify these issues by comparing different models in different languages, (2) develop methods for improving undesirable properties of the models. Our initial experiments using the multilingual model XLM-R show that indeed multilingual LMs capture moral norms, even with potentially higher human-agreement than monolingual ones. However, it is not yet clear to what extent these moral norms differ between languages.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09904v1.Do_Multilingual_Language_Models_Capture_Differing_Moral_Norms.mp3" length="" type="audio/mpeg"/>
<itunes:duration>377.23425</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.09904v1.Do_Multilingual_Language_Models_Capture_Differing_Moral_Norms.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer</title>
<itunes:title>Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content -- the two core aspects of the task -- we achieve a new state-of-the-art.]]></itunes:summary>
<description><![CDATA[Scarcity of parallel data causes formality style transfer models to have scarce success in preserving content. We show that fine-tuning pre-trained language (GPT-2) and sequence-to-sequence (BART) models boosts content preservation, and that this is possible even with limited amounts of parallel data. Augmenting these models with rewards that target style and content -- the two core aspects of the task -- we achieve a new state-of-the-art.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06947v2.Thank_you_BART_Rewarding_Pre_Trained_Models_Improves_Formality_Style_Transfer.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1281.515</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06947v2.Thank_you_BART_Rewarding_Pre_Trained_Models_Improves_Formality_Style_Transfer.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
<itunes:title>Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.]]></itunes:summary>
<description><![CDATA[We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2476.61725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Designing for Engaging with News using Moral Framing towards Bridging Ideological Divides</title>
<itunes:title>Designing for Engaging with News using Moral Framing towards Bridging Ideological Divides</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Society is showing signs of strong ideological polarization. When pushed to seek perspectives different from their own, people often reject diverse ideas or find them unfathomable. Work has shown that framing controversial issues using the values of the audience can improve understanding of opposing views. In this paper, we present our work designing systems for addressing ideological division through educating U.S. news consumers to engage using a framework of fundamental human values known as Moral Foundations. We design and implement a series of new features that encourage users to challenge their understanding of opposing views, including annotation of moral frames in news articles, discussion of those frames via inline comments, and recommendations based on relevant moral frames. We describe two versions of features---the first covering a suite of ways to interact with moral framing in news, and the second tailored towards collaborative annotation and discussion. We conduct a field evaluation of each design iteration with 71 participants in total over a period of 6-8 days, finding evidence suggesting users learned to re-frame their discourse in moral values of the opposing side. Our work provides several design considerations for building systems to engage with moral framing.]]></itunes:summary>
<description><![CDATA[Society is showing signs of strong ideological polarization. When pushed to seek perspectives different from their own, people often reject diverse ideas or find them unfathomable. Work has shown that framing controversial issues using the values of the audience can improve understanding of opposing views. In this paper, we present our work designing systems for addressing ideological division through educating U.S. news consumers to engage using a framework of fundamental human values known as Moral Foundations. We design and implement a series of new features that encourage users to challenge their understanding of opposing views, including annotation of moral frames in news articles, discussion of those frames via inline comments, and recommendations based on relevant moral frames. We describe two versions of features---the first covering a suite of ways to interact with moral framing in news, and the second tailored towards collaborative annotation and discussion. We conduct a field evaluation of each design iteration with 71 participants in total over a period of 6-8 days, finding evidence suggesting users learned to re-frame their discourse in moral values of the opposing side. Our work provides several design considerations for building systems to engage with moral framing.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/3492861.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4086.622</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/3492861.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Why GANs are overkill for NLP</title>
<itunes:title>Why GANs are overkill for NLP</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This work offers a novel theoretical perspective on why, despite numerous attempts, adversarial approaches to generative modeling (e.g., GANs) have not been as popular for certain generation tasks, particularly sequential tasks such as Natural Language Generation, as they have in others, such as Computer Vision. In particular, on sequential data such as text, maximum-likelihood approaches are significantly more utilized than GANs. We show that, while it may seem that maximizing likelihood is inherently different than minimizing distinguishability, this distinction is largely artificial and only holds for limited models. We argue that minimizing KL-divergence (i.e., maximizing likelihood) is a more efficient approach to effectively minimizing the same distinguishability criteria that adversarial models seek to optimize. Reductions show that minimizing distinguishability can be seen as simply boosting likelihood for certain families of models including n-gram models and neural networks with a softmax output layer. To achieve a full polynomial-time reduction, a novel next-token distinguishability model is considered.]]></itunes:summary>
<description><![CDATA[This work offers a novel theoretical perspective on why, despite numerous attempts, adversarial approaches to generative modeling (e.g., GANs) have not been as popular for certain generation tasks, particularly sequential tasks such as Natural Language Generation, as they have in others, such as Computer Vision. In particular, on sequential data such as text, maximum-likelihood approaches are significantly more utilized than GANs. We show that, while it may seem that maximizing likelihood is inherently different than minimizing distinguishability, this distinction is largely artificial and only holds for limited models. We argue that minimizing KL-divergence (i.e., maximizing likelihood) is a more efficient approach to effectively minimizing the same distinguishability criteria that adversarial models seek to optimize. Reductions show that minimizing distinguishability can be seen as simply boosting likelihood for certain families of models including n-gram models and neural networks with a softmax output layer. To achieve a full polynomial-time reduction, a novel next-token distinguishability model is considered.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09838v1.Why_GANs_are_overkill_for_NLP.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2475.04975</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2205.09838v1.Why_GANs_are_overkill_for_NLP.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Pinto Fires and Personal Ethics: A Script Analysis of Missed Opportunities</title>
<itunes:title>Pinto Fires and Personal Ethics: A Script Analysis of Missed Opportunities</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[ABSTRACT. This article details the personal involvement of the author in the early stages of the infamous Pinto fire case. The paper first presents an insider account of the context and decision environment within which he failed to initiate an early recall of defective vehicles. A cognitive script analysis of the personal experience is then offered as an explanation of factors that led to a decision that now is commonly seen as a definitive study in uuethicaI corporate behavior. The main analytical thesis is that script schemas that were guiding cognition and action at the time precluded consideration of issues in ethical terms because the scripts did not include ethical dimensions.]]></itunes:summary>
<description><![CDATA[ABSTRACT. This article details the personal involvement of the author in the early stages of the infamous Pinto fire case. The paper first presents an insider account of the context and decision environment within which he failed to initiate an early recall of defective vehicles. A cognitive script analysis of the personal experience is then offered as an explanation of factors that led to a decision that now is commonly seen as a definitive study in uuethicaI corporate behavior. The main analytical thesis is that script schemas that were guiding cognition and action at the time precluded consideration of issues in ethical terms because the scripts did not include ethical dimensions.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/BF00870550.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3002.61875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/BF00870550.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Aligning AI With Shared Human Values</title>
<itunes:title>Aligning AI With Shared Human Values</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.]]></itunes:summary>
<description><![CDATA[We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.02275v5.Aligning_AI_With_Shared_Human_Values.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4847.6995</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.02275v5.Aligning_AI_With_Shared_Human_Values.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Harm inflation: Making sense of concept creep</title>
<itunes:title>Harm inflation: Making sense of concept creep</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/10.1080@10463283.2020.1796080.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5220.83275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/10.1080@10463283.2020.1796080.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Exploring the Limits of Natural Language Inference Based Setup for Few-Shot Intent Detection</title>
<itunes:title>Exploring the Limits of Natural Language Inference Based Setup for Few-Shot Intent Detection</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[One of the core components of goal-oriented dialog systems is the task of Intent Detection. Few-shot Learning upon Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works making use of metric-based and optimization-based methods have been proposed, the task is still challenging in large label spaces and much smaller number of shots. Generalized Few-shot learning is more difficult due to the presence of both novel and seen classes during the testing phase. In this work, we propose a simple and effective method based on Natural Language Inference that not only tackles the problem of few shot intent detection, but also proves useful in zero-shot and generalized few shot learning problems. Our extensive experiments on a number of Natural Language Understanding (NLU) and Spoken Language Understanding (SLU) datasets show the effectiveness of our approach. In addition, we highlight the settings in which our NLI based method outperforms the baselines by huge margins.]]></itunes:summary>
<description><![CDATA[One of the core components of goal-oriented dialog systems is the task of Intent Detection. Few-shot Learning upon Intent Detection is challenging due to the scarcity of available annotated utterances. Although recent works making use of metric-based and optimization-based methods have been proposed, the task is still challenging in large label spaces and much smaller number of shots. Generalized Few-shot learning is more difficult due to the presence of both novel and seen classes during the testing phase. In this work, we propose a simple and effective method based on Natural Language Inference that not only tackles the problem of few shot intent detection, but also proves useful in zero-shot and generalized few shot learning problems. Our extensive experiments on a number of Natural Language Understanding (NLU) and Spoken Language Understanding (SLU) datasets show the effectiveness of our approach. In addition, we highlight the settings in which our NLI based method outperforms the baselines by huge margins.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07434v1.Exploring_the_Limits_of_Natural_Language_Inference_Based_Setup_for_Few_Shot_Intent_Detection.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1713.65875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.07434v1.Exploring_the_Limits_of_Natural_Language_Inference_Based_Setup_for_Few_Shot_Intent_Detection.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Exposure to opposing views on social media can increase political polarization</title>
<itunes:title>Exposure to opposing views on social media can increase political polarization</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[There is mounting concern that social media sites contribute to political polarization by creating "echo chambers" that insulate people from opposing views about current events. We surveyed a large sample of Democrats and Republicans who visit Twitter at least three times each week about a range of social policy issues. One week later, we randomly assigned respondents to a treatment condition in which they were offered financial incentives to follow a Twitter bot for 1 month that exposed them to messages from those with opposing political ideologies (e.g., elected officials, opinion leaders, media organizations, and nonprofit groups). Respondents were resurveyed at the end of the month to measure the effect of this treatment, and at regular intervals throughout the study period to monitor treatment compliance. We find that Republicans who followed a liberal Twitter bot became substantially more conservative posttreatment. Democrats exhibited slight increases in liberal attitudes after following a conservative Twitter bot, although these effects are not statistically significant. Notwithstanding important limitations of our study, these findings have significant implications for the interdisciplinary literature on political polarization and the emerging field of computational social science. political polarization | computational social science | social networks | social media | sociology P olitical polarization in the United States has become a central focus of social scientists in recent decades (1-7). Americans are deeply divided on controversial issues such as inequality, gun control, and immigration-and divisions about such issues have become increasingly aligned with partisan identities in recent years (8, 9). Partisan identification now predicts preferences about a range of social policy issues nearly three times as well as any other demographic factor-such as education or age (10). These partisan divisions not only impede compromise in the design and implementation of social policies but also have farreaching consequences for the effective function of democracy more broadly (11-15). America's cavernous partisan divides are often attributed to "echo chambers," or patterns of information sharing that reinforce preexisting political beliefs by limiting exposure to opposing political views (16)(17)(18)(19)(20). Concern about selective exposure to information and political polarization has increased in the age of social media (16,(21)(22)(23). The vast majority of Americans now visit a social media site at least once each day, and a rapidly growing number of them list social media as their primary source of news (24). Despite initial optimism that social media might enable people to consume more heterogeneous sources of information about current events, there is growing concern that such forums exacerbate political polarization because of social network homophily, or the well-documented tendency of people to form social network ties to those who are similar to themselves (25, 26). The endogenous relationship between social network formation and political attitudes also creates formidable challenges for the study of social media echo chambers and political polarization, since it is notoriously difficult to establish whether social media networks shape political opinions, or vice versa (27)(28)(29). Here, we report the results of a large field experiment designed to examine whether disrupting selective exposure to partisan information among Twitter users shapes their political attitudes. Our research is governed by three preregistered hypotheses. The first hypothesis is that disrupting selective exposure to partisan information will decrease political polarization because of intergroup contact effects. A vast literature indicates contact between opposing groups can challenge stereotypes that develop in the absence of positive interactions between them (30). Studies also indicate intergroup contact increases the likelihood of deliberation and political compromise (31-33). However, all of these previous studies examine interpersonal contact between members of rival groups. In contrast, our experiment creates virtual contact between members of the public and opinion leaders from the opposing political party on a social media site. It is not yet known whether such virtual contact creates the Significance Social media sites are often blamed for exacerbating political polarization by creating "echo chambers" that prevent people from being exposed to information that contradicts their preexisting beliefs. We conducted a field experiment that offered a large group of Democrats and Republicans financial compensation to follow bots that retweeted messages by elected officials and opinion leaders with opposing political views. Republican participants expressed substantially more conservative views after following a liberal Twitter bot, whereas Democrats' attitudes became slightly more liberal after following a conservative Twitter bot-although this effect was not statistically significant. Despite several limitations, this study has important implications for the emerging field of computational social science and ongoing efforts to reduce political polarization online.]]></itunes:summary>
<description><![CDATA[There is mounting concern that social media sites contribute to political polarization by creating "echo chambers" that insulate people from opposing views about current events. We surveyed a large sample of Democrats and Republicans who visit Twitter at least three times each week about a range of social policy issues. One week later, we randomly assigned respondents to a treatment condition in which they were offered financial incentives to follow a Twitter bot for 1 month that exposed them to messages from those with opposing political ideologies (e.g., elected officials, opinion leaders, media organizations, and nonprofit groups). Respondents were resurveyed at the end of the month to measure the effect of this treatment, and at regular intervals throughout the study period to monitor treatment compliance. We find that Republicans who followed a liberal Twitter bot became substantially more conservative posttreatment. Democrats exhibited slight increases in liberal attitudes after following a conservative Twitter bot, although these effects are not statistically significant. Notwithstanding important limitations of our study, these findings have significant implications for the interdisciplinary literature on political polarization and the emerging field of computational social science. political polarization | computational social science | social networks | social media | sociology P olitical polarization in the United States has become a central focus of social scientists in recent decades (1-7). Americans are deeply divided on controversial issues such as inequality, gun control, and immigration-and divisions about such issues have become increasingly aligned with partisan identities in recent years (8, 9). Partisan identification now predicts preferences about a range of social policy issues nearly three times as well as any other demographic factor-such as education or age (10). These partisan divisions not only impede compromise in the design and implementation of social policies but also have farreaching consequences for the effective function of democracy more broadly (11-15). America's cavernous partisan divides are often attributed to "echo chambers," or patterns of information sharing that reinforce preexisting political beliefs by limiting exposure to opposing political views (16)(17)(18)(19)(20). Concern about selective exposure to information and political polarization has increased in the age of social media (16,(21)(22)(23). The vast majority of Americans now visit a social media site at least once each day, and a rapidly growing number of them list social media as their primary source of news (24). Despite initial optimism that social media might enable people to consume more heterogeneous sources of information about current events, there is growing concern that such forums exacerbate political polarization because of social network homophily, or the well-documented tendency of people to form social network ties to those who are similar to themselves (25, 26). The endogenous relationship between social network formation and political attitudes also creates formidable challenges for the study of social media echo chambers and political polarization, since it is notoriously difficult to establish whether social media networks shape political opinions, or vice versa (27)(28)(29). Here, we report the results of a large field experiment designed to examine whether disrupting selective exposure to partisan information among Twitter users shapes their political attitudes. Our research is governed by three preregistered hypotheses. The first hypothesis is that disrupting selective exposure to partisan information will decrease political polarization because of intergroup contact effects. A vast literature indicates contact between opposing groups can challenge stereotypes that develop in the absence of positive interactions between them (30). Studies also indicate intergroup contact increases the likelihood of deliberation and political compromise (31-33). However, all of these previous studies examine interpersonal contact between members of rival groups. In contrast, our experiment creates virtual contact between members of the public and opinion leaders from the opposing political party on a social media site. It is not yet known whether such virtual contact creates the Significance Social media sites are often blamed for exacerbating political polarization by creating "echo chambers" that prevent people from being exposed to information that contradicts their preexisting beliefs. We conducted a field experiment that offered a large group of Democrats and Republicans financial compensation to follow bots that retweeted messages by elected officials and opinion leaders with opposing political views. Republican participants expressed substantially more conservative views after following a liberal Twitter bot, whereas Democrats' attitudes became slightly more liberal after following a conservative Twitter bot-although this effect was not statistically significant. Despite several limitations, this study has important implications for the emerging field of computational social science and ongoing efforts to reduce political polarization online.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pnas.1804840115.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1896.90775</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/pnas.1804840115.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Autoregressive Search Engines: Generating Substrings as Document Identifiers</title>
<itunes:title>Autoregressive Search Engines: Generating Substrings as Document Identifiers</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.]]></itunes:summary>
<description><![CDATA[Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.10628v1.Autoregressive_Search_Engines_Generating_Substrings_as_Document_Identifiers.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4549.956</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.10628v1.Autoregressive_Search_Engines_Generating_Substrings_as_Document_Identifiers.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Neural Machine Translation with Gumbel-Greedy Decoding</title>
<itunes:title>Neural Machine Translation with Gumbel-Greedy Decoding</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Previous neural machine translation models used some heuristic search algorithms (e.g., beam search) in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.]]></itunes:summary>
<description><![CDATA[Previous neural machine translation models used some heuristic search algorithms (e.g., beam search) in order to avoid solving the maximum a posteriori problem over translation sentences at test time. In this paper, we propose the Gumbel-Greedy Decoding which trains a generative network to predict translation under a trained model. We solve such a problem using the Gumbel-Softmax reparameterization, which makes our generative network differentiable and trainable through standard stochastic gradient methods. We empirically demonstrate that our proposed model is effective for generating sequences of discrete words.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.07518v1.Neural_Machine_Translation_with_Gumbel_Greedy_Decoding.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2177.33225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.07518v1.Neural_Machine_Translation_with_Gumbel_Greedy_Decoding.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems</title>
<itunes:title>The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic]]></itunes:summary>
<description><![CDATA[Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user's trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot's reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models' implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.03021v1.The_Moral_Integrity_Corpus_A_Benchmark_for_Ethical_Dialogue_Systems.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3364.284</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.03021v1.The_Moral_Integrity_Corpus_A_Benchmark_for_Ethical_Dialogue_Systems.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Identifying Automatically Generated Headlines using Transformers</title>
<itunes:title>Identifying Automatically Generated Headlines using Transformers</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8% of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7%, indicating that content generated from language models can be filtered out accurately.]]></itunes:summary>
<description><![CDATA[False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8% of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7%, indicating that content generated from language models can be filtered out accurately.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.13375v3.Identifying_Automatically_Generated_Headlines_using_Transformers.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1225.63925</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2009.13375v3.Identifying_Automatically_Generated_Headlines_using_Transformers.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</title>
<itunes:title>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.]]></itunes:summary>
<description><![CDATA[Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2503.41875</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Aspect-Controlled Neural Argument Generation</title>
<itunes:title>Aspect-Controlled Neural Argument Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we train a language model for argument generation that can be controlled on a fine-grained level to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that our generation model is able to generate high-quality, aspect-specific arguments. Moreover, these arguments can be used to improve the performance of stance detection models via data augmentation and to generate counter-arguments. We publish all datasets and code to fine-tune the language model.]]></itunes:summary>
<description><![CDATA[We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we train a language model for argument generation that can be controlled on a fine-grained level to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that our generation model is able to generate high-quality, aspect-specific arguments. Moreover, these arguments can be used to improve the performance of stance detection models via data augmentation and to generate counter-arguments. We publish all datasets and code to fine-tune the language model.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2005.00084v1.Aspect_Controlled_Neural_Argument_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2983.915</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2005.00084v1.Aspect_Controlled_Neural_Argument_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Multitasking Framework for Unsupervised Simple Definition Generation</title>
<itunes:title>Multitasking Framework for Unsupervised Simple Definition Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner's dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.]]></itunes:summary>
<description><![CDATA[The definition generation task can help language learners by providing explanations for unfamiliar words. This task has attracted much attention in recent years. We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers. A significant challenge of this task is the lack of learner's dictionaries in many languages, and therefore the lack of data for supervised training. We explore this task and propose a multitasking framework SimpDefiner that only requires a standard dictionary with complex definitions and a corpus containing arbitrary simple texts. We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders. By jointly training these components, the framework can generate both complex and simple definitions simultaneously. We demonstrate that the framework can generate relevant, simple definitions for the target words through automatic and manual evaluations on English and Chinese datasets. Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87%.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12926v1.Multitasking_Framework_for_Unsupervised_Simple_Definition_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2079.11175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.12926v1.Multitasking_Framework_for_Unsupervised_Simple_Definition_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification</title>
<itunes:title>What do You Mean by Relation Extraction? A Survey on Datasets and Study on Scientific Relation Classification</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taill\'e et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.]]></itunes:summary>
<description><![CDATA[Over the last five years, research on Relation Extraction (RE) witnessed extensive progress with many new dataset releases. At the same time, setup clarity has decreased, contributing to increased difficulty of reliable empirical evaluation (Taill\'e et al., 2020). In this paper, we provide a comprehensive survey of RE datasets, and revisit the task definition and its adoption by the community. We find that cross-dataset and cross-domain setups are particularly lacking. We present an empirical study on scientific Relation Classification across two datasets. Despite large data overlap, our analysis reveals substantial discrepancies in annotation. Annotation discrepancies strongly impact Relation Classification performance, explaining large drops in cross-dataset evaluations. Variation within further sub-domains exists but impacts Relation Classification only to limited degrees. Overall, our study calls for more rigour in reporting setups in RE and evaluation across multiple test sets.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.13516v1.What_do_You_Mean_by_Relation_Extraction_A_Survey_on_Datasets_and_Study_on_Scientific_Relation_Classification.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2557.649</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.13516v1.What_do_You_Mean_by_Relation_Extraction_A_Survey_on_Datasets_and_Study_on_Scientific_Relation_Classification.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction</title>
<itunes:title>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.]]></itunes:summary>
<description><![CDATA[Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2580.55825</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Teaching Temporal Logics to Neural Networks</title>
<itunes:title>Teaching Temporal Logics to Neural Networks</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.]]></itunes:summary>
<description><![CDATA[We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04218v3.Teaching_Temporal_Logics_to_Neural_Networks.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2988.6695</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2003.04218v3.Teaching_Temporal_Logics_to_Neural_Networks.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels</title>
<itunes:title>An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.]]></itunes:summary>
<description><![CDATA[Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates \textit{without labeled examples} and \textit{without direct access to the model}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11364v1.An_Information_theoretic_Approach_to_Prompt_Engineering_Without_Ground_Truth_Labels.mp3" length="" type="audio/mpeg"/>
<itunes:duration>5348.5715</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.11364v1.An_Information_theoretic_Approach_to_Prompt_Engineering_Without_Ground_Truth_Labels.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Star Temporal Classification: Sequence Classification with Partially Labeled Data</title>
<itunes:title>Star Temporal Classification: Sequence Classification with Partially Labeled Data</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.]]></itunes:summary>
<description><![CDATA[We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2381.40075</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>BERTMap: A BERT-based Ontology Alignment System</title>
<itunes:title>BERTMap: A BERT-based Ontology Alignment System</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.]]></itunes:summary>
<description><![CDATA[Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in knowledge integration. Owing to the success of machine learning in many domains, it has been applied in OM. However, the existing methods, which often adopt ad-hoc feature engineering or non-contextual word embeddings, have not yet outperformed rule-based systems especially in an unsupervised setting. In this paper, we propose a novel OM system named BERTMap which can support both unsupervised and semi-supervised settings. It first predicts mappings using a classifier based on fine-tuning the contextual embedding model BERT on text semantics corpora extracted from ontologies, and then refines the mappings through extension and repair by utilizing the ontology structure and logic. Our evaluation with three alignment tasks on biomedical ontologies demonstrates that BERTMap can often perform better than the leading OM systems LogMap and AML.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.02682v3.BERTMap_A_BERT_based_Ontology_Alignment_System.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2397.44</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.02682v3.BERTMap_A_BERT_based_Ontology_Alignment_System.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer</title>
<itunes:title>From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.]]></itunes:summary>
<description><![CDATA[Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKGC/tree/main/GenKGC.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.02113v4.From_Discrimination_to_Generation_Knowledge_Graph_Completion_with_Generative_Transformer.mp3" length="" type="audio/mpeg"/>
<itunes:duration>912.7705</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.02113v4.From_Discrimination_to_Generation_Knowledge_Graph_Completion_with_Generative_Transformer.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes</title>
<itunes:title>Scruples: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.   We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.   A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.]]></itunes:summary>
<description><![CDATA[As AI systems become an increasing part of people's everyday lives, it becomes ever more important that they understand people's ethical norms. Motivated by descriptive ethics, a field of study that focuses on people's descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics.   We introduce Scruples, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks.   A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.09094v2.Scruples_A_Corpus_of_Community_Ethical_Judgments_on_32_000_Real_Life_Anecdotes.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4356.12725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2008.09094v2.Scruples_A_Corpus_of_Community_Ethical_Judgments_on_32_000_Real_Life_Anecdotes.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning</title>
<itunes:title>Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Neural models command state-of-the-art performance across NLP tasks, including ones involving "reasoning". Models claiming to reason about the evidence presented to them should attend to the correct parts of the input avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of models do so?} In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study -- they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over-sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine-tuning the model on perturbed data does not help it overcome the above challenges.]]></itunes:summary>
<description><![CDATA[Neural models command state-of-the-art performance across NLP tasks, including ones involving "reasoning". Models claiming to reason about the evidence presented to them should attend to the correct parts of the input avoiding spurious patterns therein, be self-consistent in their predictions across inputs, and be immune to biases derived from their pre-training in a nuanced, context-sensitive fashion. {\em Do the prevalent *BERT-family of models do so?} In this paper, we study this question using the problem of reasoning on tabular data. Tabular inputs are especially well-suited for the study -- they admit systematic probes targeting the properties listed above. Our experiments demonstrate that a RoBERTa-based model, representative of the current state-of-the-art, fails at reasoning on the following counts: it (a) ignores relevant parts of the evidence, (b) is over-sensitive to annotation artifacts, and (c) relies on the knowledge encoded in the pre-trained language model rather than the evidence presented in its tabular inputs. Finally, through inoculation experiments, we show that fine-tuning the model on perturbed data does not help it overcome the above challenges.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.00578v3.Is_My_Model_Using_The_Right_Evidence_Systematic_Probes_for_Examining_Evidence_Based_Tabular_Reasoning.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3382.7005</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.00578v3.Is_My_Model_Using_The_Right_Evidence_Systematic_Probes_for_Examining_Evidence_Based_Tabular_Reasoning.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</title>
<itunes:title>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.]]></itunes:summary>
<description><![CDATA[Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.02937v1.Last_Layer_Re_Training_is_Sufficient_for_Robustness_to_Spurious_Correlations.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4514.8735</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.02937v1.Last_Layer_Re_Training_is_Sufficient_for_Robustness_to_Spurious_Correlations.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Decoding speech from non-invasive brain recordings</title>
<itunes:title>Decoding speech from non-invasive brain recordings</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.]]></itunes:summary>
<description><![CDATA[Decoding language from brain activity is a long-awaited goal in both healthcare and neuroscience. Major milestones have recently been reached thanks to intracranial devices: subject-specific pipelines trained on invasive brain responses to basic language tasks now start to efficiently decode interpretable features (e.g. letters, words, spectrograms). However, scaling this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we propose a single end-to-end architecture trained with contrastive learning across a large cohort of individuals to predict self-supervised representations of natural speech. We evaluate our model on four public datasets, encompassing 169 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to natural speech. The results show that our model can identify, from 3s of MEG signals, the corresponding speech segment with up to 72.5% top-10 accuracy out of 1,594 distinct segments (and 44% top-1 accuracy), and up to 19.1% out of 2,604 segments for EEG recordings -- hence allowing the decoding of phrases absent from the training set. Model comparison and ablation analyses show that these performances directly benefit from our original design choices, namely the use of (i) a contrastive objective, (ii) pretrained representations of speech and (iii) a common convolutional architecture simultaneously trained across several participants. Together, these results delineate a promising path to decode natural language processing in real time from non-invasive recordings of brain activity.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.12266v1.Decoding_speech_from_non_invasive_brain_recordings.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2668.30375</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2208.12266v1.Decoding_speech_from_non_invasive_brain_recordings.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?</title>
<itunes:title>How Do Your Biomedical Named Entity Recognition Models Generalize to Novel Entities?</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The number of biomedical literature on new biomedical concepts is rapidly increasing, which necessitates a reliable biomedical named entity recognition (BioNER) model for identifying new and unseen entity mentions. However, it is questionable whether existing models can effectively handle them. In this work, we systematically analyze the three types of recognition abilities of BioNER models: memorization, synonym generalization, and concept generalization. We find that although current best models achieve state-of-the-art performance on benchmarks based on overall performance, they have limitations in identifying synonyms and new biomedical concepts, indicating they are overestimated in terms of their generalization abilities. We also investigate failure cases of models and identify several difficulties in recognizing unseen mentions in biomedical literature as follows: (1) models tend to exploit dataset biases, which hinders the models' abilities to generalize, and (2) several biomedical names have novel morphological patterns with weak name regularity, and models fail to recognize them. We apply a statistics-based debiasing method to our problem as a simple remedy and show the improvement in generalization to unseen mentions. We hope that our analyses and findings would be able to facilitate further research into the generalization capabilities of NER models in a domain where their reliability is of utmost importance.]]></itunes:summary>
<description><![CDATA[The number of biomedical literature on new biomedical concepts is rapidly increasing, which necessitates a reliable biomedical named entity recognition (BioNER) model for identifying new and unseen entity mentions. However, it is questionable whether existing models can effectively handle them. In this work, we systematically analyze the three types of recognition abilities of BioNER models: memorization, synonym generalization, and concept generalization. We find that although current best models achieve state-of-the-art performance on benchmarks based on overall performance, they have limitations in identifying synonyms and new biomedical concepts, indicating they are overestimated in terms of their generalization abilities. We also investigate failure cases of models and identify several difficulties in recognizing unseen mentions in biomedical literature as follows: (1) models tend to exploit dataset biases, which hinders the models' abilities to generalize, and (2) several biomedical names have novel morphological patterns with weak name regularity, and models fail to recognize them. We apply a statistics-based debiasing method to our problem as a simple remedy and show the improvement in generalization to unseen mentions. We hope that our analyses and findings would be able to facilitate further research into the generalization capabilities of NER models in a domain where their reliability is of utmost importance.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00160v3.How_Do_Your_Biomedical_Named_Entity_Recognition_Models_Generalize_to_Novel_Entities.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2831.334</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.00160v3.How_Do_Your_Biomedical_Named_Entity_Recognition_Models_Generalize_to_Novel_Entities.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>BERT Busters: Outlier Dimensions that Disrupt Transformers</title>
<itunes:title>BERT Busters: Outlier Dimensions that Disrupt Transformers</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.]]></itunes:summary>
<description><![CDATA[Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06990v2.BERT_Busters_Outlier_Dimensions_that_Disrupt_Transformers.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2280.0195</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.06990v2.BERT_Busters_Outlier_Dimensions_that_Disrupt_Transformers.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Kent Academic Repository Full text document (pdf) Versions of research Citation for published version Link to record in KAR Document Version UNSPECIFIED Moral Perfectionism and Moral Values, Virtues, and Judgments: A Preliminary Investigation</title>
<itunes:title>Kent Academic Repository Full text document (pdf) Versions of research Citation for published version Link to record in KAR Document Version UNSPECIFIED Moral Perfectionism and Moral Values, Virtues, and Judgments: A Preliminary Investigation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Yang Stoeber Wang (2015) PAID.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1623.458</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/Yang Stoeber Wang (2015) PAID.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation</title>
<itunes:title>Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithms remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.]]></itunes:summary>
<description><![CDATA[The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithms remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1804.06609v2.Fast_Lexically_Constrained_Decoding_with_Dynamic_Beam_Allocation_for_Neural_Machine_Translation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2279.236</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1804.06609v2.Fast_Lexically_Constrained_Decoding_with_Dynamic_Beam_Allocation_for_Neural_Machine_Translation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>QuALITY: Question Answering with Long Input Texts, Yes!</title>
<itunes:title>QuALITY: Question Answering with Long Input Texts, Yes!</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).]]></itunes:summary>
<description><![CDATA[To enable building and testing models on long-document comprehension, we introduce QuALITY, a multiple-choice QA dataset with context passages in English that have an average length of about 5,000 tokens, much longer than typical current models can process. Unlike in prior work with passages, our questions are written and validated by contributors who have read the entire passage, rather than relying on summaries or excerpts. In addition, only half of the questions are answerable by annotators working under tight time constraints, indicating that skimming and simple search are not enough to consistently perform well. Current models perform poorly on this task (55.4%) and significantly lag behind human performance (93.5%).]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.08608v1.QuALITY_Question_Answering_with_Long_Input_Texts_Yes.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3701.76</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2112.08608v1.QuALITY_Question_Answering_with_Long_Input_Texts_Yes.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Deep reinforcement learning from human preferences</title>
<itunes:title>Deep reinforcement learning from human preferences</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.]]></itunes:summary>
<description><![CDATA[For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.03741v3.Deep_reinforcement_learning_from_human_preferences.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3256.6595</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1706.03741v3.Deep_reinforcement_learning_from_human_preferences.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Do liberals and conservatives use different moral languages? Two replications and six extensions of Graham, Haidt, and Nosek's (2009) moral text analysis</title>
<itunes:title>Do liberals and conservatives use different moral languages? Two replications and six extensions of Graham, Haidt, and Nosek's (2009) moral text analysis</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[a b s t r a c t Do liberals and conservatives tend to use different moral languages? The Moral Foundations Hypothesis states that liberals rely more on foundations of care/harm and fairness/cheating whereas conservatives rely more on loyalty/betrayal, authority/subversion, and purity/degradation in their moral functioning. In support, Graham, Haidt, and Nosek (2009; Study 4) showed that sermons delivered by liberal and conservative pastors differed as predicted in their moral word usage, except for the loyalty foundation. I present two high-powered replication studies in religious contexts and six extension studies in politics, the media, and organizations to test ideological differences in moral language usage. On average, replication success rate was 30% and effect sizes were 38 times smaller than those in the original study. ]]></itunes:summary>
<description><![CDATA[a b s t r a c t Do liberals and conservatives tend to use different moral languages? The Moral Foundations Hypothesis states that liberals rely more on foundations of care/harm and fairness/cheating whereas conservatives rely more on loyalty/betrayal, authority/subversion, and purity/degradation in their moral functioning. In support, Graham, Haidt, and Nosek (2009; Study 4) showed that sermons delivered by liberal and conservative pastors differed as predicted in their moral word usage, except for the loyalty foundation. I present two high-powered replication studies in religious contexts and six extension studies in politics, the media, and organizations to test ideological differences in moral language usage. On average, replication success rate was 30% and effect sizes were 38 times smaller than those in the original study. ]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0092656619301278-main.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4085.81225</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1-s2.0-S0092656619301278-main.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence</title>
<itunes:title>The Implications of Diverse Human Moral Foundations for Assessing the Ethicality of Artificial Intelligence</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Organizations are making massive investments in artificial intelligence (AI), and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AI-and the AI itself-process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organization's use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the person's moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches (such as moral reframing) for thinking about conflicts in moral judgments concerning AI.]]></itunes:summary>
<description><![CDATA[Organizations are making massive investments in artificial intelligence (AI), and recent demonstrations and achievements highlight the immense potential for AI to improve organizational and human welfare. Yet realizing the potential of AI necessitates a better understanding of the various ethical issues involved with deciding to use AI, training and maintaining it, and allowing it to make decisions that have moral consequences. People want organizations using AI and the AI systems themselves to behave ethically, but ethical behavior means different things to different people, and many ethical dilemmas require trade-offs such that no course of action is universally considered ethical. How should organizations using AI-and the AI itself-process ethical dilemmas where humans disagree on the morally right course of action? Though a variety of ethical AI frameworks have been suggested, these approaches do not adequately address how people make ethical evaluations of AI systems or how to incorporate the fundamental disagreements people have regarding what is and is not ethical behavior. Drawing on moral foundations theory, we theorize that a person will perceive an organization's use of AI, its data procedures, and the resulting AI decisions as ethical to the extent that those decisions resonate with the person's moral foundations. Since people hold diverse moral foundations, this highlights the crucial need to consider individual moral differences at multiple levels of AI. We discuss several unresolved issues and suggest potential approaches (such as moral reframing) for thinking about conflicts in moral judgments concerning AI.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-022-05057-6.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4303.30775</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-022-05057-6.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences</title>
<itunes:title>Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.]]></itunes:summary>
<description><![CDATA[In social settings, much of human behavior is governed by unspoken rules of conduct. For artificial systems to be fully integrated into social environments, adherence to such norms is a central prerequisite. We investigate whether contemporary NLG models can function as behavioral priors for systems deployed in social settings by generating action hypotheses that achieve predefined goals under moral constraints. Moreover, we examine if models can anticipate likely consequences of (im)moral actions, or explain why certain actions are preferable by generating relevant norms. For this purpose, we introduce 'Moral Stories', a crowd-sourced dataset of structured, branching narratives for the study of grounded, goal-oriented social reasoning. Finally, we propose decoding strategies that effectively combine multiple expert models to significantly improve the quality of generated actions, consequences, and norms compared to strong baselines, e.g. though abductive reasoning.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.15738v1.Moral_Stories_Situated_Reasoning_about_Norms_Intents_Actions_and_their_Consequences.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2987.076</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2012.15738v1.Moral_Stories_Situated_Reasoning_about_Norms_Intents_Actions_and_their_Consequences.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Correcting Robot Plans with Natural Language Feedback</title>
<itunes:title>Correcting Robot Plans with Natural Language Feedback</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.]]></itunes:summary>
<description><![CDATA[When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05186v1.Correcting_Robot_Plans_with_Natural_Language_Feedback.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2761.0905</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2204.05186v1.Correcting_Robot_Plans_with_Natural_Language_Feedback.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</title>
<itunes:title>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.]]></itunes:summary>
<description><![CDATA[In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.02177v1.Grokking_Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1779.5395</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.02177v1.Grokking_Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail</title>
<itunes:title>The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.]]></itunes:summary>
<description><![CDATA[Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field's successes, often in response to the field's widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.08300v3.The_Dangers_of_Underclaiming_Reasons_for_Caution_When_Reporting_How_NLP_Systems_Fail.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2660.075</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2110.08300v3.The_Dangers_of_Underclaiming_Reasons_for_Caution_When_Reporting_How_NLP_Systems_Fail.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Triple-to-Text: Converting RDF Triples into High-Quality Natural Languages via Optimizing an Inverse KL Divergence</title>
<itunes:title>Triple-to-Text: Converting RDF Triples into High-Quality Natural Languages via Optimizing an Inverse KL Divergence</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Knowledge base is one of the main forms to represent information in a structured way. A knowledge base typically consists of Resource Description Frameworks (RDF) triples which describe the entities and their relations. Generating natural language description of the knowledge base is an important task in NLP, which has been formulated as a conditional language generation task and tackled using the sequence-to-sequence framework. Current works mostly train the language models by maximum likelihood estimation, which tends to generate lousy sentences. In this paper, we argue that such a problem of maximum likelihood estimation is intrinsic, which is generally irrevocable via changing network structures. Accordingly, we propose a novel Triple-to-Text (T2T) framework, which approximately optimizes the inverse Kullback-Leibler (KL) divergence between the distributions of the real and generated sentences. Due to the nature that inverse KL imposes large penalty on fake-looking samples, the proposed method can significantly reduce the probability of generating low-quality sentences. Our experiments on three real-world datasets demonstrate that T2T can generate higher-quality sentences and outperform baseline models in several evaluation metrics.]]></itunes:summary>
<description><![CDATA[Knowledge base is one of the main forms to represent information in a structured way. A knowledge base typically consists of Resource Description Frameworks (RDF) triples which describe the entities and their relations. Generating natural language description of the knowledge base is an important task in NLP, which has been formulated as a conditional language generation task and tackled using the sequence-to-sequence framework. Current works mostly train the language models by maximum likelihood estimation, which tends to generate lousy sentences. In this paper, we argue that such a problem of maximum likelihood estimation is intrinsic, which is generally irrevocable via changing network structures. Accordingly, we propose a novel Triple-to-Text (T2T) framework, which approximately optimizes the inverse Kullback-Leibler (KL) divergence between the distributions of the real and generated sentences. Due to the nature that inverse KL imposes large penalty on fake-looking samples, the proposed method can significantly reduce the probability of generating low-quality sentences. Our experiments on three real-world datasets demonstrate that T2T can generate higher-quality sentences and outperform baseline models in several evaluation metrics.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01965v1.Triple_to_Text_Converting_RDF_Triples_into_High_Quality_Natural_Languages_via_Optimizing_an_Inverse_KL_Divergence.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2619.68975</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1906.01965v1.Triple_to_Text_Converting_RDF_Triples_into_High_Quality_Natural_Languages_via_Optimizing_an_Inverse_KL_Divergence.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics</title>
<itunes:title>COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.]]></itunes:summary>
<description><![CDATA[Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.11705v2.COLD_Decoding_Energy_based_Constrained_Text_Generation_with_Langevin_Dynamics.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3260.0295</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.11705v2.COLD_Decoding_Energy_based_Constrained_Text_Generation_with_Langevin_Dynamics.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The normative challenge for illusionist views of consciousness</title>
<itunes:title>The normative challenge for illusionist views of consciousness</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Illusionists about phenomenal consciousness claim that phenomenal consciousness does not exist but merely seems to exist. At the same time, it is quite intuitive for there to be some kind of link between phenomenality and value. For example, some situations seem good or bad in virtue of the conscious experiences they feature. Illusionist views of phenomenal consciousness then face what I call the normative challenge. They have to say where they stand regarding the idea that there is a link between phenomenality and value. If they accept that there is such a link, they might be committed to revisionary normative consequences (and some of them may prove to be uncomfortable). If they deny that there is such link, they might avoid revisionary normative consequences (without being guaranteed against them) but then they have to give reasons to deny that such link obtains, which is not a trivial task. The existence of the normative challenge does not show that illusionism is false, but it shows that illusionism might have important consequences in the normative domain, which have to be clarified.]]></itunes:summary>
<description><![CDATA[Illusionists about phenomenal consciousness claim that phenomenal consciousness does not exist but merely seems to exist. At the same time, it is quite intuitive for there to be some kind of link between phenomenality and value. For example, some situations seem good or bad in virtue of the conscious experiences they feature. Illusionist views of phenomenal consciousness then face what I call the normative challenge. They have to say where they stand regarding the idea that there is a link between phenomenality and value. If they accept that there is such a link, they might be committed to revisionary normative consequences (and some of them may prove to be uncomfortable). If they deny that there is such link, they might avoid revisionary normative consequences (without being guaranteed against them) but then they have to give reasons to deny that such link obtains, which is not a trivial task. The existence of the normative challenge does not show that illusionism is false, but it shows that illusionism might have important consequences in the normative domain, which have to be clarified.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/KAMTNC-2v1.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3091.48725</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/KAMTNC-2v1.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>It's a Match: Moralization and the Effects of Moral Foundations Congruence on Ethical and Unethical Leadership Perception</title>
<itunes:title>It's a Match: Moralization and the Effects of Moral Foundations Congruence on Ethical and Unethical Leadership Perception</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[While much research has focused on the effects of ethical and unethical leadership, little is known about how followers come to perceive their leaders as ethical or unethical. In this article, we investigate the co-creation of ethical and unethical leadership perceptions. Specifically, we draw from emerging research on moral congruence in organizational behaviour and empirically investigate the role of congruence in leaders' and followers' moral foundations in followers' perceptions of ethical and unethical leadership. By analysing objective congruence scores from 67 leader-follower dyads by means of polynomial regression with surface response analysis, we find partial support for our theoretically derived predictions. Significant effects were revealed for the fairness, loyalty, and authority moral foundations but not for the care and sanctity moral foundations. We discuss theoretical and practical implications of these findings.]]></itunes:summary>
<description><![CDATA[While much research has focused on the effects of ethical and unethical leadership, little is known about how followers come to perceive their leaders as ethical or unethical. In this article, we investigate the co-creation of ethical and unethical leadership perceptions. Specifically, we draw from emerging research on moral congruence in organizational behaviour and empirically investigate the role of congruence in leaders' and followers' moral foundations in followers' perceptions of ethical and unethical leadership. By analysing objective congruence scores from 67 leader-follower dyads by means of polynomial regression with surface response analysis, we find partial support for our theoretically derived predictions. Significant effects were revealed for the fairness, loyalty, and authority moral foundations but not for the care and sanctity moral foundations. We discuss theoretical and practical implications of these findings.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-019-04178-9.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4337.92</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/s10551-019-04178-9.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Towards mental time travel: a hierarchical memory for reinforcement learning agents</title>
<itunes:title>Towards mental time travel: a hierarchical memory for reinforcement learning agents</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore "mentally time-travel" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.]]></itunes:summary>
<description><![CDATA[Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore "mentally time-travel" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.14039v3.Towards_mental_time_travel_a_hierarchical_memory_for_reinforcement_learning_agents.mp3" length="" type="audio/mpeg"/>
<itunes:duration>4672.88825</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.14039v3.Towards_mental_time_travel_a_hierarchical_memory_for_reinforcement_learning_agents.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Transformers are Sample Efficient World Models</title>
<itunes:title>Transformers are Sample Efficient World Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.]]></itunes:summary>
<description><![CDATA[Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games. Our approach sets a new state of the art for methods without lookahead search, and even surpasses MuZero. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our codebase at https://github.com/eloialonso/iris.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00588v1.Transformers_are_Sample_Efficient_World_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2326.9095</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2209.00588v1.Transformers_are_Sample_Efficient_World_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Conversational Paradigm for Program Synthesis</title>
<itunes:title>A Conversational Paradigm for Program Synthesis</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.]]></itunes:summary>
<description><![CDATA[Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming benchmark (MTPB), where solving each problem requires multi-step synthesis via multi-turn conversation between the user and the model. Our findings show the emergence of conversational capabilities and the effectiveness of the proposed conversational program synthesis paradigm. In addition, our model CodeGen (with up to 16B parameters trained on TPU-v4) outperforms OpenAI's Codex on the HumanEval benchmark. We make the training library JaxFormer including checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13474v3.A_Conversational_Paradigm_for_Program_Synthesis.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3244.8</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13474v3.A_Conversational_Paradigm_for_Program_Synthesis.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Deep Double Descent: Where Bigger Models and More Data Hurt</title>
<itunes:title>Deep Double Descent: Where Bigger Models and More Data Hurt</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.]]></itunes:summary>
<description><![CDATA[We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02292v1.Deep_Double_Descent_Where_Bigger_Models_and_More_Data_Hurt.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2631.654</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1912.02292v1.Deep_Double_Descent_Where_Bigger_Models_and_More_Data_Hurt.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction</title>
<itunes:title>Introducing Neural Bag of Whole-Words with ColBERTer: Contextualized Late Interactions using Enhanced Reduction</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recent progress in neural information retrieval has demonstrated large gains in effectiveness, while often sacrificing the efficiency and interpretability of the neural model compared to classical approaches. This paper proposes ColBERTer, a neural retrieval model using contextualized late interaction (ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier, ColBERTer's reductions dramatically lower ColBERT's storage requirements while simultaneously improving the interpretability of its token-matching scores. To this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and optional lexical matching components into one model. For its multi-vector component, ColBERTer reduces the number of stored vectors per document by learning unique whole-word representations for the terms in each document and learning to identify and remove word representations that are not essential to effective scoring. We employ an explicit multi-task, multi-stage training to facilitate using very small vector dimensions. Results on the MS MARCO and TREC-DL collection show that ColBERTer can reduce the storage footprint by up to 2.5x, while maintaining effectiveness. With just one dimension per token in its smallest setting, ColBERTer achieves index storage parity with the plaintext size, with very strong effectiveness results. Finally, we demonstrate ColBERTer's robustness on seven high-quality out-of-domain collections, yielding statistically significant gains over traditional retrieval baselines.]]></itunes:summary>
<description><![CDATA[Recent progress in neural information retrieval has demonstrated large gains in effectiveness, while often sacrificing the efficiency and interpretability of the neural model compared to classical approaches. This paper proposes ColBERTer, a neural retrieval model using contextualized late interaction (ColBERT) with enhanced reduction. Along the effectiveness Pareto frontier, ColBERTer's reductions dramatically lower ColBERT's storage requirements while simultaneously improving the interpretability of its token-matching scores. To this end, ColBERTer fuses single-vector retrieval, multi-vector refinement, and optional lexical matching components into one model. For its multi-vector component, ColBERTer reduces the number of stored vectors per document by learning unique whole-word representations for the terms in each document and learning to identify and remove word representations that are not essential to effective scoring. We employ an explicit multi-task, multi-stage training to facilitate using very small vector dimensions. Results on the MS MARCO and TREC-DL collection show that ColBERTer can reduce the storage footprint by up to 2.5x, while maintaining effectiveness. With just one dimension per token in its smallest setting, ColBERTer achieves index storage parity with the plaintext size, with very strong effectiveness results. Finally, we demonstrate ColBERTer's robustness on seven high-quality out-of-domain collections, yielding statistically significant gains over traditional retrieval baselines.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13088v1.Introducing_Neural_Bag_of_Whole_Words_with_ColBERTer_Contextualized_Late_Interactions_using_Enhanced_Reduction.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3681.5935</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2203.13088v1.Introducing_Neural_Bag_of_Whole_Words_with_ColBERTer_Contextualized_Late_Interactions_using_Enhanced_Reduction.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>


</channel>
</rss>
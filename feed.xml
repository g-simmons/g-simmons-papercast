<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
<channel>
<title>g-simmons-papercast</title>
<link>https://g-simmons.github.io/g-simmons-papercast/</link>
<language>en-us</language>
<atom:link href="https://g-simmons.github.io/g-simmons-papercast/feed.xml" rel="self" type="application/rss+xml"/>
<copyright>Rights to paper content are reserved by the authors for each paper. I make no claim to ownership or copyright of the content of this podcast.</copyright>
<itunes:subtitle></itunes:subtitle>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:summary>Podcast of text-to-speech for arbitrarily chosen NLP papers.</itunes:summary>
<itunes:keywords>Machine Learning, Natural Language Processing, Artificial Intelligence</itunes:keywords>
<description>Podcast of text-to-speech for arbitrarily chosen NLP papers.</description>
<itunes:owner><itunes:name>Gabriel Simmons</itunes:name><itunes:email>gsimmons@ucdavis.edu</itunes:email></itunes:owner>
<itunes:image href="https://g-simmons.github.io/g-simmons-papercast/cover.jpg"/>
<itunes:category text="Mathematics"></itunes:category>
<itunes:category text="Tech News"></itunes:category>
<itunes:category text="Courses"></itunes:category>


<item>
<title>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
<itunes:title>Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.]]></itunes:summary>
<description><![CDATA[Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2004.245</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/ConnectionistTemporalClassification_Graves_2006.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Structured Prediction as Translation between Augmented Natural Languages</title>
<itunes:title>Structured Prediction as Translation between Augmented Natural Languages</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.]]></itunes:summary>
<description><![CDATA[We propose a new framework, Translation between Augmented Natural Languages (TANL), to solve many structured prediction language tasks including joint entity and relation extraction, nested named entity recognition, relation classification, semantic role labeling, event extraction, coreference resolution, and dialogue state tracking. Instead of tackling the problem by training task-specific discriminative classifiers, we frame it as a translation task between augmented natural languages, from which the task-relevant information can be easily extracted. Our approach can match or outperform task-specific models on all tasks, and in particular, achieves new state-of-the-art results on joint entity and relation extraction (CoNLL04, ADE, NYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and semantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while using the same architecture and hyperparameters for all tasks and even when training a single model to solve all tasks at the same time (multi-task learning). Finally, we show that our framework can also significantly improve the performance in a low-resource regime, thanks to better use of label semantics.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3945.35175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2101.05779v3.Structured_Prediction_as_Translation_between_Augmented_Natural_Languages.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</title>
<itunes:title>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.]]></itunes:summary>
<description><![CDATA[This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1327.62125</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1609.03193v2.Wav2Letter_an_End_to_End_ConvNet_based_Speech_Recognition_System.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Fast and Scalable Structural SVM with Slack Rescaling</title>
<itunes:title>Fast and Scalable Structural SVM with Slack Rescaling</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.]]></itunes:summary>
<description><![CDATA[We present an efficient method for training slack-rescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violating-label in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1172.924</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1510.06002v2.Fast_and_Scalable_Structural_SVM_with_Slack_Rescaling.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Doc2Dict: Information Extraction as Text Generation</title>
<itunes:title>Doc2Dict: Information Extraction as Text Generation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[]]></itunes:summary>
<description><![CDATA[]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2235.06275</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2105.07510v2.Doc2Dict_Information_Extraction_as_Text_Generation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population</title>
<itunes:title>DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.]]></itunes:summary>
<description><![CDATA[We present a new open-source and extensible knowledge extraction toolkit, called DeepKE (Deep learning based Knowledge Extraction), supporting standard fully supervised, low-resource few-shot and document-level scenarios. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured texts according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. Besides, we present an online platform in http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has been equipped with Google Colab tutorials and comprehensive documents for beginners. We release the source code at https://github.com/zjunlp/DeepKE, with a demo video.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1716.8195</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.03335v2.DeepKE_A_Deep_Learning_Based_Knowledge_Extraction_Toolkit_for_Knowledge_Base_Population.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</title>
<itunes:title>Ultra-fine Entity Typing with Indirect Supervision from Natural Language Inference</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.]]></itunes:summary>
<description><![CDATA[The task of ultra-fine entity typing (UFET) seeks to predict diverse and free-form words or phrases that describe the appropriate types of entities mentioned in sentences. A key challenge for this task lies in the large amount of types and the scarcity of annotated data per type. Existing systems formulate the task as a multi-way classification problem and train directly or distantly supervised classifiers. This causes two issues: (i) the classifiers do not capture the type semantics since types are often converted into indices; (ii) systems developed in this way are limited to predicting within a pre-defined type set, and often fall short of generalizing to types that are rarely seen or unseen in training. This work presents LITE, a new approach that formulates entity typing as a natural language inference (NLI) problem, making use of (i) the indirect supervision from NLI to infer type information meaningfully represented as textual hypotheses and alleviate the data scarcity issue, as well as (ii) a learning-to-rank objective to avoid the pre-defining of a type set. Experiments show that, with limited training data, LITE obtains state-of-the-art performance on the UFET task. In addition, LITE demonstrates its strong generalizability, by not only yielding best results on other fine-grained entity typing benchmarks, more importantly, a pre-trained LITE system works well on new data containing unseen types.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3" length="" type="audio/mpeg"/>
<itunes:duration>3168.5485</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.06167v1.Ultra_fine_Entity_Typing_with_Indirect_Supervision_from_Natural_Language_Inference.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>The Quest for a Common Model of the Intelligent Decision Maker</title>
<itunes:title>The Quest for a Common Model of the Intelligent Decision Maker</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.]]></itunes:summary>
<description><![CDATA[The premise of Multi-disciplinary Conference on Reinforcement Learning and Decision Making is that multiple disciplines share an interest in goal-directed decision making over time. The idea of this paper is to sharpen and deepen this premise by proposing a perspective on the decision maker that is substantive and widely held across psychology, artificial intelligence, economics, control theory, and neuroscience, which I call the "common model of the intelligent agent". The common model does not include anything specific to any organism, world, or application domain. The common model does include aspects of the decision maker's interaction with its world (there must be input and output, and a goal) and internal components of the decision maker (for perception, decision-making, internal evaluation, and a world model). I identify these aspects and components, note that they are given different names in different disciplines but refer essentially to the same ideas, and discuss the challenges and benefits of devising a neutral terminology that can be used across disciplines. It is time to recognize and build on the convergence of multiple diverse disciplines on a substantive common model of the intelligent agent.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3" length="" type="audio/mpeg"/>
<itunes:duration>1499.1935</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2202.13252v1.The_Quest_for_a_Common_Model_of_the_Intelligent_Decision_Maker.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>A Fully Differentiable Beam Search Decoder</title>
<itunes:title>A Fully Differentiable Beam Search Decoder</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.]]></itunes:summary>
<description><![CDATA[We introduce a new beam search decoder that is fully differentiable, making it possible to optimize at training time through the inference procedure. Our decoder allows us to combine models which operate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to input sequences by considering all possible alignments between the two. We demonstrate our approach scales by applying it to speech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcriptions. Recent research efforts have shown that deep neural networks with attention-based mechanisms are powerful enough to successfully train an acoustic model from the final transcription, while implicitly learning a language model. Instead, we show that it is possible to discriminatively train an acoustic model jointly with an explicit and possibly pre-trained language model.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2335.11175</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/1902.06022v1.A_Fully_Differentiable_Beam_Search_Decoder.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</title>
<itunes:title>Zero-Shot Information Extraction as a Unified Text-to-Triple Translation</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.]]></itunes:summary>
<description><![CDATA[We cast a suite of information extraction tasks into a text-to-triple translation framework. Instead of solving each task relying on task-specific datasets and models, we formalize the task as a translation between task-specific input text and output triples. By taking the task-specific input, we enable a task-agnostic translation by leveraging the latent knowledge that a pre-trained language model has about the task. We further demonstrate that a simple pre-training task of predicting which relational information corresponds to which input text is an effective way to produce task-specific outputs. This enables the zero-shot transfer of our framework to downstream tasks. We study the zero-shot performance of this framework on open information extraction (OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and factual probe (Google-RE and T-REx). The model transfers non-trivially to most tasks and is often competitive with a fully supervised method without the need for any task-specific training. For instance, we significantly outperform the F1 score of the supervised open information extraction without needing to use its training set.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2476.565</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2109.11171v1.Zero_Shot_Information_Extraction_as_a_Unified_Text_to_Triple_Translation.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</title>
<itunes:title>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.]]></itunes:summary>
<description><![CDATA[Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2503.236</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2108.12472v1.ReGen_Reinforcement_Learning_for_Text_and_Knowledge_Base_Generation_using_Pretrained_Language_Models.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction</title>
<itunes:title>KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization
  for Relation Extraction</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.]]></itunes:summary>
<description><![CDATA[Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in https://github.com/zjunlp/KnowPrompt for reproducibility.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2580.454</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2104.07650v6.KnowPrompt_Knowledge_aware_Prompt_tuning_with_Synergistic_Optimization_for_Relation_Extraction.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>

<item>
<title>Star Temporal Classification: Sequence Classification with Partially Labeled Data</title>
<itunes:title>Star Temporal Classification: Sequence Classification with Partially Labeled Data</itunes:title>
<itunes:author>Gabriel Simmons</itunes:author>
<itunes:subtitle></itunes:subtitle>
<itunes:summary><![CDATA[We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.]]></itunes:summary>
<description><![CDATA[We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.]]></description>
<enclosure url="https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3" length="" type="audio/mpeg"/>
<itunes:duration>2381.40075</itunes:duration>
<itunes:season>1</itunes:season>
<itunes:episode></itunes:episode>
<itunes:episodeType>full</itunes:episodeType>
<guid isPermaLink="false">https://g-simmons.github.io/g-simmons-papercast/data/mp3s/2201.12208v1.Star_Temporal_Classification_Sequence_Classification_with_Partially_Labeled_Data.mp3</guid>
<pubDate></pubDate>
<itunes:explicit>NO</itunes:explicit>
</item>


</channel>
</rss>
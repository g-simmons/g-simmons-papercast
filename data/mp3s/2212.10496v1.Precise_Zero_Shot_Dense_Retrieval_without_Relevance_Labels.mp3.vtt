WEBVTT

00:00:00.000 --> 00:00:04.042
Precise Zero-Shot Dense Retrieval without Relevance Labels

00:00:04.042 --> 00:00:06.934
While dense retrieval has been shown effec-

00:00:06.934 --> 00:00:08.189
1

00:00:08.189 --> 00:00:09.844
Introduction

00:00:09.844 --> 00:00:25.249
Dense retrieval (Lee et al., 2019;Karpukhin et al., 2020), the method of retrieving documents using semantic embedding similarities, has been shown successful across tasks like web search, question answering, and fact verification.

00:00:25.249 --> 00:00:53.579
A variety of methods such as negative mining Qu et al., 2021), distillation (Qu et al., 2021;Lin et al., 2021b;Hofsttter et al., 2021) and task-specific pre-training (Izacard et al., 2021;Gao and Callan, 2021;Lu et al., 2021;Gao and Callan, 2022;Liu and Shao, 2022) have been proposed to improve the effectiveness of supervised dense retrieval models.

00:00:53.579 --> 00:00:57.959
On the other hand, zero-shot dense retrieval still remains difficult.

00:00:57.959 --> 00:01:07.939
Many recent works consider the alternative transfer learning setup, where the dense retrievers are trained on a high-resource dataset and then evaluated on queries from new tasks.

00:01:07.939 --> 00:01:18.332
The MS-MARCO collection (Bajaj et al., 2016), a massive judged dataset with a large number of judged querydocument pairs, is arguably the most commonly used.

00:01:18.332 --> 00:01:20.837
As argued by Izacard et al.

00:01:20.837 --> 00:01:27.504
2021), in practice, however, the existence of such a large dataset cannot always be assumed.

00:01:27.504 --> 00:01:33.984
Even MS-MARCO restricts commercial use and cannot be adopted in a variety of real-world search scenarios.

00:01:33.984 --> 00:01:43.939
In this paper, we aim to build effective fully zero-shot dense retrieval systems that require no relevance supervision, work out-of-box and generalize across tasks.

00:01:43.939 --> 00:01:50.357
As supervision is not available, we start by examining self-supervised representation learning methods.

00:01:50.357 --> 00:01:54.037
Modern deep learning enables two distinct learning algorithms.

00:01:54.037 --> 00:02:18.929
At the token level, generative large language models (LLM) pretrained on large corpus have demonstrated strong natural language understanding (NLU) and generation (NLG) capabilities (Brown et al., 2020;Chen et al., 2021;Rae et al., 2021;Hoffmann et al., 2022;Thoppilan et al., 2022;Chowdhery et al., 2022).

00:02:18.929 --> 00:02:32.259
At the document level, text (chunk) encoders pre-trained with contrastive objectives learn to encode document-document similarity into inner-product (Izacard et al., 2021;Gao and Callan, 2022).

00:02:32.259 --> 00:02:50.452
On top of these, one extra insight into LLM is borrowed: the LLMs further trained to follow instructions can zero-shot generalize to diverse unseen instructions (Ouyang et al., 2022;Sanh et al., 2022;Min et al., 2022;Wei et al., 2022).

00:02:50.452 --> 00:02:51.994
Ouyang et al.

00:02:51.994 --> 00:03:00.787
2022) show that with a small amount of data, GPT-3 (Brown et al., 2020)  to human intent to follow instructions.

00:03:00.787 --> 00:03:17.992
With these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task performed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1).

00:03:17.992 --> 00:03:26.009
First, we feed the query to the generative model and instruct it to "write a document that answers the question", i.e. a hypothetical document.

00:03:26.009 --> 00:03:27.196
We expect the gene

00:00:00.000 --> 00:00:08.892
rative process to capture "relevance" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document.

00:00:08.892 --> 00:00:15.597
In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector.

00:00:15.597 --> 00:00:24.365
Here, we expect the encoder's dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding.

00:00:24.365 --> 00:00:27.782
We use this vector to search against the corpus embeddings.

00:00:27.782 --> 00:00:31.300
The most similar real documents are retrieved and returned.

00:00:31.300 --> 00:00:37.505
The retrieval leverages document-document similarity encoded in the inner-product during contrastive training.

00:00:37.505 --> 00:00:46.147
Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed.

00:00:46.147 --> 00:00:51.415
Instead, the retrieval task is cast into two NLU and NLG tasks.

00:00:51.415 --> 00:00:53.645
HyDE appears unsupervised.

00:00:53.645 --> 00:00:59.625
No model is trained in HyDE: both the generative model and the contrastive encoder remain intact.

00:00:59.625 --> 00:01:04.730
Supervision signals were only involved in instruction learning of our backbone LLM.

00:01:04.730 --> 00:01:30.022
In our experiments, we show HyDE using Instruct-GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly outperforms the previous state-of-the-art Contrieveronly zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese.

00:01:30.022 --> 00:01:31.752
Related Works

00:01:31.752 --> 00:01:44.269
Dense Retrieval (Lee et al., 2019;Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019).

00:01:44.269 --> 00:02:02.312
Researchers studied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sampling Qu et al., 2021), and also introduced distillation (Qu et al., 2021;Lin et al., 2021b;Hofsttter et al., 2021).

00:02:02.312 --> 00:02:18.417
Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021;Gao and Callan, 2021;Lu et al., 2021;Gao and Callan, 2022;Liu and Shao, 2022).

00:02:18.417 --> 00:02:32.222
The popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017).

00:02:32.222 --> 00:02:35.277
Instructions-Following Language Models

00:02:35.277 --> 00:02:55.832
Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022;Sanh et al., 2022;Min et al., 2022;Wei et al., 2022).

00:02:55.832 --> 00:03:04.312
This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022).

00:03:04.312 --> 00:03:07.104
Concurrent to us, Asai et al.

00:03:07.104 --> 00:03:11.459
2022) studied "Task-aware Retrieval with Instructions".

00:03:11.459 --> 00:03:17.202
They fine-tuned dense encoders that can also encode task-specific instruction prepended to query.

00:03:17.202 --> 00:03:23.092
In comparison, we use an unsupervised encoder and handle different tasks and their instruction w

00:00:00.000 --> 00:00:04.517
ith an instruction following generative LLM, as described above.

00:00:04.517 --> 00:00:06.934
Zero-Shot Dense Retrieval

00:00:06.934 --> 00:00:13.252
The tasks of zeroshot (dense) retrieval are arguably empirically defined by Thakur et al.

00:00:13.252 --> 00:00:16.769
2021) for the neural retrieval community.

00:00:16.769 --> 00:00:20.399
Their BEIR benchmark consists of diverse retrieval tasks.

00:00:20.399 --> 00:00:36.579
The paper and many follow-up research generally consider the Transfer Learning setup where the dense retriever is first learned using a diverse and richly supervised corpus and query collection, namely MS-MARCO (Thakur et al., 2021;Yu et al., 2022).

00:00:36.579 --> 00:00:39.759
However, as stated by Izacard et al.

00:00:39.759 --> 00:00:44.052
2021), such a large collection can rarely be assumed.

00:00:44.052 --> 00:00:50.994
In this paper, therefore, we study the problem of building effective dense retrieval systems without relevance labels.

00:00:50.994 --> 00:00:53.412
Similar to Izacard et al.

00:00:53.412 --> 00:00:58.992
2021), we also do not assume access to the test time corpora for training.

00:00:58.992 --> 00:01:03.759
This is a more realistic setup and prevents over-engineering on the test corpora.

00:01:03.759 --> 00:01:06.277
By the definition in Sachan et al.

00:01:06.277 --> 00:01:10.944
2022), our setup can be roughly considered as "unsupervised".

00:01:10.944 --> 00:01:13.637
Strictly, as with Sachan et al.

00:01:13.637 --> 00:01:20.642
2022), the only supervision resides in the LLM, in the processing of learning to follow instructions.

00:01:20.642 --> 00:01:36.022
Generative Retrieval Generative search is a new class of retrieval methods that use neural generative models as search indices (Metzler et al., 2021;Tay et al., 2022;Bevilacqua et al., 2022;Lee et al., 2022).

00:01:36.022 --> 00:01:44.939
These models use (constrained) decoding to generate document identifiers, such as id and sub-string, which map directly to real documents.

00:01:44.939 --> 00:01:57.044
They have to go through special training procedures over relevance data; effective search may also need to use novel forms of search indices (Bevilacqua et al., 2022;Lee et al., 2022).

00:01:57.044 --> 00:02:03.174
In comparison, our method uses the standard MIPS index and requires no training or training data.

00:02:03.174 --> 00:02:10.754
Our generative model produces an intermediate hypothetical document to be fed into a dense encoder, instead of a real document.

00:02:10.754 --> 00:02:12.509
Methodology

00:02:12.509 --> 00:02:17.939
In this section, we first formally define the problem of (zero-shot) dense retrieval.

00:02:17.939 --> 00:02:21.544
Then we will introduce how HyDE is designed to solve it.

00:02:21.544 --> 00:02:23.224
Preliminaries

00:02:23.224 --> 00:02:28.742
Dense retrieval models similarity between query and document with inner product similarity.

00:02:28.742 --> 00:02:40.091
Given a query q and document d, it uses two encoder function enc q and enc d to map them into d dimension vectors v q , v d , whose inner product is used as similarity measurement.

00:02:40.091 --> 00:02:58.246
sim(q, d) = enc q (q), enc d (d) = v q , v d (1) For zero-shot retrieval, we consider L query sets Q 1 , Q 2 , ..., Q L and their corresponding search corpus, document sets D 1 , D 2 , ..., D L .

00:02:58.246 --> 00:03:03.226
Denote the j-th query from i-th set query set Q i as q ij .

00:03:03.226 --> 00:03:12.769
We need to fully define mapping functions enc q and enc d without access to any query set Q i , document set D i , or any relevance judgment r ij .

00:03:12.769 --> 00:03:25.684
The difficulty of zero-shot dense retrieval lies precisely in Equation 1: it requires learning of two embedding functions (for query and document respectively) into the same embedding space where inner product captures relevance.

00:00:00.000 --> 00:00:04.842
Without relevance judgments/scores to fit, learning becomes intractable.

00:00:04.842 --> 00:00:06.172
HyDE

00:00:06.172 --> 00:00:14.527
HyDE circumvents the aforementioned learning problem by performing search in documentonly embedding space that captures documentdocument similarity.

00:00:14.527 --> 00:00:25.069
This can be easily learned using unsupervised contrastive learning (Izacard et al., 2021;Gao et al., 2021;Gao and Callan, 2022).

00:00:25.069 --> 00:00:29.974
We set document encoder enc d directly as a contrastive encoder enc con .

00:00:29.974 --> 00:00:36.179
f = enc d = enc con (2) This function is also denoted as f for simplicity.

00:00:36.179 --> 00:00:41.472
This unsupervised contrastive encoder will be shared by all incoming document corpus.

00:00:41.472 --> 00:00:51.543
v d = f (d) d  D 1  D 2  ...  D L(3) To build the query vector, we consider in addition an instruction following LM, InstructLM.

00:00:51.543 --> 00:00:58.923
It takes a query q and a textual instruction INST and follows them to perform the task specified by INST.

00:00:58.923 --> 00:01:14.790
For simplicity, denote, g(q, INST) = InstructLM(q, INST)(4) Now we can use g to map queries to "hypothetical" documents by sampling from g, setting INST to be "write a paragraph that answers the question".

00:01:14.790 --> 00:01:23.870
The generated document is not real, can and is likely to be ungrounded factually (Brown et al., 2020;Thoppilan et al., 2022).

00:01:23.870 --> 00:01:26.900
We only require it to capture relevance pattern.

00:01:26.900 --> 00:01:31.130
This is done by generating documents, i.e. providing examples.

00:01:31.130 --> 00:01:45.173
Critically, here we offload relevance modeling from representation learning model to an NLG model that generalizes significantly more easily, naturally, and effectively (Brown et al., 2020;Ouyang et al., 2022).

00:01:45.173 --> 00:01:49.965
Generating examples also replaces explicit modeling of relevance scores.

00:01:49.965 --> 00:01:54.333
We can now encode the generated document using the document encoder f .

00:01:54.333 --> 00:02:04.431
Write, E[v q ij ] = E[f (g(q ij , INST i ))](5) Formally, g defines a probability distribution based on the chain rule.

00:02:04.431 --> 00:02:13.748
In this paper, we simply consider the expectation value, assuming the distribution of v q ij is uni-modal, i.e. the query is not ambiguous.

00:02:13.748 --> 00:02:17.978
The study of ambiguous queries and diversity is left to future work.

00:02:17.978 --> 00:02:24.141
We estimate Equation 5 by sampling N documents from g, [d 1 ,d 2 , ...,d N ] .

00:02:24.141 --> 00:02:50.723
v q ij = 1 N d k g(q ij ,INST i ) f (d k ) (6) = 1 N N k=1 f (d k )(7) We also consider the query as a possible hypothesis, v q ij = 1 N + 1 [ N k=1 f (d k ) + f (q ij )](8) Inner product is computed betweenv q ij and the set of all document vectors {f (d)|d  D i }.

00:02:50.723 --> 00:02:53.278
The most similar documents are retrieved.

00:02:53.278 --> 00:03:01.608
Here the encoder function f serves as a lossy compressor that outputs dense vectors, where the extra details are filtered and left out from the vector.

00:03:01.608 --> 00:03:06.713
It further grounds the hypothetical vector to the actual corpus and the real documents.

00:03:06.713 --> 00:03:10.343
The full HyDE system is illustrated in Figure 1.

00:03:10.343 --> 00:03:12.048
Experiments

00:03:12.048 --> 00:03:13.403
Setup

00:03:13.403 --> 00:03:23.545
Implementation We implement HyDE using InstructGPT, a GPT-3 model from the instruct series (text-davinci-003; Ouyang et al.

00:03:23.545 --> 00:03:28.938
2022)) and Contriever models (Izacard et al., 2021).

00:03:28.938 --> 00:03:36.980
We sample from InstructGPT using the OpenAI playground default temperature of 0.7 for open-ended generations.

00:03:36.980 --> 00:03:40.228
We use the English-only Contriever model for English r

00:00:00.000 --> 00:00:04.605
etrieval tasks and multilingual mContriever for non-English tasks.

00:00:04.605 --> 00:00:10.710
We conducted retrieval experiments with the Pyserini toolkit (Lin et al., 2021a).

00:00:10.710 --> 00:00:24.940
Datasets We consider web search query sets TREC DL19 (Craswell et al., 2020a) and DL20 (Craswell et al., 2020b); they are based on the MS-MARCO dataset (Bajaj et al., 2016).

00:00:24.940 --> 00:00:32.332
We also use a diverse collection of 6 low-resource datasets from the BEIR dataset (Thakur et al., 2021).

00:00:32.332 --> 00:00:41.662
For non-English retrieval, we consider Swahili, Korean, Japanese, and Bengali from the Mr.Tydi dataset (Zhang et al., 2021).

00:00:41.662 --> 00:00:44.830
We use different instructions for each dataset.

00:00:44.830 --> 00:00:51.747
They share a similar structure but have different quantifiers to control the exact form of the generated hypothetical documents.

00:00:51.747 --> 00:00:55.777
These instructions can be found in subsection A.1.

00:00:55.777 --> 00:00:57.157
Compared

00:00:57.157 --> 00:01:03.137
Systems Contriever models, Contriever and mContriever, serve as our major baseline.

00:01:03.137 --> 00:01:06.579
They are trained using unsupervised contrastive learning.

00:01:06.579 --> 00:01:10.434
HyDE retrievers share the exact same embedding spaces with them.

00:01:10.434 --> 00:01:13.614
The only difference is how the query vector is built.

00:01:13.614 --> 00:01:17.844
These comparisons allow us to easily examine the effect of HyDE.

00:01:17.844 --> 00:01:23.037
The classical heuristic-based lexical retriever BM25 is also included.

00:01:23.037 --> 00:01:28.954
Several systems that involve fine-tuning on massive relevance data are also included as references.

00:01:28.954 --> 00:01:35.559
We consider models fine-tuned on MS-MARCO and transferred, DPR and ANCE, from the BEIR paper.

00:01:35.559 --> 00:01:44.902
For multilingual, we include the mDPR model from Mr.Tydi paper and MS-MARCO fine-tuned mBERT and XLM-R from the Contriever paper.

00:01:44.902 --> 00:01:55.769
We also include the state-ofthe-art transfer learning models: Contriever and mContriever fine-tuned on MS-MARCO, denoted Contriever FT and mContriever FT .

00:01:55.769 --> 00:02:11.812
These models have run through the state-of-the-art retrieval model training pipeline that involves second-stage retrieval-specific pre-training (Lee et al., 2019) and a few rounds of fine-tuning (Qu et al., 2021); they should be considered empirical upper bounds.

00:02:11.812 --> 00:02:13.379
Web Search

00:02:13.379 --> 00:02:16.859
In  Table 2: Low resource tasks from BEIR.

00:02:16.859 --> 00:02:20.952
Best performing w/o relevance and overall system(s) are marked bold.

00:02:20.952 --> 00:02:23.994
both precision-oriented and recall metrics.

00:02:23.994 --> 00:02:32.549
While unsupervised Contriever can underperform the classical BM25 approach, HyDE outperforms BM25 by large margins.

00:02:32.549 --> 00:02:36.617
HyDE remains competitive even when compared to fine-tuned models.

00:02:36.617 --> 00:02:44.722
Note that TREC DL19/20 are search tasks defined on MS-MARCO and there, all the fine-tuned models are richly supervised.

00:02:44.722 --> 00:02:52.577
On TREC DL19, HyDE shows comparable map and ndcg@10 to Contriever FT and best recall@1k.

00:02:52.577 --> 00:03:00.869
On DL20, HyDE gets around 10% lower map and ndcg@10 than Contriever FT and similar recall@1k.

00:03:00.869 --> 00:03:10.474
The ANCE model shows better ndcg@10 numbers than HyDE but lower recall, suggesting it may be biased to a subset of queries and/or relevant documents.

00:03:10.474 --> 00:03:12.654
Low Resource Retrieval

00:03:12.654 --> 00:03:17.409
In Table 2, we show retrieval results on lowresource tasks from BEIR.

00:03:17.409 --> 00:03:25.464
Similar to web search, HyDE again brings sizable improvements to Contriever across the board in terms of both ndcg and recall.

00:03:25.464 --> 00:03:28.780
HyDE is only outperformed by BM25 on one

00:00:00.000 --> 00:00:08.980
dataset, TREC-Covid but with a tiny 0.2 margin; in comparison, the underlying Contriever underperforms by more than 50%.

00:00:08.980 --> 00:00:14.022
We also observe HyDE demonstrates strong performance compared to fine-tuned models.

00:00:14.022 --> 00:00:24.315
HyDE generally shows better performance than ANCE and DPR, even though the two are fine-tuned on MS-MARCO and ANCE also involves some sophisticated hard negative techniques.

00:00:24.315 --> 00:00:29.457
Contriever FT shows performance advantages on FiQA and DBPedia.

00:00:29.457 --> 00:00:33.650
These involve retrieval of financial posts or entities respectively.

00:00:33.650 --> 00:00:41.554
We believe the performance difference can be attributed to the  under-specification of the instruction; more elaborative instructions may help.

00:00:41.554 --> 00:00:43.847
Multilingual Retrieval

00:00:43.847 --> 00:00:48.014
Multilingual setup poses several additional challenges to HyDE.

00:00:48.014 --> 00:00:57.194
The small-sized contrastive encoder gets saturated as the number of languages scales (Conneau et al., 2020;Izacard et al., 2021).

00:00:57.194 --> 00:01:09.862
Meanwhile, our generative LLM faces an opposite issue: with languages of not as high resource as English or French, the high capacity LLM can get under-trained (Hoffmann et al., 2022).

00:01:09.862 --> 00:01:15.567
Nevertheless, in Table 3, we still find HyDE able to improve the mContriever model.

00:01:15.567 --> 00:01:20.984
It can outperform non-Contriever models fine-tuned on and transferred from MS-MARCO.

00:01:20.984 --> 00:01:27.139
On the other hand, we do observe some margins between HyDE and fine-tuned mContriever FT .

00:01:27.139 --> 00:01:39.507
Since HyDE and mContriever FT use similar contrastive encoders, we hypothesize this is because the non-English languages we considered are under-trained in both pre-training and instruction learning stages.

00:01:39.507 --> 00:01:41.162
Analysis

00:01:41.162 --> 00:01:45.767
The generative LLM and contrastive encoder make up the backbone of HyDE.

00:01:45.767 --> 00:01:49.997
In this section, we study the effect of changing their realizations.

00:01:49.997 --> 00:01:55.789
In particular, we consider smaller language models (LM) and fine-tuned encoders.

00:01:55.789 --> 00:01:59.432
We conduct our studies on TREC DL19/20.

00:01:59.432 --> 00:02:02.099
Effect of Different Generative Models

00:02:02.099 --> 00:02:07.267
In Table 4, we show HyDE using other instruction-following language models.

00:02:07.267 --> 00:02:20.509
In particular, we consider a 52-billion Cohere model (command-xlarge-20221108) and a 11-billion FLAN model (FLAN-T5-xxl;Wei et al.

00:02:20.509 --> 00:02:28.139
2022)  models bring improvement to the unsupervised Contriever, with larger models bringing larger improvements.

00:02:28.139 --> 00:02:34.519
At the time when this paper is written, the Cohere model is still experimental without much detail disclosed.

00:02:34.519 --> 00:02:41.324
We can only tentatively hypothesize that training techniques may have also played some role in the performance difference.

00:02:41.324 --> 00:02:43.829
HyDE with Fine-tuned Encoder

00:02:43.829 --> 00:02:52.747
To begin with, HyDE with fine-tuned encoder is not the intended usage: HyDE is more powerful and irreplaceable when few relevance labels are present.

00:02:52.747 --> 00:02:57.889
Here we are interested to find out if and how HyDE embedding can affect fine-tuned encoders.

00:02:57.889 --> 00:03:05.719
In Table 4, we see that less powerful instruction LMs can negatively impact the overall performance of the fine-tuned retriever.

00:03:05.719 --> 00:03:12.637
To remind our readers, Contriever FT is in-domain supervisedly fine-tuned for TREC DL19/20).

00:03:12.637 --> 00:03:15.479
The performance degradations remain small.

00:03:15.479 --> 00:03:19.816
On the other hand, we also observe the InstructGPT model able to

00:00:00.000 --> 00:00:03.930
further bring up the performance, especially on DL19.

00:00:03.930 --> 00:00:11.072
This suggests that there may still exist certain factors not captured by the fine-tuned encoder but only by the generative model.

00:00:11.072 --> 00:00:12.677
Conclusion

00:00:12.677 --> 00:00:17.907
At the end of the paper, we encourage the readers to take a moment and reflect on the HyDE model.

00:00:17.907 --> 00:00:22.012
Compare it to some of the other recently seen retrievers or re-ranker.

00:00:22.012 --> 00:00:32.342
These other models probably differ in their architecture, training method, and/or task, but probably all of them involve modeling relevance scores between a pair of query and document.

00:00:32.342 --> 00:00:37.984
Dense retrievers consider vector similarities while self-attentive re-rankers regression scores.

00:00:37.984 --> 00:00:45.239
In comparison, the concept of relevance in HyDE is captured by an NLG model and the language generation process.

00:00:45.239 --> 00:00:52.482
We demonstrate in many cases, HyDE can be as effective as dense retrievers that learn to model numerical relevance scores.

00:00:52.482 --> 00:00:57.599
So, is numerical relevance just a statistical artifact of language understanding?

00:00:57.599 --> 00:01:03.679
Will a weak retriever theoretically suffice as the NLU & NLG models rapidly become stronger?

00:01:03.679 --> 00:01:08.447
Rushing to conclusions is not smart; more works need to be done to get answers.

00:01:08.447 --> 00:01:11.889
With this paper, we just want to raise these questions.

00:01:11.889 --> 00:01:19.382
Concretely in this paper, we introduce a new paradigm of interactions between LLM and dense encoder/retriever.

00:01:19.382 --> 00:01:27.399
We demonstrate (part of) relevance modeling and instruction understanding can be delegated to the more powerful and flexible LLM.

00:01:27.399 --> 00:01:31.379
As a consequence, the need for relevance labels is removed.

00:01:31.379 --> 00:01:39.822
We are excited to see how this can be generalized further to more sophisticated tasks like multi-hop retrieval/QA and conversational search.

00:01:39.822 --> 00:01:46.364
We argue HyDE is also of practical use though not necessarily over the entire lifespan of a search system.

00:01:46.364 --> 00:01:56.469
At the very beginning of the life of the search system, serving queries using HyDE offers performance comparable to a fine-tuned model, which no other relevance-free model can offer.

00:01:56.469 --> 00:02:01.712
As the search log grows, a supervised dense retriever can be gradually rolled out.

00:02:01.712 --> 00:02:09.112
As the dense retriever grows stronger, more queries will be routed to it, with only less common and emerging ones going to HyDE backend.
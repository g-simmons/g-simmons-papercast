WEBVTT

00:00:00.000 --> 00:00:04.242
Efficient Methods for Natural Language Processing: A Survey

00:00:04.242 --> 00:00:13.659
Getting the most out of limited resources allows advances in natural language processing (NLP) research and practice while being conservative with resources.

00:00:13.659 --> 00:00:17.639
Those resources may be data, time, storage, or energy.

00:00:17.639 --> 00:00:27.244
Recent work in NLP has yielded interesting results from scaling; however, using only scale to improve results means that resource consumption also scales.

00:00:27.244 --> 00:00:33.799
That relationship motivates research into efficient methods that require less resources to achieve similar results.

00:00:33.799 --> 00:00:43.879
This survey relates and synthesises methods and findings in those efficiencies in NLP, aiming to guide new researchers in the field and inspire the development of new methods.

00:00:43.879 --> 00:00:45.534
Introduction

00:00:45.534 --> 00:00:51.752
Training increasingly large deep learning models has become an emerging trend in the past decade (Fig. 1).

00:00:51.752 --> 00:01:01.257
While the steady increase of model parameters led to state-of-the-art performance and new research directions such as prompting, this also becomes increasingly problematic.

00:01:01.257 --> 00:01:13.049
First, such models often have restricted access, hence are not democratized, or even if so, still require a substantial amount of compute resources to run (Zhan et al., 2021).

00:01:13.049 --> 00:01:20.842
Second, they are not sustainable and require large amounts of energy for training and inference (Schwartz et al., 2020a).

00:01:20.842 --> 00:01:28.609
Third, models cannot be scaled-up indefinitely as their size is limited by the available hardware (Thompson et al., 2020).

00:01:28.609 --> 00:01:34.789
To tackle these limitations, methods that focus on improving efficiency are becoming increasingly popular.

00:01:34.789 --> 00:01:36.094
Definition.

00:01:36.094 --> 00:01:45.224
Efficiency is commonly referred to as the relation between resources going into a system and its output, with an efficient system producing * Equal contribution.

00:01:45.224 --> 00:01:47.792
outputs without a waste of resources.

00:01:47.792 --> 00:02:19.209
For NLP in particular, we consider efficiency as the cost of a model in relation to the results it produces: Cost(R)  E  D  H(1) Equation (1) describes the training cost of an AI model producing a certain (R)esult as proportional to three (non-exhaustive) factors: (1) the cost of model execution on a single (E)xample, (2) the size of the training (D)ataset and (3) the number of training runs required for model selection or (H)yperparameter tuning (Schwartz et al., 2020a).

00:02:19.209 --> 00:02:26.002
The Cost() can then be measured along multiple dimensions such as the computational, time-wise, or environmental cost.

00:02:26.002 --> 00:02:37.019
Each of them can be further quantified in multiple ways; for instance, computational cost may include the total number of floating point operations (FLOPs) or the number of model parameters.

00:02:37.019 --> 00:02:51.699
As using a single cost indicator can be misleading (Dehghani et al., 2021), this survey will collect and organize works on efficient NLP across multiple facets and discuss which dimensions can be beneficial for what use cases and stakeholders.

00:02:51.699 --> 00:02:53.404
Scope of this survey.

00:02:53.404 --> 00:03:00.672
Our goal is to provide a gentle introduction into the broad range of methods that aim to improve efficiency with a focus on NLP.

00:03:00.672 --> 00:03:04.336
We thus structure this survey by following the typical NLP mod

00:00:00.000 --> 00:00:06.367
el pipeline ( Fig. 2) and present the existing methods that aim to make the respective stage more efficient.

00:00:06.367 --> 00:00:17.660
To provide a practical guide to efficiency for NLP researchers, we address this work to two groups of readers: (1) Researchers from all fields of NLP working with limited resources.

00:00:17.660 --> 00:00:24.590
Depending on the bottleneck of resources, readers can directly jump to one of the covered aspects of the NLP pipeline.

00:00:24.590 --> 00:00:32.695
For instance, if the main limitation is to be expected at inference time, the methods described in Section 6 are the most relevant ones.

00:00:32.695 --> 00:00:38.312
2) Researchers interested in improving the stateof-the-art in efficiency methods in NLP.

00:00:38.312 --> 00:00:43.955
Here, the study can serve as an entry point to find opportunities for new research directions.

00:00:43.955 --> 00:00:51.310
To guide the reader, we present a diagram with the typology of efficient NLP methods considered in this survey in Fig. 3.

00:00:51.310 --> 00:01:06.515
Moreover, while hardware choices can have a large impact on the efficiency of models, most NLP researchers do not have direct control over decisions regarding hardware, and most hardware optimizations can be employed swimmingly during all stages of the pipeline.

00:01:06.515 --> 00:01:13.320
We hence focus our work on algorithmic approaches, but provide appropriate pointers regarding hardware in Section 7.

00:01:13.320 --> 00:01:22.062
Finally, we further discuss how to quantify efficiency, what factors to consider during evaluation, and how to decide upon the best suited model.

00:01:22.062 --> 00:01:23.292
Data

00:01:23.292 --> 00:01:29.847
One way to increase efficiency can be to use less training instances and/or to better utilize the available ones.

00:01:29.847 --> 00:01:37.102
In this survey, we focus on approaches that aim to reduce the training data under the assumption that the provided labels are correct.

00:01:37.102 --> 00:01:41.019
1 1 For erroneous labels we refer to Northcutt et al.

00:01:41.019 --> 00:01:43.774
2021); Paullada et al.

00:01:43.774 --> 00:01:46.629
2021); Kreutzer et al.

00:01:46.629 --> 00:01:49.172
2022); Klie et al.

00:01:49.172 --> 00:01:50.977
2022).

00:01:50.977 --> 00:01:52.669
Filtering

00:01:52.669 --> 00:02:01.512
Recent works show that improving data quality can substantially boost the performance while reducing training costs (in contrast to increasing the data quantity).

00:02:01.512 --> 00:02:13.254
For instance, Mishra and Sachdeva (2020) find that using 2% of the SNLI data (Bowman et al., 2015) can achieve comparable performances to using the full data.

00:02:13.254 --> 00:02:14.709
Lee et al.

00:02:14.709 --> 00:02:24.452
2022b) show that removing duplicates during pre-training can already substantially increase training efficiency with equal or even better model performance.

00:02:24.452 --> 00:02:33.407
Similar trends are found in the development process of recent models such as OPT (Zhang et al., 2022) that include a deduplication step.

00:02:33.407 --> 00:02:43.012
Finally, various works focus on better understanding how individual instances contribute towards a model's performance (Swayamdipta et al., 2020).

00:02:43.012 --> 00:02:44.854
Curriculum Learning

00:02:44.854 --> 00:02:55.772
Curriculum learning aims to increase data efficiency by finding a good ordering of the available training instances (Elman, 1993;Bengio et al., 2009).

00:02:55.772 --> 00:02:59.077
Similar trends have been observed by Dodge et al.

00:02:59.077 --> 00:03:02.007
2020) for transformer models.

00:03:02.007 --> 00:03:03.749
Heuristic approaches.

00:03:03.749 --> 00:03:10.367
Many approaches opt for an easy-instances-first ordering by heuristically estimating the instance difficulty.

00:03:10.367 --> 00:03:13.909
For transformer architectures, Platanios et al.

00:03:13.909 --> 00:03:15.460
2019) find t

00:00:00.000 --> 00:00:07.905
hat considering the competence of the model can further improve performance and reduce training time in neural machine translation (NMT).

00:00:07.905 --> 00:00:16.622
Similar results have been observed in natural language understanding (Xu et al., 2020) and question answering (Tay et al., 2019).

00:00:16.622 --> 00:00:19.465
For language modeling, Press et al.

00:00:19.465 --> 00:00:27.645
2021) show that an initial training on short sequences can substantially reduce training time while retaining model performance.

00:00:27.645 --> 00:00:29.312
Agrawal et al.

00:00:29.312 --> 00:00:38.317
2021) further investigate binning training instances based on their complexity and achieve comparable performances with less training steps.

00:00:38.317 --> 00:00:40.359
Multi-task Learning

00:00:40.359 --> 00:00:42.997
T5 (Raffel et al., 2020);

00:00:42.997 --> 00:00:55.577
IA) 3 (Liu et al., 2022a) Zero-shot Learning T0 (Sanh et al., 2022); FLAN (Wei et al., 2022a) Inference Self-paced Learning.

00:00:55.577 --> 00:01:10.494
Instead of using heuristics such as sentence length or word rarity (Platanios et al., 2019;Xu et al., 2020), self-paced learning adaptively selects instances that would be useful for model training (Kumar et al., 2010).

00:01:10.494 --> 00:01:20.912
Self-paced learning has been successfully applied in NMT using the model and data uncertainty  and dialog generation coupled with knowledge distillation (Zhu et al., 2021).

00:01:20.912 --> 00:01:22.292
Zhan et al.

00:01:22.292 --> 00:01:28.409
2021) even propose to learn meta curricula that would transfer well to other domains.

00:01:28.409 --> 00:01:30.077
Active Learning

00:01:30.077 --> 00:01:41.182
Data efficiency can be improved even before training by selectively annotating instances that are most helpful for model training (Settles, 2012;Ren et al., 2021b).

00:01:41.182 --> 00:01:45.474
The key challenge is to assess the helpfulness without knowing the actual label.

00:01:45.474 --> 00:01:52.842
Existing approaches thus often use a model's uncertainty or the underlying instance representation (or both) for sampling.

00:01:52.842 --> 00:02:09.497
Uncertainty-based approaches assume that instances with the highest uncertainty add the most information once labeled (Lewis and Gale, 1994) and focus on good uncertainty estimates (Tang et al., 2002;Gal et al., 2017;Yuan et al., 2020).

00:02:09.497 --> 00:02:24.702
Representation-based approaches instead focus on maximizing the diversity of selected instances (Bod et al., 2011;Sener and Savarese, 2018;Gissin and Shalev-Shwartz, 2019;Kirsch et al., 2019).

00:02:24.702 --> 00:02:39.444
Although various works show the potential of active learning for NLP (Ein-Dor et al., 2020;Yuan et al., 2022), there are still open questions about its generalizability to different tasks and models (Lowell et al., 2019).

00:02:39.444 --> 00:02:56.849
Other issues are outliers in the data that can be harmful for uncertainty-based strategies (Karamcheti et al., 2021) and the potential increase in annotation difficulty and consequently, annotation cost (Settles et al., 2008;Lee et al., 2022a).

00:02:56.849 --> 00:02:58.329
Prompting

00:02:58.329 --> 00:03:12.059
Inspired by human interactions with models such as GPT-3 (Brown et al., 2020), prompting refers to asking the model to perform a predictive task by casting it as a textual input (Liu et al., 2021a).

00:03:12.059 --> 00:03:18.289
The final prediction is then inferred from the output of the language model (Li and Liang, 2021).

00:03:18.289 --> 00:03:26.980
In general, prompts can be either crafted manually or automatically using fill-in templates or prefix strings for token, span, and sentence-level comp

00:00:00.000 --> 00:00:08.942
letion (Petroni et al., 2019;Brown et al., 2020;Shin et al., 2020;Li and Liang, 2021).

00:00:08.942 --> 00:00:19.660
This makes prompting applicable to more challenging NLP tasks, such as question answering, summarization, and machine translation (Schick and Schtze, 2021).

00:00:19.660 --> 00:00:28.552
Since no training nor fine-tuning is required, prompting emerges as an efficient alternative for handling NLP tasks in an unsupervised fashion.

00:00:28.552 --> 00:00:29.682
2

00:00:29.682 --> 00:00:31.424
Model Design

00:00:31.424 --> 00:00:41.967
An active area of research is in designing more efficient models, either by implementing architec-2 See the survey of (Liu et al., 2021a) for more information.

00:00:41.967 --> 00:00:47.134
tural changes or by attaching new modules that accelerate the workflow of the main model.

00:00:47.134 --> 00:00:56.302
In this section we will outline current developments made in transformers, e.g., by adapting its architecture or combining it with external resources.

00:00:56.302 --> 00:00:58.469
Sparse Activations

00:00:58.469 --> 00:01:07.587
As Derczynski (2020) show, the choice (and implementation) of the activation function can make an order of magnitude difference on the execution time.

00:01:07.587 --> 00:01:12.517
To accelerate inference by leveraging sparse activations, Fedus et al.

00:01:12.517 --> 00:01:19.609
2022) propose the Switch Transformer, which routes computation to dedicated specialists ("experts").

00:01:19.609 --> 00:01:37.839
This approach is based on a mixture of experts architecture (Jacobs et al., 1991;Shazeer et al., 2017) and can scale to up to a trillion parameters given enough memory bandwidth, which is often the bottleneck that grows with the number of experts (Rajbhandari et al., 2022).

00:01:37.839 --> 00:01:56.107
Another example of sparse activations is the adaptively sparse transformer model (Correia et al., 2019), which replaces the (dense) softmax activation in attention heads by (sparse) entmax activations, optimally learning the propensity of sparsity of each head automatically from the data.

00:01:56.107 --> 00:02:08.249
Building on this, Sparsefinder (Treviso et al., 2022) allows a more efficient attention mechanism for transformers by identifying the sparsity pattern of entmax attention before computing it.

00:02:08.249 --> 00:02:10.354
Parameter Efficiency

00:02:10.354 --> 00:02:23.972
Some works investigate reducing the number of parameters; for instance, by sharing weights across layers of the model, such as Universal Transformers (Dehghani et al., 2019) and ALBERT (Lan et al., 2019).

00:02:23.972 --> 00:02:32.202
Perceiver (Jaegle et al., 2021) suggests a similar approach, but inserts the original input within any inner layer.

00:02:32.202 --> 00:02:40.457
ALBERT further uses matrix decomposition to reduce the size of the embedding layer, which is one of the largest consumer of model parameters.

00:02:40.457 --> 00:02:51.812
Finally, Subformer (Reid et al., 2021) investigates ways for weight sharing in Transformers, and shows that sharing only the middle layers of the model works better than the alternatives.

00:02:51.812 --> 00:02:54.117
Attention in Transformers

00:02:54.117 --> 00:03:04.822
A limitation of attention mechanisms in transformer models is their quadratic dependency on the sequence length, leading to variants that focus on efficient attention for long-range sequences.

00:03:04.822 --> 00:03:07.364
Retrieval-Augmented Models

00:03:07.364 --> 00:03:18.520
A promising direction in text generation is to combine parametric models with retrieval mechanisms, leading to semi-parametric models (Gu et al., 2018;Lewis et al., 20

00:00:00.000 --> 00:00:01.217
20b).

00:00:01.217 --> 00:00:20.822
4 At inference time, the model retrieves tokens / phrases / sentences from a database, which are then used by the model through interpolation of probability distributions (Khandelwal et al., 2019), gating mechanisms (Yogatama et al., 2021), or attention (Borgeaud et al., 2022.

00:00:20.822 --> 00:00:25.527
This typically amounts to trading model size with the number of database entries.

00:00:25.527 --> 00:00:43.945
E.g., RETRO (Borgeaud et al., 2022) matches the performance of GPT-3, Jurassic-1 (Lieber et al., 2021), and Gopher (Rae et al., 2021) despite having 25 times fewer parameters, by retrieving chunks of tokens from a 2 trillion token database.

00:00:43.945 --> 00:01:01.175
These models also have good generalization properties: by retrieving from domain-specific databases, models can be applied to domains not seen during training (Khandelwal et al., 2019(Khandelwal et al., , 2021, avoiding the need to fine-tune the model for each domain.

00:01:01.175 --> 00:01:06.692
Having an explicit memory also allows retrieval-augmented models to be adapted "onthe-fly".

00:01:06.692 --> 00:01:09.110
For instance, Martins et al.

00:01:09.110 --> 00:01:18.190
2022b) show that adding corrected examples to a database leads to better translations than fine-tuning while reducing the total translation time.

00:01:18.190 --> 00:01:26.170
A downside, however, is that retrieval-augmented models are generally slow, since they need to perform retrieval during inference.

00:01:26.170 --> 00:01:51.649
Several recent works proposed strategies to alleviate this issue, such as pruning the database, having smaller input-dependent databases, reducing the representations dimension, caching information, and reducing the number of retrieval steps (He et al., 2021a;Meng et al., 2022;Wang et al., 2021b;Martins et al., 2022a,b;Alon et al., 2022).

00:01:51.649 --> 00:01:53.254
Pre-training

00:01:53.254 --> 00:02:01.484
Pre-training is a common step in developing NLP models (Peters et al., 2018;Devlin et al., 2019).

00:02:01.484 --> 00:02:13.752
It typically involves a form of self-supervision of large amounts on textual data, such as prediction of masked words (e.g., BERT) or language modeling (e.g., GPT family of models).

00:02:13.752 --> 00:02:19.169
The pre-trained models are subsequently fine-tuned for specific tasks (Section 5).

00:02:19.169 --> 00:02:29.374
In addition to improving performance, the pre-training step can significantly improve efficiency (Peters et al., 2018;Kovaleva et al., 2019).

00:02:29.374 --> 00:02:31.617
For example, He et al.

00:02:31.617 --> 00:02:34.534
2019);Neyshabur et al.

00:02:34.534 --> 00:02:39.864
2020) show that pre-training improves convergence speed on downstream tasks.

00:02:39.864 --> 00:02:51.957
As Fig. 1 shows, the increase in size of these models has been constant for the past several years and has revealed capabilities that only emerge once models become very large (Wei et al., 2022b).

00:02:51.957 --> 00:03:04.937
However, pre-training these increasingly large models is computationally demanding (Strubell et al., 2019;Schwartz et al., 2020a), leading to the important challenge of reducing their costs.

00:03:04.937 --> 00:03:06.892
Dynamic Masking

00:03:06.892 --> 00:03:13.122
The choice of the objective task can determine the success of the pre-trained model when applied on downstream tasks.

00:03:13.122 --> 00:03:19.877
Self-supervised learning objectives have been a key component for pre-training models on large amounts of unlabeled data.

00:03:19.877 --> 00:03:25.324
These objectives vary depending on whether the task is modeled using a decoder, an encoder, o

00:00:00.000 --> 00:00:01.142
r both.

00:00:01.142 --> 00:00:02.685
Decoder only.

00:00:02.685 --> 00:00:23.715
The classic objective function for decoder only models, such as GPT (Radford et al., 2019;Brown et al., 2020) and PaLM (Chowdhery et al., 2022), is the causal language modeling (CLM) objective, which predicts the next word given a prefix using the cross-entropy loss over the whole vocabulary.

00:00:23.715 --> 00:00:25.207
Encoder only.

00:00:25.207 --> 00:00:49.812
A common way for pre-training encoder only models is presented by BERT (Devlin et al., 2019), which uses two objective tasks: (1) The masked language model (MLM) task, aiming at filling randomly masked tokens of a textual input, and (2) the next sentence prediction (NSP) task, with the goal of predicting whether the two random sentences appear consecutively in the training data.

00:00:49.812 --> 00:00:57.630
To make better use of the available data, various works have investigated masking strategies that differ from the static masking used in BERT.

00:00:57.630 --> 00:00:59.997
For instance, Liu et al.

00:00:59.997 --> 00:01:13.140
2019) show that dynamically masking tokens during training-i.e., randomly masking 15% of the tokens at each step instead of masking them once before training-can already improve efficiency with a comparable performance.

00:01:13.140 --> 00:01:20.070
In addition, they show that the NSP objective can be dropped from the pre-training phase in order to get better model performance.

00:01:20.070 --> 00:01:34.200
Other works show that masking specific tokens (such as objects or content words; Bitton et al., 2021) or more tokens (Wettig et al., 2022) leads to higher performance and more efficient use of the available data.

00:01:34.200 --> 00:01:49.330
ELECTRA (Clark et al., 2020) and DeBERTa (He et al., 2021b) experiment with replaced token detection (RTD), a new selfsupervised learning objective that uses a small generator model to replace tokens in the input.

00:01:49.330 --> 00:01:54.897
Both works show that RTD leads to faster and better performing pre-training compared to BERT.

00:01:54.897 --> 00:02:14.564
Encoder-Decoder Another approach, suggested in T5 (Raffel et al., 2020) and BART (Lewis et al., 2020a), uses a denoising sequence-to-sequence objective to pretrain an encoder-decoder LM, allowing the decoder to predict a span of tokens for masked positions rather than a single token.

00:02:14.564 --> 00:02:16.132
Fine-Tuning

00:02:16.132 --> 00:02:21.362
Fine-tuning refers to the step of adapting a pretrained model to a new downstream task.

00:02:21.362 --> 00:02:27.329
In general, fine-tuning specifically refers to gradient-based training on downstream task data.

00:02:27.329 --> 00:02:35.872
In this survey, we use a broader definition of fine-tuning that includes any method used to apply a pre-trained model to a downstream task.

00:02:35.872 --> 00:02:38.402
Parameter-Efficient Fine-Tuning

00:02:38.402 --> 00:02:44.694
Gradient-based fine-tuning typically involves training all of a model's parameters on downstream task data.

00:02:44.694 --> 00:02:52.224
This means that each time a pre-trained model is fine-tuned on a new task, an entirely new set of model parameters is created.

00:02:52.224 --> 00:02:57.579
If a model is fine-tuned on many tasks, the storage requirements can become onerous.

00:02:57.579 --> 00:03:06.909
The seminal ELMo work originally adapted a pre-trained model to downstream tasks by training a new classification layer and leaving the rest of the parameters fixed.

00:03:06.909 --> 00:03:12.004
This approach updates dramatically fewer parameters than training the full model but has been shown to p

00:00:00.000 --> 00:00:05.667
roduce worse performance and has therefore become less common (Devlin et al., 2019).

00:00:05.667 --> 00:00:16.235
An alternative is parameter-efficient fine-tuning (PEFT), which aims to adapt a model to a new task while only updating or adding a relatively small number of parameters.

00:00:16.235 --> 00:00:30.765
Adapters (Houlsby et al., 2019;Bapna and Firat, 2019;Rebuffi et al., 2017), which inject new trainable dense layers into a pretrained model, were the first PEFT method proposed for NLP models.

00:00:30.765 --> 00:00:42.895
Adapters have recently been improved by the "Compacter" method of (Karimi Mahabadi et al., 2021), which constructs the adapter parameter matrices through Kronecker products of low-rank matrices.

00:00:42.895 --> 00:01:02.750
As an alternative to adding new layers, parameter-efficiency can be achieved by directly modifying activations with learned vectors, either by concatenation (Lester et al., 2021;Li and Liang, 2021), multiplication (Liu et al., 2022a), or addition (Ben Zaken et al., 2022).

00:01:02.750 --> 00:01:20.417
Alternatively, rather than adding new parameters or changing the model's computational graph, it is possible to make updates to the original model cheaper to store through the use of sparse (Sung et al., 2021;Guo et al., 2021) or low-rank (Hu et al., 2022) updates.

00:01:20.417 --> 00:01:33.910
Finally, it has been shown that optimization can be performed in a low-dimensional subspace (Li et al., 2018); storing the updates in this subspace can be seen as a PEFT method (Aghajanyan et al., 2021b).

00:01:33.910 --> 00:01:51.777
State-of-the-art PEFT methods add or update roughly four orders of magnitude fewer parameters than full-model fine-tuning without sacrificing (and in some cases improving) performance (Hu et al., 2022;Karimi Mahabadi et al., 2021;Liu et al., 2022a).

00:01:51.777 --> 00:01:54.669
Multi-Task and Zero-Shot Learning

00:01:54.669 --> 00:02:04.337
While traditional transfer learning includes finetuning, there are other paradigms that allow for immediate application of a pre-trained model to a downstream task of interest.

00:02:04.337 --> 00:02:15.067
Multi-task learning (Caruana, 1997;Ruder, 2017) aims to train a single model that can perform a wide variety of tasks out of the box.

00:02:15.067 --> 00:02:20.647
Typically, this is done by explicitly training the model on data from all tasks of interest.

00:02:20.647 --> 00:02:27.177
If a multi-task model has already been trained on a given downstream task, then no fine-tuning is necessary.

00:02:27.177 --> 00:02:42.207
Recent work has additionally demonstrated that multi-task models are also amenable to fine-tuning (Raffel et al., 2020;Aghajanyan et al., 2021a;Aribandi et al., 2022;Liu et al., 2022a).

00:02:42.207 --> 00:02:47.937
In certain cases, a multi-task model can be applied to a new task without any fine-tuning.

00:02:47.937 --> 00:02:51.667
This ability is referred to as zero-shot generalization.

00:02:51.667 --> 00:02:53.397
Radford et al.

00:02:53.397 --> 00:02:57.739
2017Radford et al. ( , 2019 and Brown et al.

00:02:57.739 --> 00:03:05.882
2020) demonstrated that language models trained with an unsupervised objective were able to perform a variety of tasks out-of-the-box.

00:03:05.882 --> 00:03:08.037
Later, Sanh et al.

00:03:08.037 --> 00:03:10.667
2022) and Wei et al.

00:03:10.667 --> 00:03:17.084
2022a) showed that multitask training can also enable zero-shot generalization abilities.

00:03:17.084 --> 00:03:25.972
While zero-shot generalization can circumvent fine-tuning completely, it has (as of writing) only been demonstrated on large and computationally-inten

00:00:00.000 --> 00:00:01.542
sive models.

00:00:01.542 --> 00:00:03.772
Inference and Compression

00:00:03.772 --> 00:00:08.127
Various approaches have been proposed to improve efficiency at inference time.

00:00:08.127 --> 00:00:19.419
Compression methods such as pruning (LeCun et al., 1989) and distillation (Hinton et al., 2015) assume that smaller models are more efficient than larger models.

00:00:19.419 --> 00:00:27.212
Adaptive computation works accelerate inference by ignoring inner modules for making a prediction (Schwartz et al., 2020b).

00:00:27.212 --> 00:00:34.842
Finally, quantization is an orthogonal approach that directly increases efficiency by modifying the underlying data type.

00:00:34.842 --> 00:00:36.197
Pruning

00:00:36.197 --> 00:00:38.989
Initially proposed by LeCun et al.

00:00:38.989 --> 00:00:52.669
1989), removing unnecessary weights from a neural network aims to avoid unnecessary computation to reduce inference time with limited accuracy loss, and furthermore, decrease memory capacity and bandwidth requirements.

00:00:52.669 --> 00:00:58.024
Pruning can be applied on different levels within a model: for instance, Voita et al.

00:00:58.024 --> 00:01:00.704
2019); Michel et al.

00:01:00.704 --> 00:01:09.197
2019) find that only few attention heads substantially contribute towards a model's prediction and propose to prune the rest; Correia et al.

00:01:09.197 --> 00:01:11.652
2019); Ji et al.

00:01:11.652 --> 00:01:14.219
2021); Qu et al.

00:01:14.219 --> 00:01:20.874
2022) verified that the weak attention values in the transformers can be pruned without accuracy loss.

00:01:20.874 --> 00:01:32.454
Others focus on pruning individual weights (Sanh et al., 2019;Gordon et al., 2020) or layers (Dong et al., 2017;Sajjad et al., 2020).

00:01:32.454 --> 00:01:54.184
Finally, some works try to identify good criteria for pruning specific weights/layers (Sanh et al., 2020;Hoefler et al., 2021) or even propose to dynamically drop layers (Fan et al., 2020) during inference; sometimes in combination with other efficiency methods such as adapters (Rckl et al., 2021).

00:01:54.184 --> 00:02:06.452
The increasing popularity of pruning methods has further raised the question of how to quantify and compare them (Tessera et al., 2021;Blalock et al., 2020;Gale et al., 2019).

00:02:06.452 --> 00:02:07.707
5

00:02:07.707 --> 00:02:09.374
Distillation

00:02:09.374 --> 00:02:15.367
Whereas pruning primarily focuses on removing weights from a pre-trained model, Hinton et al.

00:02:15.367 --> 00:02:24.559
2015) instead propose to train a smaller model (student) from scratch by using the pre-trained model to obtain a supervision signal (teacher).

00:02:24.559 --> 00:02:42.414
While early works focus on distilling task-specific models (Kim and Rush, 2016), recent works focus on distilling pre-trained models that can then be fine-tuned on specific downstream tasks (Sanh et al., 2019;Jiao et al., 2020;Sun et al., 2020).

00:02:42.414 --> 00:02:44.594
Adaptive Computation

00:02:44.594 --> 00:02:51.049
An alternative to compression approaches can be to adaptively decide for each instance which part of a model to use.

00:02:51.049 --> 00:03:05.804
For example, early exit predictions allow a system to only utilize the outputs of lower (early) layers in a model to make a prediction (Dabre et al., 2020;Elbayad et al., 2020;Schwartz et al., 2020b;.

00:03:05.804 --> 00:03:07.597
Quantization

00:03:07.597 --> 00:03:14.152
Various data types can be utilized as the underlying representation in neural networks (Section 7).

00:03:14.152 --> 00:03:19.769
Mapping high-precision data types to low-precision ones is commonly referred to as quantization.

00:03:19.769 --> 00:03:26.812
While quantization saves memory and computational cost, reducing the precision can lead to a loss in terms of accuracy.

00:03:26.812 --> 00:03:29.068
Therefore, quantization often req

00:00:00.000 --> 00:00:03.017
uires a careful model construction and training.

00:00:03.017 --> 00:00:04.935
Low-and mixed-precision.

00:00:04.935 --> 00:00:30.040
Various works target specific precision-levels such as integers (Kim et al., 2021), 8-bit (Quinn andBallesteros, 2018;Zafrir et al., 2019;Bhandare et al., 2019;Prato et al., 2020) and 3-bit quantization (Ji et al., 2021;Zadeh et al., 2022), and even ternary and binary representations Bai et al., 2020).

00:00:30.040 --> 00:00:37.807
Other works investigate mixed-precision quantization as different components may have a different sensitivity regarding their underlying precision.

00:00:37.807 --> 00:00:40.287
For instance, Shen et al.

00:00:40.287 --> 00:00:48.005
2020) show that embedding layers require more precise parameter representations than the attention layer while Kim et al.

00:00:48.005 --> 00:00:54.485
2021) show that nonlinear functions require more bits than the general matrix multiplication.

00:00:54.485 --> 00:01:04.652
Others define quantization as a constrained optimization problem to automatically identify layers where a lower precision is sufficient (Hubara et al., 2021).

00:01:04.652 --> 00:01:12.070
These works show that customized quantization schemes across different components can maintain the accuracy while increasing efficiency.

00:01:12.070 --> 00:01:14.237
Quantization-aware training.

00:01:14.237 --> 00:01:29.280
Finally, several works propose to consider quantization already during training to make them robust against performance losses after quantization (Zafrir et al., 2019;Kim et al., 2021;Stock et al., 2021).

00:01:29.280 --> 00:01:31.485
For instance, Bai et al.

00:01:31.485 --> 00:01:38.902
2020);  propose to utilize knowledge distillation to maintain the accuracy of binarized and ternarized models.

00:01:38.902 --> 00:01:40.532
Other Methods

00:01:40.532 --> 00:01:48.949
Although this survey presents the most prominent research areas that aim to improve inference efficiency, there exist several other methods with the same goal.

00:01:48.949 --> 00:01:51.104
For instance, Wu et al.

00:01:51.104 --> 00:02:03.959
2022) combine several methods to achieve utmost model compression, while other works improve task-specific mechanisms, such as beam-search in machine translation (Peters and Martins, 2021).

00:02:03.959 --> 00:02:12.064
Moreover, parallelism can also be exploited to further increase inference efficiency (Rajbhandari et al., 2022).

00:02:12.064 --> 00:02:14.219
Hardware Utilization

00:02:14.219 --> 00:02:20.199
Finally, we discuss several methods that consider the underlying hardware used for training and inference.

00:02:20.199 --> 00:02:27.617
A majority of the effort is dedicated to reducing GPU memory consumption as one of the major bottlenecks in transformer models.

00:02:27.617 --> 00:02:36.597
Note that many of the presented techniques can be applied across different stages of training and inference (Fig. 2), and can be combined for further efficiency.

00:02:36.597 --> 00:02:37.939
Data types.

00:02:37.939 --> 00:02:49.294
Traditionally, neural networks use the IEEE 765 single-precision 32-bit float which consists of 4 bytes representing a floating point number (float32).

00:02:49.294 --> 00:02:55.524
However, this substantially affects the memory consumption in GPUs with the increase of model parameters.

00:02:55.524 --> 00:03:09.104
Combining half-precision (float16) and single-precision (float32) data representations can cut a network's memory consumption in half for inference and almost half for training (Micikevicius et al., 2018).

00:03:09.104 --> 00:03:20.059
An alternative to float16 is the Brain Floating Point (bfloat16) that is utilized in TPUs and can lead to a more stable training (Kalamkar et al., 2019).

00:03:20.059 --> 00:03:21.112
bfloat16

00:00:00.000 --> 00:00:07.230
and float16 can furthermore lead to double the FLOP/S due to hardware support in many modern CPUs and GPUs.

00:00:07.230 --> 00:00:09.472
Reducing optimizer memory.

00:00:09.472 --> 00:00:17.952
Because the Adam optimizer keeps track of first and second order momentum, it needs to store two floats for each parameter in the neural network.

00:00:17.952 --> 00:00:27.495
Therefore, to train a model containing K parameters, the GPU must store 3K parameters corresponding to the model, the first and the second order momentum.

00:00:27.495 --> 00:00:41.825
Libraries like DeepSpeed (Ren et al., 2021a) allows the optimizer to be offloaded from GPU memory and into CPU RAM where the computations are performed in the CPU using highly-efficient AVX instructions.

00:00:41.825 --> 00:00:50.617
bitsandbytes (Dettmers et al., 2022) uses dynamic block-wise quantization and 8-bit integers to represent optimizers.

00:00:50.617 --> 00:00:59.910
Block-wise quantization requires bitsandbytes to split each tensor into blocks that are individually quantized, reducing the inter-GPU communication.

00:00:59.910 --> 00:01:09.765
bitsandbytes can reduce Adam's GPU memory consumption by 75% and, in many cases, speed up training by reducing inter-GPU communication.

00:01:09.765 --> 00:01:17.507
While bitsandbytes runs on GPUs, the method is theoretically compatible with the optimizer offloading presented in DeepSpeed.

00:01:17.507 --> 00:01:19.400
Specialized hardware.

00:01:19.400 --> 00:01:32.130
Specialized hardware for NLP applications that utilizes Application Specific Integrated Circuits (ASICs) or Field Programmable Gate Arrays (FPGAs) exists, but is not broadly available.

00:01:32.130 --> 00:01:43.485
These hardware designs use dedicated computational units for irregular methods that improve efficiency (such as quantization and pruning discussed in Section 6), hence they improve the efficiency.

00:01:43.485 --> 00:01:44.902
Wang et al.

00:01:44.902 --> 00:01:55.307
2021a) proposed hardware that predicts the unnecessary components in the transformers and prunes them, including redundant heads/tokens and weak attention values.

00:01:55.307 --> 00:01:56.800
Qu et al.

00:01:56.800 --> 00:02:04.642
2022) specifically proposed specialized hardware to schedule data loading to alleviate the imbalance introduced by pruning.

00:02:04.642 --> 00:02:25.635
Other works develop new types of dedicated processors and memories to match the properties of the components in the transformers; for instance, softmax and layer normalization Liu et al., 2021b), and embedded Resistive RAM (a nonvolatile memory with low latency and energy comsumption) to store word embeddings (Tambe et al., 2021).

00:02:25.635 --> 00:02:27.027
Co-design.

00:02:27.027 --> 00:02:39.120
Finally, we provide some pointers for works that jointly optimize the design of hardware, software, and algorithms which historically has been an important driver of efficiency gains (Hooker, 2021).

00:02:39.120 --> 00:02:41.512
For instance, Lepikhin et al.

00:02:41.512 --> 00:02:51.742
2021) demonstrate that improving the underlying compiler can already substantially improve parallelization allowing them to scale their model up to 600B parameters.

00:02:51.742 --> 00:03:05.485
Other prominent examples for co-design also focus on improvements of mixture of experts models that consider the underlying hardware leading to substantial speedups (He et al., 2022;Rajbhandari et al., 2022).

00:03:05.485 --> 00:03:07.665
Lastly, Barham et al.

00:03:07.665 --> 00:03:16.144
2022) propose a novel gang-scheduling approach together with parallel asynchronous dispatch that further leads to substantial efficiency improv

00:00:00.000 --> 00:00:01.180
ements.

00:00:01.180 --> 00:00:02.972
Edge Devices

00:00:02.972 --> 00:00:12.302
Running advanced NLP models to resourceconstrained devices provides better user experience as it preserves user-privacy and reduces inference latency.

00:00:12.302 --> 00:00:17.107
Various works specifically target increasing the efficiency for on-device settings.

00:00:17.107 --> 00:00:31.637
Squeeze-BERT (Iandola et al., 2020) is a mobile BERT-like architecture that incorporates efficient group convolutions into self-attention and it runs faster on mobile devices than other efficient models like MobileBERT (Sun et al., 2020).

00:00:31.637 --> 00:00:40.154
EdgeFormer (Ge and Wei, 2022) is a lightweight encoder-decoder transformer architecture that is designed for on-device settings.

00:00:40.154 --> 00:00:47.759
It runs on mobile CPUs under low latency and provides high-quality machine translation and grammar error correction abilities.

00:00:47.759 --> 00:00:58.302
Ghost-BERT (Huang et al., 2021) uses ghost modules that are built on top of depthwise separable convolutions used in MobileNets (Howard et al., 2017).

00:00:58.302 --> 00:01:07.607
LiteTransformer  utilizes longshort range attention to encode local context by convolutions and captures long range dependencies by attention operations.

00:01:07.607 --> 00:01:14.524
It improves the Transformer performance on machine translation tasks by a large margin under resource-constrained settings.

00:01:14.524 --> 00:01:28.754
Finally, ProFormer (Sankar et al., 2021) uses locality sensitive hashing and local projection attention layers to build word embeddings for text classification and reduces the runtime and memory for on-device deployments.

00:01:28.754 --> 00:01:30.547
Evaluation

00:01:30.547 --> 00:01:42.489
To evaluate efficiency, it is important to establish what resource-e.g., money, data, memory, time, power consumption, carbon emissions, etc-one attempts to constrain.

00:01:42.489 --> 00:01:53.082
Furthermore, efficiency does not intrinsically guarantee a reduction in overall resource consumption, as the resulting cost reduction may lead to an increase in demand and counteract its gains.

00:01:53.082 --> 00:02:07.062
This is known as Jevons paradox (Jevons, 1866) and is in part moderated by time lag between efficiency gains and demand increase, and external, human-influenced factors such as energy pricing and regulation.

00:02:07.062 --> 00:02:09.067
Measuring Efficiency

00:02:09.067 --> 00:02:14.309
There are often multiple factors that need to be traded-off against each other when improving efficiency.

00:02:14.309 --> 00:02:22.552
For instance, while a longer training of models may increase their task performance, at the same time, it increases the resource consumption.

00:02:22.552 --> 00:02:24.332
Pareto optimality.

00:02:24.332 --> 00:02:34.749
One solution for this issue can be to identify Pareto-optimal solutions, those for which no other system reaches a better or equal task performance with lower resource consumption.

00:02:34.749 --> 00:02:47.354
As there still may be more than one Pareto-optimal solution, the final choice depends on the application context; e.g., a small, average-quality model and a large, high-quality model can both be optimal.

00:02:47.354 --> 00:03:00.659
Consequently, as long as a model contributes to or extends the Pareto-optimal curve for a given problem and measurement space, it contributes something new-even if other solutions may use less resources or produce higher quality scores.

00:03:00.659 --> 00:03:05.452
Advancing NLP through pushing Pareto barriers is an established practice.

00:03:05.452 --> 00:03:08.188
For instance, the WNGT 2020

00:00:00.000 --> 00:00:11.642
machine translation shared task (Birch et al., 2020) considers the Pareto frontier between real time taken, system or GPU memory usage, and model size, as well as BLEU.

00:00:11.642 --> 00:00:22.685
Especially in MT evaluation, such trade-offs are commonplace (Kim et al., 2019;Bogoychev et al., 2020;Behnke and Heafield, 2021).

00:00:22.685 --> 00:00:24.727
Puvis de Chavannes et al.

00:00:24.727 --> 00:00:34.145
2021) include power consumption as a trade-off against perplexity to explore Pareto-efficient hyperparameter combinations for transformer models.

00:00:34.145 --> 00:00:36.362
Finally, Liu et al.

00:00:36.362 --> 00:00:45.030
2022b) examine Pareto efficiency for a number of tasks in an attempt to narrow model selection search space to efficient examples.

00:00:45.030 --> 00:00:46.785
Power consumption.

00:00:46.785 --> 00:00:50.390
One resource to measure efficiency is power consumption.

00:00:50.390 --> 00:00:57.607
There exist various way to measure power consumption, for instance, by using specific hardware such as an electricity meter.

00:00:57.607 --> 00:01:03.775
While this can provide precise figures with a high temporal accuracy, it cannot provide a fine-grained estimate.

00:01:03.775 --> 00:01:08.955
Moreover, this does not cover external energy costs such as cooling or networking.

00:01:08.955 --> 00:01:22.947
Another way is to utilize software tools such as MLCO2 (Luccioni et al., 2019) (2022) introduce a model card for NLP systems that encourages researchers to document efficiency in a consistent manner.

00:01:22.947 --> 00:01:27.715
Note that measuring power consumption programmatically comes with a number of caveats.

00:01:27.715 --> 00:01:36.145
First, sampling frequency is often restricted for a number of reasons at various levels of the stack and may result in a lag in measurement start.

00:01:36.145 --> 00:01:45.200
Consequently, shorter experiments may log an energy use of zero, and there will almost always be some part of a process' real energy demand that is missed.

00:01:45.200 --> 00:01:53.930
Second, inefficiencies such as heat loss are not reported by current APIs and hence, does not cover cooling and other system management activities.

00:01:53.930 --> 00:01:58.110
Third, not all architectures and operating systems are supported.

00:01:58.110 --> 00:02:06.065
For instance, power consumption under OSX is difficult to manage, and direct figures for TPU power consumption are not available.

00:02:06.065 --> 00:02:07.545
Carbon emission.

00:02:07.545 --> 00:02:13.162
Besides power consumption, the aforementioned works often also report the carbon emissions.

00:02:13.162 --> 00:02:20.667
They are computed using the power consumption and the carbon intensity of the marginal energy generation that is used to run the program.

00:02:20.667 --> 00:02:35.297
Thus, low-energy does not mean low-carbon, and high-energy models can-in the right region and with some care-be zero-carbon in terms of point energy consumption impact, if executed at the right time (i.e., when the energy mix is low-carbon intensity).

00:02:35.297 --> 00:02:46.290
For estimating the CO2 emissions from a specific program execution, APIs such as ElectricityMap 6 provide real-time access to carbon intensity for many regions.

00:02:46.290 --> 00:02:57.645
However, as carbon intensity varies and is affected by other factors like the power usage efficiency in a data center, it is often a poor basis for comparison; in fact, Henderson et al.

00:02:57.645 --> 00:03:02.000
2020) recommend to use multiple runs for a stable estimate.

00:03:02.000 --> 00:03:09.855
Furthermore, one needs to consider that zero-carbon program executions still consume energy (to beware of Jevons paradox).

00:03:09.855 --> 00:03:10.888
Financial impa

00:00:00.000 --> 00:00:01.242
ct.

00:00:01.242 --> 00:00:05.672
Monetary cost is a resource that one typically prefers to be efficient with.

00:00:05.672 --> 00:00:11.277
Both fixed and running costs affect NLP, depending on how one chooses to execute a model.

00:00:11.277 --> 00:00:20.645
As hardware configurations and their prices form discrete points on a typically non-linear scale, it is worth paying attention to efficient cost points and fitting to these.

00:00:20.645 --> 00:00:27.900
Implementing pre-emptible processes that can recover from interruptions also often allows access to much cheaper resources.

00:00:27.900 --> 00:00:35.255
When calculating or amortizing hardware costs, one should also factor in downtime, maintenance, and configuration.

00:00:35.255 --> 00:00:40.422
Measuring the total cost of ownership (TCO) provides a more useful metric.

00:00:40.422 --> 00:00:42.165
FLOP/s.

00:00:42.165 --> 00:00:51.145
Finally, a frequently reported efficiency measure are the floating point operations (FLOPs) and floating points per second (FLOP/s).

00:00:51.145 --> 00:00:59.650
While this discrete metric sounds well-defined in terms of what the hardware does, there is some variation at multiple stages of the stack, adding uncertainty.

00:00:59.650 --> 00:01:18.580
For example, different operations may count as a FLOP on different hardware; non-floating-point operations are not considered; hardware is rarely 100% utilised and achieving this productively is a challenge, so theoretical FLOP/s performance cannot be multiplied with time elapsed to yield the amount of computing performed.

00:01:18.580 --> 00:01:29.497
Still, FLOP/s per unit power can indicate which hardware choices have the potential to offer Pareto-efficient tradeoffs for these factors (Hsu et al., 2005).

00:01:29.497 --> 00:01:31.964
Trade-offs with other Desiderata

00:01:31.964 --> 00:01:39.882
One major, but seldomly studied concern when improving the efficiency are trade-offs with other desiderata such as fairness and robustness.

00:01:39.882 --> 00:01:42.224
For instance, Hooker et al.

00:01:42.224 --> 00:01:45.329
2020); Renduchintala et al.

00:01:45.329 --> 00:01:48.247
2021); Silva et al.

00:01:48.247 --> 00:01:55.789
2021) find that compression techniques such as pruning can amplify existing biases; Mohammadshahi et al.

00:01:55.789 --> 00:02:00.044
2022) further showcase this in a multilingual setting.

00:02:00.044 --> 00:02:05.824
So far, not many works investigate preserving a model's fairness when increasing its efficiency.

00:02:05.824 --> 00:02:08.879
To quantify such effects, Xu et al.

00:02:08.879 --> 00:02:12.622
2021) propose loyalty as a novel metric.

00:02:12.622 --> 00:02:20.639
Finally, Xu and Hu (2022) attempt to study these effects more systematically, however, with mixed conclusions.

00:02:20.639 --> 00:02:40.957
While, more positive insights have been found with other desiderata such as the out-of-distribution (OOD) generalization (Ahia et al., 2021;Iofinova et al., 2022) and model transfer (Gordon et al., 2020), we find that more work is necessary to better understand the impact of efficiency methods on them.

00:02:40.957 --> 00:02:43.899
Open Challenges in Measuring Efficiency

00:02:43.899 --> 00:02:52.504
The choice of hardware can lead to pronounced differences in certain efficiency measurements such as latency and thoroughput (Lee-Thorp et al., 2022).

00:02:52.504 --> 00:02:56.109
Properly measuring efficiency still remains a big challenge.

00:02:56.109 --> 00:02:58.339
For instance, Cao et al.

00:02:58.339 --> 00:03:05.307
2020) show that using software-based tools can often introduce large errors in estimating the true energy.

00:03:05.307 --> 00:03:15.820
Instead, more accurate estimates may be obtained by training a classifier on ground-truth energies obtained from power monitors (Cao et al., 2021); however, scaling

00:00:00.000 --> 00:00:04.817
them to new hardware devices and multiple GPUs stull remains an open problem.

00:00:04.817 --> 00:00:06.947
Separating different stages.

00:00:06.947 --> 00:00:12.240
It is important to separately characterize the efficiency of pretraining and fine-tuning stages.

00:00:12.240 --> 00:00:20.895
For example, models may present different memory requirements during training yet result in trained models with comparable inference memory consumption.

00:00:20.895 --> 00:00:27.337
This is because training often involves design choice that increases the memory overhead of backward propagation.

00:00:27.337 --> 00:00:32.067
For example, certain optimizers can require significantly more memory.

00:00:32.067 --> 00:00:41.247
In a similar vein, parameter sharing techniques have few benefits during training but show memory improvements at inference (Dehghani et al., 2021).

00:00:41.247 --> 00:00:43.852
Disagreement between cost factors.

00:00:43.852 --> 00:00:49.220
As partially discussed in Section 7, cost indicators may disagree with each other.

00:00:49.220 --> 00:01:02.937
For instance, mixture of experts increase the overall parameter count, but improve the trade-off between quality and FLOPs, as they minimize the per-data cost by routing to subsections of the model (Rajbhandari et al., 2022).

00:01:02.937 --> 00:01:09.292
Conversely, unstructured sparsity techniques can significantly minimize the overall number of FLOPs.

00:01:09.292 --> 00:01:20.335
Yet in practice, it introduces low-level operations that can lead to far higher memory requirements to store the indices that indicate what part of the matrix is sparse (Qu et al., 2022).

00:01:20.335 --> 00:01:22.502
Finally, Dao et al.

00:01:22.502 --> 00:01:29.332
2021) find specific sparsity patterns that achieve more predictable speedups with current hardware.

00:01:29.332 --> 00:01:31.299
Model Selection

00:01:31.299 --> 00:01:33.142
Hyperparameter search.

00:01:33.142 --> 00:01:39.222
The performance of machine learning methods can be substantially improved by a careful choice of hyperparameters.

00:01:39.222 --> 00:01:58.227
Model-based techniques such as Bayesian optimization (BO) (Snoek et al., 2012;Feurer et al., 2015) and graph-based semi-supervised learning (Zhang and Duh, 2020) use surrogate models to search efficiently for optimal hyperparameters, avoiding expensive grid search or manual tuning.

00:01:58.227 --> 00:02:15.494
The SMAC3 library (Lindauer et al., 2022) implements several BO strategies, including a budgetlimited variant for expensive deep learning tasks, and is integrated into auto-sklearn (Feurer et al., 2020) and auto-pytorch (Zimmer et al., 2021).

00:02:15.494 --> 00:02:39.037
A complementary approach to reduce the cost of hyperparameter optimization is the successive halving algorithm (SHA) (Jamieson and Talwalkar, 2016) and its massively parallel variant, asynchronous SHA (ASHA, , which test multiple hyperparameter settings in parallel for a fixed number of training iterations, then discard the half of the settings with the worst validation set performance.

00:02:39.037 --> 00:02:48.054
However, with limited computational budgets, both BO and ASHA can sometimes fail to identify good settings (Liu and Wang, 2021).

00:02:48.054 --> 00:02:59.122
It is unclear whether these methods can also be used to choose random initial weights or to order training samples, which also have a substantial effect on model performance (Dodge et al., 2020).

00:02:59.122 --> 00:03:01.089
Hyperparameter transfer.

00:03:01.089 --> 00:03:08.188
To minimise the number of trials needed to find optimal hyperparameter settings, we can transfer knowledge from other datasets or ta

00:00:00.000 --> 00:00:04.980
sks -similar to how an ML engineer might select reasonable settings by hand.

00:00:04.980 --> 00:00:15.997
Transfer neural processes  provide a way to transfer observations, parameters and configurations from previous tasks using Bayesian optimization with a neural process as the surrogate model.

00:00:15.997 --> 00:00:23.790
This can lead to more accurate models with fewer trials than conventional BO approaches, but has yet to be tested for large NLP models.

00:00:23.790 --> 00:00:34.432
Furthermore, when training a large neural network, the cost of each tuning step can be reduced using Transfer  to tune a small model, then transfer the hyperparameters to a larger model.

00:00:34.432 --> 00:00:50.637
First, the target model is parameterized using Maximal Update Parametrization (P) (Yang and Littwin, 2021), which finds a suitable smaller model (reduced width or depth) whose optimal hyperparameters will be similar to those of the larger target model.

00:00:50.637 --> 00:00:58.342
The small model is tuned using any preferred approach, and the chosen hyperparameter values are then used directly for the large target model.

00:00:58.342 --> 00:01:11.109
Transfer is applicable to many different hyperparameters, including learning rate, momentum, weight initialization variance and weight multipliers, but not to those controlling regularization, such as dropout.

00:01:11.109 --> 00:01:12.714
Conclusion

00:01:12.714 --> 00:01:23.857
In this survey, we organized the existing literature according to the traditional NLP pipeline, and provided a broad overview of existing methods to increase efficiency and their shortcomings.

00:01:23.857 --> 00:01:33.712
As our discussion shows, efficiency in NLP can be achieved in many different ways; but is also subject to various open challenges such as a good metric to quantify it.

00:01:33.712 --> 00:01:35.342
References

00:01:35.342 --> 00:01:42.922
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.

00:01:42.922 --> 00:01:44.777
2021a.

00:01:44.777 --> 00:01:49.494
Muppet: Massive multi-task representations with pre-finetuning.

00:01:49.494 --> 00:01:51.224
Acknowledgements

00:01:51.224 --> 00:02:02.092
This work was initiated at and benefitted substantially from the Dagstuhl Seminar 22232: Efficient and Equitable Natural Language Processing in the Age of Deep Learning.

00:02:02.092 --> 00:02:24.772
We further thank Yuki Arase, Jesse Dodge, Jessica Forde, Jonathan Frankle, Iryna Gurevych, Alexander Koller, Alexander Lser, Alexandra Sasha Luccioni, Haritz Puerto, Nils Reimers, Leonardo Riberio, Anna Rogers, Andreas Rckl, Noah A. Smith, Emma Strubell, and Thomas Wolf for a fruitful discussion and helpful feedback at the seminar.
WEBVTT

00:00:00.000 --> 00:00:02.742
Predictive Coding or Just Feature Discovery?

00:00:02.742 --> 00:00:06.784
An Alternative Account of Why Language Models Fit Brain Data

00:00:06.784 --> 00:00:16.052
Many recent studies have shown that representations drawn from neural network language models are extremely effective at predicting brain responses to natural language.

00:00:16.052 --> 00:00:18.632
But why do these models work so well?

00:00:18.632 --> 00:00:27.262
One proposed explanation is that language models and brains are similar because they have the same objective: to predict upcoming words before they are perceived.

00:00:27.262 --> 00:00:32.579
This explanation is attractive because it lends support to the popular theory of predictive coding.

00:00:32.579 --> 00:00:36.297
We provide several analyses that cast doubt on this claim.

00:00:36.297 --> 00:00:45.777
First, we show that the ability to predict future words does not uniquely (or even best) explain why some representations are a better match to the brain than others.

00:00:45.777 --> 00:00:55.007
Second, we show that within a language model, representations that are best at predicting future words are strictly worse brain models than other representations.

00:00:55.007 --> 00:01:08.487
Finally, we argue in favor of an alternative explanation for the success of language models in neuroscience: These models are effective at predicting brain responses because they generally capture a wide variety of linguistic phenomena.

00:01:08.487 --> 00:01:10.142
INTRODUCTION

00:01:10.142 --> 00:01:16.297
Predictive coding is a cognitive theory of the high-level mechanisms underlying sensory processing in the brain.

00:01:16.297 --> 00:01:21.164
It holds that the brain is constantly attempting to predict future events before they occur.

00:01:21.164 --> 00:01:28.457
These predictions are revised and updated via error signals generated upon comparison of predictions with observations.

00:01:28.457 --> 00:01:41.549
Predictive coding is attractive as a theory because it provides a concrete, conceptually simple, and mechanistically plausible objective for brain processing that seems to also relate to our own introspective experience of what it feels like to learn.

00:01:41.549 --> 00:01:55.879
Although originally formulated to explain visual processing in the brain (Huang & Rao, 2011;Jiang & Rao, 2021;Rao & Ballard, 1999), this theory has also been extended to language processing.

00:01:55.879 --> 00:02:05.522
For language, predictive coding theories posit that the brain works to preemptively generate predictions about future words and sentences as it perceives natural language stimuli.

00:02:05.522 --> 00:02:10.614
Evidence for predictive coding in language processing comes from several strands of research.

00:02:10.614 --> 00:02:36.282
First, many studies have shown electrophysiological signals associated with syntactically or semantically incongruent words or surprisal (Frank et al., 2015;Gagnepain et al., 2012;Heilbron et al., 2022;Kuperberg & Jaeger, 2016;Kutas & Hillyard, 1984;Mnte et al., 1990;Schmitt et al., 2021;Shain et al., 2020).

00:02:36.282 --> 00:02:42.037
These signals are thought to correspond to "prediction error" between what was predicted and what actually occurred.

00:02:42.037 --> 00:02:51.204
a n o p e n a c c e s s j o u r n a l Citation: Antonello, R., & Huth, A. (2022).

00:02:51.204 --> 00:02:53.947
Predictive coding or just feature discovery?

00:02:53.947 --> 00:02:57.739
An alternative account of why language models fit brain data.

00:02:57.739 --> 00:02:59.969
Neurobiology of Language.

00:02:59.969 --> 00:03:01.824
Advance publication.

00:03:01.824 --> 00:03:12.976
https://doi.org/10.1162/nol _a_00087 Second, many recent studies have shown

00:03:12.976 --> 00:03:49.643
that neural network language models (NNLMs), which embody (some elements of ) predictive coding theory, are much more effective at explaining brain activity elicited by natural language than earlier methods (Anderson et al., 2021;Antonello et al., 2021;Caucheteux et al., 2021a;Caucheteux & King, 2022;Goldstein et al., 2021;Jain & Huth, 2018;Jat et al., 2019;Li et al., 2021;Schrimpf et al., 2021;Tikochinski et al., 2021;Toneva et al., 2020).

00:03:49.643 --> 00:03:58.811
Some of these studies claim that the superiority of NNLMs over other methods is evidence for predictive coding theory in language Schrimpf et al., 2021).

00:03:58.811 --> 00:04:06.578
We argue in this paper that the high performance of these models should not be construed as positive evidence in support of a theory of predictive coding.

00:04:06.578 --> 00:04:15.770
As an alternative, we propose that the prediction task which these NNLMs attempt to solve is simply one way out of many to discover useful linguistic features.

00:04:15.770 --> 00:04:18.688
Language Models and Encoding Models

00:04:18.688 --> 00:04:29.018
Unidirectional NNLMs are artificial neural networks that are trained to perform a "next word prediction" task (Dai et al., 2019;Radford et al., 2019).

00:04:29.018 --> 00:04:38.823
Specifically, these neural networks are trained to generate a probability distribution over the next word in a sequence, conditioned on a context consisting of previous words.

00:04:38.823 --> 00:04:46.453
For example, when fed the context "Better late than", a language model might assign a high probability to the next word being "never.

00:04:46.453 --> 00:05:05.245
Compared to tasks that require labeled data, such as translation, question answering, or word sense disambiguation, NNLMs have a distinct advantage because of the near-limitless amount of data that can be used to train them; almost any natural language text that can be scraped from the internet is valid data to train an NNLM.

00:05:05.245 --> 00:05:24.488
Further, in order to do effective nextword prediction, NNLMs need to capture a great deal about the statistical regularities in natural language, including everything from part of speech (Tsai et al., 2019) to topic (Sun et al., 2019) to coreference information (Joshi et al., 2019).

00:05:24.488 --> 00:05:34.655
The ease of training NNLMs and their ability to learn many types of statistical dependencies has, in recent years, developed into the paradigm of language model fine-tuning.

00:05:34.655 --> 00:05:54.835
In this paradigm, representations extracted from existing NNLMs are retooled for other linguistic tasks such as named entity recognition (Li et al., 2020), summarization (Nikolich et al., 2021), question answering (Su et al., 2019), and sentiment analysis (Socher et al., 2013).

00:05:54.835 --> 00:06:13.178
Fine-tuning from NNLMs often outperforms models that are trained from scratch on these tasks, as it allows the model to reuse linguistic features that were learned by the original NNLM, and helps make up for the limited and costly hand-labeled training data that many downstream tasks currently require (Dodge et al., 2020).

00:06:13.178 --> 00:06:25.720
State-of-the-art NNLMs are typically organized into a series of architecturally homogeneous layers of processing blocks called transformers (Radford et al., 2019;Vaswani et al., 2017).

00:06:25.720 --> 00:06:26.600
Transformer

00:06:26.600 --> 00:06:34.017
s use a mechanism known as dot product attention to selectively process some elements of their input context while ignoring others.

00:06:34.017 --> 00:06:42.135
This mechanism enables models to integrate information over much longer timescales than other methods (Vaswani et al., 2017).

00:06:42.135 --> 00:06:48.715
The output of each transformer layer is an encoded representation of its inputs, often called a hidden state.

00:06:48.715 --> 00:06:58.295
For example, in the commonly used GPT-2 Small model (Radford et al., 2019), the hidden state is a 768-dimensional vector.

00:06:58.295 --> 00:07:02.375
This output vector is then fed into the next layer as its input.

00:07:02.375 --> 00:07:13.530
These layers serve to transform information from the initial input (often provided as word embeddings; see Mikolov et al., 2013) to a next word prediction output at the last layer.

00:07:13.530 --> 00:07:27.672
For this reason, the hidden states of later layers (those near the output) generally tend to act as representations that are more suitable for next word prediction than the hidden states of earlier layers, which are more similar to the initial word embeddings.

00:07:27.672 --> 00:07:40.877
Language models are typically evaluated by a metric known as perplexity, which measures how well they can predict next Language models: Autoregressive machine learning models that are trained to predict next words given a previous context.

00:07:40.877 --> 00:07:42.482
Perplexity:

00:07:42.482 --> 00:07:47.874
A formal metric for how well a language model can predict a given data set; lower is better.

00:07:47.874 --> 00:07:55.204
Predictive coding: A neuroscientific theory that posits that the brain uses a prediction objective or error to efficiently learn.

00:07:55.204 --> 00:07:56.259
words.

00:07:56.259 --> 00:08:07.202
Low perplexity means that the model assigns a high probability to the actual next word, while high perplexity means that it assigns a low probability; that is, lower perplexity is better.

00:08:07.202 --> 00:08:21.782
Drawing on the success of NNLMs for transferring to many different language tasks, neuroscientists have used NNLM representations that encode linguistic context to predict brain responses to natural language (Jain & Huth, 2018).

00:08:21.782 --> 00:08:35.699
Regression models that attempt to predict brain response to natural language stimuli by using an analytic feature space derived from the stimuli can be called encoding models (Huth et al., 2016;Naselaris et al., 2011).

00:08:35.699 --> 00:08:47.367
Much recent work has examined the extent to which features generated by language models can be used as encoding model inputs (Caucheteux & King, 2022;Schrimpf et al., 2021).

00:08:47.367 --> 00:08:58.509
Particular interest has been afforded to these LM-based encoding models, as they appear to outperform previous approaches that used representations sourced from non-contextual word embedding spaces.

00:08:58.509 --> 00:09:07.927
The success of this approach raises a key question: Why do LM-based encoding models perform so much better than encoding models that use other feature spaces?

00:09:07.927 --> 00:09:16.657
One hypothesis is that these features work so well precisely because their training objective-next word prediction-is the same objective that the brain has learned to solve.

00:09:16.657 --> 00:09:19.537
For example, both Schrimpf et al.

00:09:19.537 --> 00:09:28.488
2021) and Caucheteux and King (2022) showed that there is a strong correlation between encoding model performance for a feature space and that

00:09:28.488 --> 00:09:31.805
feature space's capacity for next word prediction.

00:09:31.805 --> 00:09:33.360
Schrimpf et al.

00:09:33.360 --> 00:09:43.653
2021) in particular argue that this strong correlation may be taken as evidence that the next-word prediction task is a fundamental part of biological language processing.

00:09:43.653 --> 00:09:53.508
Accepting this argument requires us to interpret correlation as causation: Some representations have high encoding performance because they have high next-word prediction performance.

00:09:53.508 --> 00:09:55.200
Goldstein et al.

00:09:55.200 --> 00:10:10.455
2021) went even further, showing that embeddings for future words can be predicted at significantly above chance by brain responses before word onset, even if simple contextual and semantic information such as word meaning and bigram information is removed.

00:10:10.455 --> 00:10:12.198
Caucheteux et al.

00:10:12.198 --> 00:10:21.890
2021b) demonstrate a similar result, showing that embeddings of future words improve LM-based encoding models over using only present context.

00:10:21.890 --> 00:10:28.845
They each suggest that these results stand as strong direct evidence of predictive coding in the brain during language processing.

00:10:28.845 --> 00:10:35.950
In this article, we analyze the strength of the evidence that encoding model research provides for the theory of predictive coding.

00:10:35.950 --> 00:10:41.493
We claim that existing evidence does not favor predictive coding above alternative explanations.

00:10:41.493 --> 00:10:54.235
However, we distinguish this evidence from the theory of predictive coding itself: It is plausible that the brain is doing predictive coding even if it cannot be proven using this type of evidence.) Our claim is based on two major arguments.

00:10:54.235 --> 00:11:06.565
First, we examine the correlation between next word prediction performance and encoding performance and present an alternative hypothesis for why representations from NNLMs perform well as encoding model inputs.

00:11:06.565 --> 00:11:22.758
In this alternative hypothesis, we suggest that the high encoding performance of NNLM representations can be explained by the fact that these representations transfer effectively to representations from many other linguistic tasks, a quality which is acknowledged in the fine-tuning literature.

00:11:22.758 --> 00:11:30.313
We produce a standardized metric for this "general" transfer performance and show that it is well correlated with brain encoding performance.

00:11:30.313 --> 00:11:38.605
We construct another metric that captures transfer performance to a representation extracted from a machine translation model from English to German.

00:11:38.605 --> 00:11:51.610
We show that the correlation between this translation metric and next word prediction performance is also high, and use this to argue that one should be generally skeptical of drawing strong inferences from correlations with encoding performance alone.

00:11:51.610 --> 00:11:57.653
Encoding models: Machine learning models that predict brain response from natural stimulus features.

00:11:57.653 --> 00:12:09.633
Second, we argue that a theory of predictive coding implies that language representations that are more useful for next word prediction should in general be better at predicting brain responses when controlling for other factors.

00:12:09.633 --> 00:12:18.208
Caucheteux and King (2022) analyzed the performance of individual layer hidden states as encoding model input features and showed that the intermediate

00:12:18.208 --> 00:12:27.063
layers of these language models, which are not the best at next word prediction, consistently outperform early and later layers as encoding model features.

00:12:27.063 --> 00:12:43.855
Using a variance partitioning argument, we build on this result to show that the late representations from NNLMs, which are the best at predicting next words, explain strictly less variance in nearly every cortical voxel than intermediate representations that are less effective at predicting next words.

00:12:43.855 --> 00:12:52.522
Using these results, we further argue that the existence of predictive information in the brain does not inherently necessitate a theory of predictive coding.

00:12:52.522 --> 00:12:54.690
MATERIALS AND METHODS

00:12:54.690 --> 00:12:56.982
MRI Data Collection

00:12:56.982 --> 00:13:09.800
We used functional magnetic resonance imaging (fMRI) data collected from five human subjects as they listened to English language podcast stories over Sensimetrics S14 (2022) headphones.

00:13:09.800 --> 00:13:15.205
Subjects were not asked to make any responses, but simply to listen attentively to the stories.

00:13:15.205 --> 00:13:28.297
For encoding model training, each subject listened to approximately 5 hr of unique stories across five scanning sessions, yielding a total of 9,189 data points for each voxel across the whole brain.

00:13:28.297 --> 00:13:35.027
For model testing, the subjects listened to the same test story once in each session (i.e., five times).

00:13:35.027 --> 00:13:38.595
These responses were then averaged across repetitions.

00:13:38.595 --> 00:13:46.312
Functional signal-to-noise ratios in each voxel were computed using the mean-explainable variance method from Nishimoto et al.

00:13:46.312 --> 00:13:49.367
2017) on the repeated test data.

00:13:49.367 --> 00:13:56.785
Only voxels within 8 mm of the mid-cortical surface were analyzed, yielding roughly 90,000 voxels per subject.

00:13:56.785 --> 00:14:12.352
Languageresponsive voxels were identified as those where at least 5% of the response variance for the test story, which was played at least five times for each subject, could be explained by the average response across repetitions (Nishimoto et al., 2017).

00:14:12.352 --> 00:14:22.620
MRI data were collected on a 3T Siemens Skyra scanner at the University of Texas at Austin Biomedical Imaging Center using a 64-channel Siemens volume coil.

00:14:22.620 --> 00:14:56.062
Functional scans were collected using a gradient echo-planar imaging sequence with repetition time (TR) = 2.00 s, echo time (TE) = 30.8 ms, flip angle = 71, multiband factor (simultaneous multislice) = 2, voxel size = 2.6 mm  2.6 mm  2.6 mm (slice thickness = 2.6 mm), matrix size = 84  84, and field of view = 220 mm.

00:14:56.062 --> 00:15:08.980
Anatomical data were collected using a T1-weighted multi-echo MP-RAGE sequence with voxel size = 1 mm  1 mm  1 mm following the Freesurfer morphometry protocol (Fischl, 2012).

00:15:08.980 --> 00:15:11.935
All subjects were healthy and had normal hearing.

00:15:11.935 --> 00:15:18.440
The experimental protocol was approved by the Institutional Review Board at the University of Texas at Austin.

00:15:18.440 --> 00:15:22.120
Written informed consent was obtained from all subjects.

00:15:22.120 --> 00:15:24.287
fMRI Preprocessing

00:15:24.287 --> 00:15:34.455
All functional data were motion corrected using the FMRIB Linear Image Registration Tool (FLIRT) from FSL 5.0 (Jenkinson & Smith, 2001).

00:15:34.455 --> 00:15:41.012
FLIRT was used to align all data to a template that was made from the average across the first functional run in the first story sess

00:15:41.012 --> 00:15:42.904
ion for each subject.

00:15:42.904 --> 00:15:46.697
These automatic alignments were manually checked for accuracy.

00:15:46.697 --> 00:15:59.239
Low frequency voxel response drift was identified using a second order Savitzky-Golay filter (Savitzky & Golay, 1964) with a 120 s window and then subtracted from the signal.

00:15:59.239 --> 00:16:15.157
To avoid onset artifacts and poor detrending performance near each end of the scan, responses were trimmed by removing 20 s (10 volumes) at the beginning and end of each scan, which removed the 10 s silent period and the first and last 10 s of each story.

00:16:15.157 --> 00:16:21.486
The mean response for each voxel was subtracted and the remaining response was scaled to have unit variance.

00:16:21.486 --> 00:16:23.966
Encoding Model Construction

00:16:23.966 --> 00:16:30.859
We used the fMRI data to generate voxelwise brain encoding models for 97 different language representations.

00:16:30.859 --> 00:16:40.451
In order to temporally align word times with TR times, we applied Lanczos interpolation together with a finite impulse response model as described in Huth et al.

00:16:40.451 --> 00:16:41.994
2016).

00:16:41.994 --> 00:16:59.679
Let t i (S) correspond to the instantiation of the i th representation on our transcribed stimulus set S. Let g(t i (S)) indicate a linearized ridge regression model that uses a temporally transformed version of the representation instantiation t i (S) as predictors.

00:16:59.679 --> 00:17:08.909
The temporal transformation accounts for the lag in the hemodynamic response function (Huth et al., 2016;Nishimoto et al., 2011).

00:17:08.909 --> 00:17:16.214
We use time delays of 2, 4, 6, and 8 s of the representation to generate this temporal transformation.

00:17:16.214 --> 00:17:43.727
For each subject x, voxel v, and representation t i , we fit a separate encoding model g (x,v,t i ) to predict the BOLD responseB from our represented stimulus, that is, B (x,v,t i ) = g (x,v,t i ) t i (S). Encoding model performance for a representation was computed as the average voxelwise performance across our five subjects.

00:17:43.727 --> 00:17:46.294
Next-Word Prediction Performance

00:17:46.294 --> 00:17:54.099
We performed a linear regression between each representation and the GloVe embedding of the next word (Pennington et al., 2014).

00:17:54.099 --> 00:18:03.154
We then computed the exponentiated average cross entropy between the distribution over the predicted next word from this regression against the ground truth next word.

00:18:03.154 --> 00:18:08.272
This value is used as a metric for how well each representation predicts next words.

00:18:08.272 --> 00:18:18.714
This metric was computed using a test corpus of approximately 54,000 words consisting of transcribed podcasts (LeBel, Wagner, et al., 2021).

00:18:18.714 --> 00:18:21.307
Representational Generality

00:18:21.307 --> 00:18:34.912
For our 97 representations, we used the method and publicly available data and code from our earlier work (Antonello et al., 2021) to measure the overall generality of the information contained in these representations.

00:18:34.912 --> 00:18:37.467
Let S be our set of stimulus data.

00:18:37.467 --> 00:18:42.447
Further define U(S) as the universal input feature space for our stimuli S.

00:18:42.447 --> 00:18:46.489
We used GloVe word embeddings of our stimulus data for U(S).

00:18:46.489 --> 00:19:01.044
For each representation t 2 T , we generated an encoder E t () such that the encoder extracts only information in U(S) that is needed to predict t (S). We did this by using a bottlenecked linear neural network that maps every u 2 U(S) to an intermediate

00:19:01.044 --> 00:19:26.669
lowdimensional latent space L t = E t (U(S)) and then maps it to the given representation space, s 2 S; t s    f E t U s       where f () is mapping from L t to t (S). We used a small latent space of 20 dimensions to encourage the encoder to extract only the information in U(S) that is relevant to compute t (S). These latent spaces were then scored on how much better they transferred to other representations.

00:19:26.669 --> 00:19:44.699
The use of this approach over simple linear regression enables us to normalize representations by their dimensionality and measure the overall generality of each representation rather then the total amount of information contained in each representation, which is more dependent on the total number of dimensions in each representation.

00:19:44.699 --> 00:20:07.241
For every pair of representations (t 1 , t 2 ) 2 T , we next generate a decoder D t 1 t 2 such that D t 1 t 2 (L t 1 ) = D t 1 t 2 (E t 1 (U(S))) approximates t 2 (S). This yields a total of n 2 decoders, where n = |T | is the total number of representations.

00:20:07.241 --> 00:20:17.909
All networks were trained with batches of size 1024 and standard stochastic gradient descent with a learning rate of 10 4 for the initial encoders and 2  10 5 for the decoders.

00:20:17.909 --> 00:20:27.539
We enforce a strict linearity constraint on both the encoder and decoder to ensure that representations that are nonlinearly decodable from one another are treated as distinct .

00:20:27.539 --> 00:20:30.856
Hyperparameters were chosen via coordinate descent.

00:20:30.856 --> 00:20:43.649
We finally used the decoders to generate a pairwise tournament matrix W t for each representation t by "fighting" all pairs of decoders that output to representation t using a held-out test set S test of sentences.

00:20:43.649 --> 00:21:19.694
Element (i, j ) in W t contains the ratio of samples in the test set for which D t i t has lower mean squared error than D t j t , that is, W t i;j    E s2S test D t i t s    D t j t s     : For example, if the decoder D AC has lower mean squared error than decoder D BC for 75% of the data in S test , we assign the ratio of 0.75/0.25 = 3 to entry (A, B) in the tournament matrix W C for representation C.

00:21:19.694 --> 00:21:33.574
We then averaged these pairwise tournament matrices W t over all t to generate an average pairwise tournament matrix W* which encodes the average relative performances of each representation in transferring to the other representations in our set.

00:21:33.574 --> 00:21:43.004
Further averaging this matrix along its first axis yields a metric of the relative propensity of each representation to transfer to each other representations in general.

00:21:43.004 --> 00:21:47.371
We used this metric to denote the generality score of a representation.

00:21:47.371 --> 00:21:55.726
Finally, we isolated the pairwise tournament matrix of an intermediate representation from a machine translation model from English to German.

00:21:55.726 --> 00:22:05.893
We similarly averaged this matrix along its first axis to yield a metric of translation transfer performance for each representation that was not from the English to German model.

00:22:05.893 --> 00:22:08.661
Voxelwise Variance Partitioning

00:22:08.661 --> 00:22:12.976
For voxelwise variance partitioning, we used the method established by de

00:22:12.976 --> 00:22:14.318
Heer et al.

00:22:14.318 --> 00:22:15.898
2017).

00:22:15.898 --> 00:22:33.718
When partitioning the variance explained between two input spaces, A and B, over an output set of voxels, we generated three models per voxel v and subject x:B (x,v,t A ) ,B (x,v,t B ) , andB (x,v,t AB ) .

00:22:33.718 --> 00:22:52.402
B (x,v,t A ) andB (x,v,t B ) refer to the models generated by using only A or B respectively, as the input representation.B (x,v,t AB ) refers to the model generated by using A concatenated with B as the input representation.

00:22:52.402 --> 00:22:57.745
Variance explained was computed on a held-out pair of test stories from our podcast data.

00:22:57.745 --> 00:23:05.425
Variance explained by the concatenated model but not explained by a single model was inferred to be uniquely explained by the other single model.

00:23:05.425 --> 00:23:17.879
Only language responsive voxels where at least 5% of the response variance for the test story was explainable (Nishimoto et al., 2017) were included in our variance partitioning analyses.

00:23:17.879 --> 00:23:19.397
RESULTS

00:23:19.397 --> 00:23:24.752
Correlations Between Encoding Performance and Other Metrics on Language Representations

00:23:24.752 --> 00:23:39.794
Several recent studies (Caucheteux et al., 2021a;Schrimpf et al., 2021) have shown that language models whose representations perform better as encoding model inputs tend to perform better at predicting upcoming words or sentences.

00:23:39.794 --> 00:23:48.212
We first sought to replicate this result by examining the relationship between encoding performance and the ability of a representation to predict next words.

00:23:48.212 --> 00:24:45.704
We extracted a total of 97 representations from several different natural language processing (NLP) models, including three word embedding spaces (GloVe, BERT-E, and FLAIR; Akbik et al., 2019;Devlin et al., 2019;Pennington et al., 2014), three unidirectional language models (GPT-2 Small, GPT-2 Medium, and Transformer-XL; Dai et al., 2019;Radford et al., 2019;Wolf et al., 2019), two masked bidirectional language models (BERT and ALBERT; Devlin et al., 2019;Lan et al., 2019), four common interpretable language tagging tasks (named entity recognition, part-of-speech identification, sentence chunking, and frame semantic parsing; Akbik et al., 2019), and two machine translation models (English  Mandarin, English  German; Tiedemann & Thottingal, 2020).

00:24:45.704 --> 00:24:54.922
A full description of each of these representations is given in the Supporting Information, which is available at https://doi .

00:24:54.922 --> 00:25:01.164
org/10.1162/nol_a_00087.

00:25:01.164 --> 00:25:09.369
Using a natural language fMRI data set, we constructed voxelwise encoding models for each of the 97 language representations.

00:25:09.369 --> 00:25:18.049
For each voxel, we then computed the encoding performance as the correlation between predicted and actual BOLD responses on a held-out test data set.

00:25:18.049 --> 00:25:26.704
We measured the overall encoding performance for each representation by computing the average encoding performance across all language-responsive voxels.

00:25:26.704 --> 00:25:35.659
We then measured how well each representation can do next word prediction by computing a "linearly extractable perplexity" score (see Materials and Methods).

00:25:35.659 --> 00:25:42.944
Comparing encoding performance and next word prediction performance across the 97 representations showed that these metrics are have h

00:25:42.944 --> 00:25:54.486
igh mean correlation (r = 0.847; Figure 1A), replicating earlier results (Caucheteux et al., 2021a;Schrimpf et al., 2021).

00:25:54.486 --> 00:26:14.354
While the high correlation between next word prediction performance and encoding performance is argued to be evidence for predictive coding in the brain, an alternative hypothesis is that certain representations work well as encoding models because they contain information that is generally useful for predicting representations from many language tasks, including next word prediction.

00:26:14.354 --> 00:26:24.559
To test this hypothesis, we measured how well each of the 97 representations could predict, or "transfer to" the other 96 representations (see Materials and Methods).

00:26:24.559 --> 00:26:30.051
This yields a metric measuring general transfer performance or representational generality.

00:26:30.051 --> 00:26:38.031
This metric tells us how much generally useful language information is contained in each representation as compared to the other representations.

00:26:38.031 --> 00:26:48.611
Representations that contain information useful for explaining other representations will have higher generality values, while those that contain little useful information will have lower values.

00:26:48.611 --> 00:26:58.454
An extended discussion of this metric and the motivation behind it is given in the Supporting Information, which is available at https://doi .

00:26:58.454 --> 00:27:04.696
org/10.1162/nol_a_00087.

00:27:04.696 --> 00:27:15.501
Figure 1B shows that there exists a very strong mean correlation (r = 0.864) between how well a representation transfers in general and its encoding performance.

00:27:15.501 --> 00:27:23.919
This correlation is numerically greater but not significantly different from the correlation between encoding performance and next word prediction performance.

00:27:23.919 --> 00:27:34.436
This result provides support for the hypothesis that certain representations produce effective encoding models because they have high general transfer performance, but does not constitute proof.

00:27:34.436 --> 00:27:46.691
Indeed, the high correlation between all three metrics-next word prediction performance, general transfer performance, and encoding performance-makes differentiation between competing causal hypotheses difficult.

00:27:46.691 --> 00:27:54.284
Yet even this confusion raises a salient point: Correlation between these metrics is not sufficient to support a causal argument.

00:27:54.284 --> 00:28:03.389
To further illustrate the difficulty of making causal claims based on this type of evidence, we present a final example of the same type which is absurd on its face.

00:28:03.389 --> 00:28:20.469
In this third analysis, we compared encoding performance for each representation to one specific type of transfer performance: the ability of each representation to predict features extracted from an English-toGerman translation model (Tiedemann & Thottingal, 2020;see Materials and Methods).

00:28:20.469 --> 00:28:32.861
From the set of models used to compute our representational generality metric, we isolated those that predicted the intermediate representation of a machine translation model that was trained to convert English text to German text.

00:28:32.861 --> 00:28:41.704
We then computed the relative transfer performance of each of our representations to this machine translation representation, yielding a metric we Figure 1.

00:28:41.704 --> 00:28:42.672
Correlates of

00:28:42.672 --> 00:28:44.477
encoding performance.

00:28:44.477 --> 00:28:56.427
Plotted are 97 language representations as measured according to four metrics: (A) Average encoding performance across five subjects, next word prediction performance, shown here as negative perplexity;

00:28:56.427 --> 00:29:07.169
B) general transfer performance to other representations; and (C) transfer performance to a representation extracted from an Englishto-German translation model.

00:29:07.169 --> 00:29:11.674
In each plot, encoding performance is compared to one of the other metrics.

00:29:11.674 --> 00:29:17.117
In every case, encoding performance of a representation correlates strongly with the other metric.

00:29:17.117 --> 00:29:27.284
Additionally, representations extracted from unidirectional language models (GPT-2 Small and GPT-2 Medium) are the highest in each of these metrics.

00:29:27.284 --> 00:29:42.877
This suggests that the reason features from unidirectional models such as GPT-2 (shown in orange) perform well may be because they are generally good features that perform well when transferring to other language representations, rather than because they are simply good at next word prediction.

00:29:42.877 --> 00:29:44.607
D) Subsamples.

00:29:44.607 --> 00:29:52.499
To robustly estimate correlations, 70 points from each comparison were selected at random 10,000 times and then correlated.

00:29:52.499 --> 00:29:54.954
These are presented in the boxplot.

00:29:54.954 --> 00:29:57.947
call "Eng  De translation transfer performance.

00:29:57.947 --> 00:30:07.777
Comparing encoding performance to Eng  De translation transfer performance again showed a high mean correlation (r = 0.780; Figure 1C).

00:30:07.777 --> 00:30:10.132
How should we interpret this result?

00:30:10.132 --> 00:30:21.799
If we were to assume that this correlation suggests causation (and were not aware of the other results), we might conclude that the objective underlying the brain's processing of English language is translation to German.

00:30:21.799 --> 00:30:27.092
But this is absurd, not least because none of the subjects in this study speak fluent German.

00:30:27.092 --> 00:30:33.922
Instead, we should conclude that this correlation-like the others we have reported here-is likely the result of common causes.

00:30:33.922 --> 00:30:40.752
To effectively predict brain responses, a representation must contain many different types of linguistic information.

00:30:40.752 --> 00:30:47.719
Some types of linguistic information are useful for predicting representations extracted from an Eng  De translation model.

00:30:47.719 --> 00:30:54.149
Thus, representations that make for good encoding models also excel at translating English to German.

00:30:54.149 --> 00:30:58.254
Comparing Across Layers of Neural Network Language Models

00:30:58.254 --> 00:31:03.846
We next investigated implications of predictive coding theory just within a single NNLM.

00:31:03.846 --> 00:31:10.239
One consequence of predictive coding theory is that the brain should encode information about its next word predictions.

00:31:10.239 --> 00:31:16.631
Thus, representations that contain predictive information about next words should explain brain responses well.

00:31:16.631 --> 00:31:26.999
Further, representations that can predict next words should uniquely explain some variance in brain responses that is not explained by representations that lack that predictive information.

00:31:26.999 --> 00:31:39.116
We investigated this issue by analyzing encoding performance for different layers from two variations of the same NNLM, GPT-2 Small and GPT-2 Medium (Radford et al., 2019).

00:31:39.116 --> 00:31:42.616
In these unidirectional language models, words enter at the fi

00:31:42.616 --> 00:31:49.971
rst layer and then propagate through many intermediate layers until, at the last layer, the model predicts the next word.

00:31:49.971 --> 00:31:57.513
Across layers, the representations slowly shift from more input-like in the early layers to more prediction-like in the latest layers.

00:31:57.513 --> 00:32:19.306
Many earlier reports have shown that the best encoding performance (and transfer performance) is obtained from layers closer to the middle of such a model, and not the latest layers (Antonello et al., 2021;Caucheteux et al., 2021a;Caucheteux & King, 2022;Jain & Huth, 2018;Toneva & Wehbe, 2019).

00:32:19.306 --> 00:32:28.348
This suggests that the intermediate layers are better at capturing linguistic structure than the latest layers, even though the latest layers are best at next word prediction.

00:32:28.348 --> 00:32:37.928
This could contradict predictive coding theory, which would suggest that the latest layers, which are best at predicting future words, should also yield the best encoding models.

00:32:37.928 --> 00:32:46.846
To study this issue more closely, we both constructed encoding models and measured next word prediction performance for each layer of the two GPT models.

00:32:46.846 --> 00:32:56.626
Figure 2A shows the next word prediction performance of each layer alongside the hypothesized relationship between encoding performance and depth suggested by predictive coding.

00:32:56.626 --> 00:33:04.581
As expected, the next word prediction performance increases nearly monotonically, achieving its highest values in the latest layers.

00:33:04.581 --> 00:33:11.673
However, actual encoding model performance (averaged across voxels and subjects) does not follow this pattern.

00:33:11.673 --> 00:33:22.853
Here, consistent with earlier reports, we see that encoding performance peaks at between 60% and 80% of maximum model depth, and then falls precipitously for the latest layers.

00:33:22.853 --> 00:33:28.258
If the brain was truly representing predictions for the next word, we should not see this pattern.

00:33:28.258 --> 00:33:43.713
However, this overall comparison is not conclusive: Although the intermediate layers provide better average encoding performance, it is still possible that the latest layers, by virtue of doing better next word prediction, uniquely capture some variance in brain responses.

00:33:43.713 --> 00:33:52.743
This would be sufficient to support the theory of predictive coding, which does not require that every brain area represent next word predictions, only that some do.

00:33:52.743 --> 00:33:57.798
Put succinctly, next word prediction anywhere supports predictive coding everywhere.

00:33:57.798 --> 00:34:07.303
To explicitly test for this possibility we used a variance partitioning analysis to determine whether any brain responses are uniquely explained by the last layer.

00:34:07.303 --> 00:34:24.946
In this analysis, we measured how much of the variance in brain response could be uniquely explained by either the most performant layer in each model (measured by average voxelwise correlation) or the last layer in each model, as well as the amount of variance that could be explained equally well by either of those layers.

00:34:24.946 --> 00:34:34.288
This was done by fitting three encoding models: one with just the best layer, one with just the last layer, and one with both representations concatenated.

00:34:34.288 --> 00:34:38.528
Figure 2B and C show the results of this variance partitioning analys

00:34:38.528 --> 00:34:39.508
is.

00:34:39.508 --> 00:34:50.588
Here we see that the most performant layer (the ninth layer in GPT-2 Small) does not merely outperform the last layer, but actually dominates the last layer across the entire cortex.

00:34:50.588 --> 00:35:03.768
While much of the variance that can be explained by either layer is explained by both, the last layer uniquely explains no significant additional variance above the ninth layer, while the ninth layer explains some variance above the last layer.

00:35:03.768 --> 00:35:19.573
In fact, owing to the combination of high covariance of the 12th layer features with the ninth layer features and having low beneficial contribution of its own, the ridge regression using the concatenated model performs slightly worse than the ridge regression using just the ninth layer features.

00:35:19.573 --> 00:35:26.590
This leads to a negative average measured unique variance explained for the 12th layer, which can be seen in Figure 2C.

00:35:26.590 --> 00:35:39.320
If the brain was performing an internal prediction task, then we would expect that at least some voxels would have unique variance that could be explained only by the last layer, which is most similar to the final predictive output of the language model.

00:35:39.320 --> 00:35:51.050
The fact that no variance is uniquely explained by the last layer suggests that some intermediate structural representation that is reached in the course of next word prediction is closer to what the brain internally represents.

00:35:51.050 --> 00:36:05.705
As the intermediate layers are also the best at transferring to other representations, this further supports the hypothesis that overall representational generality-and not next word prediction-underlies the success of language models at predicting brain data.

00:36:05.705 --> 00:36:07.222
DISCUSSION

00:36:07.222 --> 00:36:16.615
Recent work has argued in favor of a predictive coding theory of linguistic cognition based on evidence from encoding models (Schrimpf et al., 2021).

00:36:16.615 --> 00:36:30.495
Among the most noteworthy claims stemming from the encoding model literature is the observation, which we have replicated, that a strong correlation exists between the encoding performance of a linguistic representation and its ability to predict next words.

00:36:30.495 --> 00:36:38.100
This correlation has been taken as causal evidence that the brain is driven by predictive mechanisms that underlie its high-level objectives.

00:36:38.100 --> 00:36:41.705
We believe, however, that this inference is flawed.

00:36:41.705 --> 00:36:52.235
It is perfectly reasonable to expect that if the brain encodes a feature, then a model that also encodes the same feature will fit the brain better than a model that does not, all other things equal.

00:36:52.235 --> 00:37:05.227
But predictive coding arguments apply this implication in the wrong direction by assuming that models that fit the brain better than others have feature X, so therefore the brain also has feature X, where "X" in this case is next word prediction.

00:37:05.227 --> 00:37:13.957
Issues with this particular type of reasoning about artificial and biological computation are discussed extensively by Guest and Martin (2021).

00:37:13.957 --> 00:37:19.612
As an analogy, consider the problem in signal processing of spectral density estimation.

00:37:19.612 --> 00:37:25.728
Linear autoregressive models are often used to provide regularized estimates of the spectrum of a signal (Ulr

00:37:25.728 --> 00:37:28.520
ych & Bishop, 1975).

00:37:28.520 --> 00:37:37.913
Yet it would be false to suggest that spectral density estimation is an example of predictive coding, as autoregressive models are merely one way to accomplish this goal.

00:37:37.913 --> 00:37:44.868
In the same way, we cannot assume that language models fit the brain well because the brain is trying to predict future inputs.

00:37:44.868 --> 00:37:54.435
The correlation between a representation's encoding performance and its ability to transfer to an English-to-German translation representation underscores this problem.

00:37:54.435 --> 00:38:08.453
If we were to apply the same logic to this correlation as is applied to the correlation between the predictive power of models and their encoding model performance, we might-absurdly-conclude that what underlies linguistic processing in the brain is German translation.

00:38:08.453 --> 00:38:26.658
Yet a much simpler explanation for both effects is that generality in transferring to linguistic tasks is highly correlated with both measures, and representations that are suitable for one sufficiently general task (such as language modeling) are likely to be suitable for many others (such as translation or brain encoding).

00:38:26.658 --> 00:38:38.200
Furthermore, one possible entailment of predictive coding theory is that representations that better encode next word prediction ought to capture some responses somewhere in the brain better than representations that do not.

00:38:38.200 --> 00:38:50.693
However, our variance partitioning analysis showed that as next-word linear decodability continues to improve across layers in GPT-2 Small, encoding performance declines not merely on average, but everywhere.

00:38:50.693 --> 00:39:05.973
One might object to an argument such as this, on the basis that such an entailment is not necessary for predictive coding and that prediction may simply be an objective of the language system, or that prediction in the brain occurs not at the word level but at a more abstract conceptual level.

00:39:05.973 --> 00:39:20.790
While this seems exceedingly plausible, we are somewhat wary of treating predictive coding itself as a rigorous scientific theory if it is only to mean that the brain uses the objective of (possibly conceptual) prediction in order to help generate or manifest the language system.

00:39:20.790 --> 00:39:36.208
We feel that this interpretation of predictive coding is vague and underdefined, as it is unclear to us what provably false statements about the nature of the language system could be made if directly measurable quantities such as linear next-word prediction performance are rejected as irrelevant.

00:39:36.208 --> 00:39:43.313
We acknowledge that the tests we have explored here may not be suitable for assessing every potential interpretation of predictive coding.

00:39:43.313 --> 00:39:53.580
Thus, we would encourage our peers in the field who hold affirmative views regarding "conceptual" predictive coding to expand and formalize them, so that they can be more precisely evaluated.

00:39:53.580 --> 00:40:04.473
Of course, it is possible that the effects of predictive coding are simply undetectable at the spatial and temporal resolution of fMRI, and that is a fundamental limitation of the analyses in this article.

00:40:04.473 --> 00:40:07.780
But suppose that we could do this variance partitioning analysis at

00:40:07.780 --> 00:40:15.197
perfect resolution, without the limitations of neuroimaging methods, limited data, and imperfect regression techniques.

00:40:15.197 --> 00:40:36.752
If we still observed no meaningful improvement anywhere in the brain from adding a later layer of a language model to an earlier one, then proponents of predictive coding would surely need to specify what quantifiable and falsifiable claims are being made about the language system according to predictive coding theory that uniquely distinguish prediction from absurd objectives like English-to-German translation.

00:40:36.752 --> 00:40:57.932
Encoding model arguments concluding that the brain learns through prediction must necessarily contend with the possibility that observed phenomena are the product of the lowdimensional structure that naturally arises across language representations (Antonello et al., 2021), whether they be from the brain or artificial models, and not the consequence of an inherently predictive process.

00:40:57.932 --> 00:41:13.000
Furthermore, eliminating the confounds between structure and prediction is extremely challenging, as any sufficiently structured linguistic system will necessarily contain some predictive information, and any sufficiently predictive linguistic system will possess inherent structure.

00:41:13.000 --> 00:41:18.542
What does this all mean for the wider claims about a theory of predictive coding for linguistic processing?

00:41:18.542 --> 00:41:26.347
We do not believe any of the results or arguments made in this article should be considered evidence against predictive coding as a cognitive theory.

00:41:26.347 --> 00:41:31.852
Indeed, predictive coding elegantly and mechanistically explains many observed phenomena.

00:41:31.852 --> 00:41:39.045
We do, however, claim that evidence from encoding model research should not be seen to currently support a theory of predictive coding.

00:41:39.045 --> 00:41:52.912
This is due to the fact that much of what is cited as the strongest evidence in favor of predictive coding from encoding model research would very likely be true even in the absence of predictive coding, as our representational generality results demonstrate.

00:41:52.912 --> 00:41:59.667
If we are to reject the existing evidence, a logical next question is What would suffice as evidence for predictive coding?

00:41:59.667 --> 00:42:09.160
One possible avenue might be to determine whether next word information can be used to predict brain activity before word onset better than information from previous words.

00:42:09.160 --> 00:42:12.777
This is exactly the approach taken by Goldstein et al.

00:42:12.777 --> 00:42:15.907
2021) and Caucheteux et al.

00:42:15.907 --> 00:42:17.675
2021b).

00:42:17.675 --> 00:42:28.292
They showed that a small but statistically significant improvement in encoding performance can be gleaned by using future words to predict brain responses, as compared to only using past words.

00:42:28.292 --> 00:42:36.672
While this is an elegant test, we feel the conclusion that is drawn-that this implies that predictive coding occurs in the brain-should still be viewed with skepticism.

00:42:36.672 --> 00:42:50.127
This is because it is challenging to differentiate between next word predictive information that is incidentally useful for prediction but was generated for some other objective, and information that has been gleaned in the process of directly trying to predict next words.

00:42:50.127 --> 00:42:51.560
As we have seen, lin

00:42:51.560 --> 00:42:58.915
guistic information is highly versatile and general, and information that is useful for one task is often useful for many others.

00:42:58.915 --> 00:43:09.545
Recall, for instance, that it is entirely possible to build a reasonably effective encoding model for English speakers using information derived from an English-to-German translation model.

00:43:09.545 --> 00:43:20.387
So it is quite reasonable to believe that some predictive or future information would be useful for brain encoding even if prediction itself is not the driving mechanism of linguistic processing in the brain.

00:43:20.387 --> 00:43:26.280
If evidence suggesting that next word information aids in brain encoding does not suffice, what might?

00:43:26.280 --> 00:43:31.722
Predictive coding as a theory seems, ironically, to not predict many phenomena uniquely.

00:43:31.722 --> 00:43:35.952
Much of what predictive coding can explain can also be explained without it.

00:43:35.952 --> 00:43:42.345
So what measurable phenomenon differentiates a world where the brain does predictive coding from one where the brain does not?

00:43:42.345 --> 00:43:50.300
The discovery of some naturally occurring low-level neural circuit that encodes prediction as an objective of language learning would be strong evidence.

00:43:50.300 --> 00:43:55.692
There is undeniably much existing evidence that is necessary for predictive coding to be true.

00:43:55.692 --> 00:44:05.272
But without direct access to the neural circuits underlying language processing, convincingly sufficient evidence for predictive coding will no doubt be difficult to produce.

00:44:05.272 --> 00:44:16.327
Cognitive theories invoking prediction as an essential element are fundamentally tied to those that invoke generality, or more simply, learned structure, as each can plausibly explain the other.

00:44:16.327 --> 00:44:20.432
There may be no easy path forward in disentangling these concepts.

00:44:20.432 --> 00:44:25.425
Predictive coding presents both a promise and a challenge to computational neurolinguists.

00:44:25.425 --> 00:44:35.730
On one hand, as a cognitive theory, it makes a relatively concrete and exceedingly plausible claim about the high-level nature of the brain that greatly coincides with our intuition.

00:44:35.730 --> 00:44:42.147
It would plainly represent a grand achievement of modern computational neuroscience if it could be proven to be true.

00:44:42.147 --> 00:44:48.877
On the other hand, serious inquiry into predictive coding naturally introduces a perfidious tangle of confounds.

00:44:48.877 --> 00:44:58.394
Finding a solution to these confounding issues may be a major step toward discovering the computational principles underlying language processing in the human brain.

00:44:58.394 --> 00:45:00.124
ACKNOWLEDGMENTS

00:45:00.124 --> 00:45:05.804
We would like to acknowledge Shailee Jain and Arjun Bose for editing and feedback on this manuscript.

00:45:05.804 --> 00:45:17.597
This research was funded by grants from the NIDCD and NSF (1R01DC020088-001), the Burroughs-Wellcome Foundation, and a gift from Intel Inc.

00:45:17.597 --> 00:45:20.339
Predictive coding or just feature discovery?

00:45:20.339 --> 00:45:42.396
Downloaded from http://direct.mit.edu/nol/article-pdf/doi/10.1162/nol_a_00087/2062803/nol_a_00087.pdf by guest on 18 December 2022